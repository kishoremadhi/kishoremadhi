{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f8af71",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-01-02T12:41:29.671436Z",
     "iopub.status.busy": "2024-01-02T12:41:29.671053Z",
     "iopub.status.idle": "2024-01-02T12:41:58.150486Z",
     "shell.execute_reply": "2024-01-02T12:41:58.149336Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 28.492055,
     "end_time": "2024-01-02T12:41:58.153094",
     "exception": false,
     "start_time": "2024-01-02T12:41:29.661039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (3.7.2)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy) (6.3.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.10.12)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (68.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.24.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\r\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\r\n",
      "Collecting spacy_transformers\r\n",
      "  Obtaining dependency information for spacy_transformers from https://files.pythonhosted.org/packages/af/e0/373f02d71ba7ba21629f523d5a05856940deea069c26daf012f809c66e92/spacy_transformers-1.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading spacy_transformers-1.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\r\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (3.7.2)\r\n",
      "Requirement already satisfied: transformers<4.37.0,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (4.36.0)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (2.0.0)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (2.4.8)\r\n",
      "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers)\r\n",
      "  Obtaining dependency information for spacy-alignments<1.0.0,>=0.7.2 from https://files.pythonhosted.org/packages/76/ea/44ffa5dbe1aa412cc274472d5b3dd3171ae2ea6aa2168fc21afa9038b7a9/spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (1.24.3)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.1.2)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.3.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.66.1)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.10.12)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (68.1.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (21.3)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (4.5.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (3.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (0.19.4)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (2023.8.8)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (0.15.0)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (0.4.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers<4.37.0,>=3.4.0->spacy_transformers) (2023.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.9)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2023.11.17)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.10)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->spacy_transformers) (1.3.0)\r\n",
      "Downloading spacy_transformers-1.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.9/197.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: spacy-alignments, spacy_transformers\r\n",
      "Successfully installed spacy-alignments-0.9.1 spacy_transformers-1.3.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!pip install spacy_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd94734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:41:58.173819Z",
     "iopub.status.busy": "2024-01-02T12:41:58.173380Z",
     "iopub.status.idle": "2024-01-02T12:42:10.159874Z",
     "shell.execute_reply": "2024-01-02T12:42:10.158962Z"
    },
    "papermill": {
     "duration": 11.999621,
     "end_time": "2024-01-02T12:42:10.162415",
     "exception": false,
     "start_time": "2024-01-02T12:41:58.162794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484a7a34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:10.184410Z",
     "iopub.status.busy": "2024-01-02T12:42:10.183904Z",
     "iopub.status.idle": "2024-01-02T12:42:10.191031Z",
     "shell.execute_reply": "2024-01-02T12:42:10.190149Z"
    },
    "papermill": {
     "duration": 0.020296,
     "end_time": "2024-01-02T12:42:10.193166",
     "exception": false,
     "start_time": "2024-01-02T12:42:10.172870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec11cac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:10.214531Z",
     "iopub.status.busy": "2024-01-02T12:42:10.214181Z",
     "iopub.status.idle": "2024-01-02T12:42:11.295187Z",
     "shell.execute_reply": "2024-01-02T12:42:11.293731Z"
    },
    "papermill": {
     "duration": 1.094319,
     "end_time": "2024-01-02T12:42:11.297597",
     "exception": false,
     "start_time": "2024-01-02T12:42:10.203278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan  2 12:42:11 2024       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   34C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\r\n",
      "| N/A   39C    P8     9W /  70W |      3MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960d6c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.319555Z",
     "iopub.status.busy": "2024-01-02T12:42:11.319184Z",
     "iopub.status.idle": "2024-01-02T12:42:11.338803Z",
     "shell.execute_reply": "2024-01-02T12:42:11.338035Z"
    },
    "papermill": {
     "duration": 0.032854,
     "end_time": "2024-01-02T12:42:11.340885",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.308031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jd_data=json.load(open('/kaggle/input/keyword-jd/annotations (5).json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8c2bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.362335Z",
     "iopub.status.busy": "2024-01-02T12:42:11.362032Z",
     "iopub.status.idle": "2024-01-02T12:42:11.425204Z",
     "shell.execute_reply": "2024-01-02T12:42:11.424279Z"
    },
    "papermill": {
     "duration": 0.07938,
     "end_time": "2024-01-02T12:42:11.430340",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.350960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': ['MUST HAVE',\n",
       "  'GOOD TO HAVE',\n",
       "  'EXPERIENCE',\n",
       "  'LOCATION',\n",
       "  'JOB TITLE'],\n",
       " 'annotations': [['Job Title: AWS Administrator Location: GA, Atlanta, (Remote) Duration: 1 Year Key Responsibilities: Establish configuration, compliance, and audit program to effectively manage AWS accounts and resources. Review cloud resources and cost drivers, compute, storage, network, managed services, database and service licenses, marketplace, and accounts. As part of our Managed Services offering for clients; monitor rightsizing, elasticity, storage optimization, and identify unused resources. Maintain cloud-based servers patching vulnerabilities, backup/restore operations, provision new servers, configure firewalls, configure monitoring systems. Manage account containerization and tagging for internal cost centers and end client billing feeds. Monitor cost allocations, security compliance, budgets and set alerts across accounts, workloads, and users. Operational monitoring to identify and address issues. Use AWS Config to monitor and track resource configuration. Set standards for resource configurations, evaluate configuration compliance, and risk, and remediate configuration drift. Use AWS CloudTrail for compliance audits by recording and storing event logs for actions made within AWS accounts. Set controls to monitor to assure compliance. Familiarity with AWS security best practices and hands-on experience in implementing security controls and compliance requirements with services like AWS Security Hub, Guard Duty, and AWS Audit Manager. Experience with monitoring tools, middleware software and ITSM tools a plus. Qualifications: Bachelor’s degree in computer science or a related field from an accredited college or university. Eight (8) years of Cloud Administration Experience. Excellent technical knowledge of IT Infrastructure, including switches, routers, server operating systems and hardware, storage arrays, and security applications. Strong system administration experience in Windows and Linux environments. Experience with automation using an established framework (SaltStack, Puppet, Chef, Ansible, etc.). Strong technical knowledge of current protocols, operating systems, and standards. Able to read and understand technical manuals and procedural documentation.  Experience in public cloud environments, including AWS and/or Azure. Experience working in an ITIL-driven environment and working knowledge of ITIL principles and processes. ',\n",
       "   {'entities': [[11, 28, 'JOB TITLE'],\n",
       "     [39, 41, 'LOCATION'],\n",
       "     [43, 50, 'LOCATION'],\n",
       "     [53, 59, 'LOCATION'],\n",
       "     [71, 72, 'EXPERIENCE'],\n",
       "     [177, 180, 'MUST HAVE'],\n",
       "     [205, 211, 'GOOD TO HAVE'],\n",
       "     [403, 410, 'GOOD TO HAVE'],\n",
       "     [424, 434, 'GOOD TO HAVE'],\n",
       "     [604, 613, 'GOOD TO HAVE'],\n",
       "     [771, 779, 'GOOD TO HAVE'],\n",
       "     [1100, 1110, 'MUST HAVE'],\n",
       "     [1514, 1518, 'GOOD TO HAVE'],\n",
       "     [1906, 1913, 'MUST HAVE'],\n",
       "     [1918, 1923, 'MUST HAVE'],\n",
       "     [1997, 2006, 'GOOD TO HAVE'],\n",
       "     [2008, 2014, 'GOOD TO HAVE'],\n",
       "     [2016, 2020, 'GOOD TO HAVE'],\n",
       "     [2022, 2029, 'GOOD TO HAVE'],\n",
       "     [2260, 2266, 'GOOD TO HAVE']]}],\n",
       "  ['Python Developer responsibilities include: Writing effective, scalable code Developing back-end components to improve responsiveness and overall performance Integrating user-facing elements into applications Job brief We are looking for a Python Developer to join our engineering team and help us develop and maintain various software products. Python Developer responsibilities include writing and testing code, debugging programs and integrating applications with third-party web services. To be successful in this role, you should have experience using server-side logic and work well in a team. Ultimately, you’ll build highly responsive web applications that align with our business needs. Responsibilities Write effective, scalable code Develop back-end components to improve responsiveness and overall performance Integrate user-facing elements into applications Test and debug programs Improve functionality of existing systems Implement security and data protection solutions Assess and prioritize feature requests Coordinate with internal teams to understand user requirements and provide technical solutions Requirements and skills Work experience as a Python Developer Expertise in at least one popular Python framework (like Django, Flask or Pyramid) Knowledge of object-relational mapping (ORM) Familiarity with front-end technologies (like JavaScript and HTML5) Team spirit Good problem-solving skills BSc in Computer Science, Engineering or relevant field ',\n",
       "   {'entities': [[0, 16, 'JOB TITLE'],\n",
       "     [239, 245, 'MUST HAVE'],\n",
       "     [399, 406, 'GOOD TO HAVE'],\n",
       "     [413, 422, 'GOOD TO HAVE'],\n",
       "     [642, 658, 'MUST HAVE'],\n",
       "     [1238, 1244, 'MUST HAVE'],\n",
       "     [1246, 1251, 'GOOD TO HAVE'],\n",
       "     [1255, 1262, 'GOOD TO HAVE'],\n",
       "     [1355, 1365, 'GOOD TO HAVE']]}],\n",
       "  ['Title : Full Stack Java Developer Job Type : Contract  Location :Onsite(Jersy City,NJ) Client : Goldman Sachs Implementation Partner : NTT DATA Job Description Required Skills: Bachelor’s or master’s degree in computer science, Engineering, or a related field. ',\n",
       "   {'entities': [[8, 18, 'JOB TITLE'],\n",
       "     [72, 82, 'LOCATION'],\n",
       "     [83, 85, 'LOCATION']]}],\n",
       "  ['Core Java developer with 5+ years’ experience in developing, maintaining, and supporting software applications using Java/J2EE, Spring framework, and other related technologies. Must have: Good hands-on experience in Java and J2EE technologies and well versed with features in Java 8 and above. Proficient in Collections, Multi-Threading concepts and should be hands-on. • Experience in UI technologies like Angular, REACT Experience in Spring Boot Hands on development background and recent experience developing and implementing Java based Web Services, primarily in a REST model. Experience building scalable and distributed micro-services. Experience with major open-source tools and frameworks such as Spring, Hibernate, Spring JPA Good understanding of Data structure and Algorithms Experience in messaging tools like Kafka Familiar with Agile software development methodologies Experience with build and development tools like Gradle and Maven Experience in using GIT projects. Experience with Linux / Unix environments Excellent problem-solving skills and communication skills Good to have: Good understanding of SQL and working with relational databases Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure). Good understating on data pipelines using ETL/ELT frameworks and tools (e.g., Informatica, Apache Spark). Familiarity with data modeling techniques and experience with data modeling tools ',\n",
       "   {'entities': [[25, 27, 'EXPERIENCE'],\n",
       "     [128, 134, 'MUST HAVE'],\n",
       "     [217, 221, 'MUST HAVE'],\n",
       "     [226, 230, 'MUST HAVE'],\n",
       "     [408, 415, 'GOOD TO HAVE'],\n",
       "     [417, 422, 'GOOD TO HAVE'],\n",
       "     [571, 575, 'GOOD TO HAVE'],\n",
       "     [707, 713, 'GOOD TO HAVE'],\n",
       "     [715, 724, 'GOOD TO HAVE'],\n",
       "     [726, 736, 'GOOD TO HAVE'],\n",
       "     [824, 829, 'GOOD TO HAVE'],\n",
       "     [844, 849, 'GOOD TO HAVE'],\n",
       "     [1001, 1006, 'MUST HAVE'],\n",
       "     [1121, 1124, 'MUST HAVE'],\n",
       "     [1238, 1241, 'GOOD TO HAVE'],\n",
       "     [1243, 1255, 'GOOD TO HAVE'],\n",
       "     [1257, 1262, 'GOOD TO HAVE'],\n",
       "     [1356, 1368, 'GOOD TO HAVE']]}],\n",
       "  ['Job Role: .Net Developer Must needed skills: ETRM (Energy trading and risk management) Location : Portland, Oregon (Hybrid Role) Weekly 2 days onsite Job Description: The ETRM Techno Functional Consultant will be responsible for providing technical and functional expertise in the Energy Trading and Risk Management (ETRM) domain. The Consultant will be responsible for the design, development, testing, and implementation of ETRM solutions. The Consultant will be required to analyze business requirements, develop technical solutions, and provide support to the end users. The Consultant will be responsible for troubleshooting and resolving technical issues related to the ETRM system. The Consultant will be required to provide training and guidance to the end users on the use of the ETRM system. Skills required. – Proficiency with ETRM Endur application, .Net, C#, SQL Server, Oracle PL/SQL ',\n",
       "   {'entities': [[10, 14, 'JOB TITLE'],\n",
       "     [45, 49, 'MUST HAVE'],\n",
       "     [98, 106, 'LOCATION'],\n",
       "     [108, 114, 'LOCATION'],\n",
       "     [862, 866, 'MUST HAVE'],\n",
       "     [868, 870, 'MUST HAVE'],\n",
       "     [872, 875, 'GOOD TO HAVE'],\n",
       "     [884, 890, 'GOOD TO HAVE']]}],\n",
       "  ['Job Title: product owner.Primary Responsibilities Translate high-level strategy & product direction into features, epics and user stories Understand the business value of requirements profoundly and convey it internally externally Consult and work with project stakeholders to understand business drivers and translate them into functional and non-functional requirements Elicit, capture, analyze, refine and document business requirements and user stories Communicate business and customer value to the team in a manner that sparks innovative thought Work closely with Agile team members Work to define the acceptance criteria for user stories Accept/reject the delivered code from within each Sprint Responsible for making ongoing decisions to continue development down the current path Create Capabilities and Features, collaborating with IT and business stakeholders to groom these components to a state of ready for the Agile teams Take responsibility for the development of ongoing enhancements, creating and prioritizing user stories within the Agile teams\\r\\n\\r\\nAbout You\\r\\n\\r\\n8+ years of experience with Agile and Scrum practices\\r\\nAgile Certified Practitioner (ACP) preferred\\r\\nExperience managing software projects built in .NET, XML and SQL Server preferred (C#?)\\r\\nExperience working with Jira\\r\\nExperience with data governance products (e.g. Varonis, CyberArk), Security/Networking and knowledge of interfacing to storage platforms (Windows, NetApp, EMC, NAS, NFS) is desirable\\r\\nProficient with project management techniques and tools\\r\\nYou can identify, define, analyze, prioritize and refine requirements, communicating milestones and visions to the development team\\r\\nYou can represent the customer with the ability to prioritize tradeoffs, clarify requirements, and work to drive user stories to acceptance throughout the Agile software development life cycle\\r\\nYou have intermediate level of proficiency with Excel and other Microsoft Products\\r\\nYou have experience writing and translating business requirements\\r\\nYou are an analytical person who can control the details but still see the full picture\\r\\nExperience and comfort solving problems in an ambiguous environment where there is constant change\\r\\nYou have the tenacity to thrive in a dynamic and fast-paced environment\\r\\nExperience in the following technologies is a plus: o Storage platforms (Windows, NetApp, EMC lsilon, NAS, NFS), o Device connectors (Unix, Windows, SSCM, ServiceNow, CMDB, Active Directory), o 3rd party interfaces (OneDrive, SharePoint, Microsoft System Center, Active Directory, PeopleSoft, Office 365), o Databases (SQL Server, Oracle or Sybase), o Map Reduce Frameworks (Hadoop)\\r\\n\\r\\n',\n",
       "   {'entities': [[11, 18, 'JOB TITLE'],\n",
       "     [570, 575, 'MUST HAVE'],\n",
       "     [1080, 1082, 'EXPERIENCE'],\n",
       "     [1165, 1168, 'GOOD TO HAVE'],\n",
       "     [1228, 1232, 'GOOD TO HAVE'],\n",
       "     [1234, 1237, 'GOOD TO HAVE'],\n",
       "     [1242, 1245, 'GOOD TO HAVE'],\n",
       "     [1294, 1298, 'MUST HAVE'],\n",
       "     [1367, 1386, 'MUST HAVE'],\n",
       "     [1916, 1921, 'GOOD TO HAVE'],\n",
       "     [2363, 2369, 'GOOD TO HAVE'],\n",
       "     [2371, 2381, 'GOOD TO HAVE'],\n",
       "     [2383, 2386, 'GOOD TO HAVE'],\n",
       "     [2388, 2391, 'GOOD TO HAVE'],\n",
       "     [2415, 2419, 'GOOD TO HAVE'],\n",
       "     [2421, 2428, 'GOOD TO HAVE'],\n",
       "     [2430, 2434, 'GOOD TO HAVE'],\n",
       "     [2436, 2446, 'GOOD TO HAVE'],\n",
       "     [2448, 2452, 'GOOD TO HAVE'],\n",
       "     [2454, 2470, 'GOOD TO HAVE'],\n",
       "     [2497, 2505, 'GOOD TO HAVE'],\n",
       "     [2507, 2517, 'GOOD TO HAVE'],\n",
       "     [2519, 2542, 'GOOD TO HAVE'],\n",
       "     [2544, 2560, 'GOOD TO HAVE'],\n",
       "     [2562, 2572, 'GOOD TO HAVE'],\n",
       "     [2574, 2584, 'GOOD TO HAVE'],\n",
       "     [2600, 2603, 'GOOD TO HAVE'],\n",
       "     [2612, 2618, 'GOOD TO HAVE'],\n",
       "     [2656, 2662, 'GOOD TO HAVE']]}],\n",
       "  [\"Job Title: Scrum Master\\r\\n\\r\\nCountry:\\r\\nUnited States of America\\r\\n\\r\\nPosition Role Type:\\r\\nOnsite\\r\\n\\r\\nAt Raytheon, the foundation of everything we do is rooted in our values and a higher calling – to help our nation and allies defend freedoms and deter aggression. We bring the strength of more than 100 years of experience and renowned engineering expertise to meet the needs of today’s mission and stay ahead of tomorrow’s threat. Our team solves tough, meaningful problems that create a safer, more secure world.\\r\\n\\r\\nJob Summary:\\r\\nRaytheon is looking for Principal Systems Engineer - Scrum Master. In this role, you will lead the Integrated Product Team (IPT) through proper Agile practices within the overall program agile strategy. You will conduct full Agile processes for the Integration and Test IPT including sprint planning, grooming, stand-ups, demos and retrospectives to ensure products integrate with the ground segment and value is provided to the customer and program goals are met. You will work with the team to maintain a healthy backlog of well written stories that are development ready and will work with the team to maintain effective story point estimation. In addition to their Scrum Master role, you will also assist the Integration & Test Lead and Product Owners with the overall integration and test system build and checkout as well as the integrated factory testing portion of the program.\\r\\n\\r\\nResponsibilities to Anticipate:\\r\\nWork with Integration and Test Team and Product Management to create effective story point estimates\\r\\nCommunicate expectations and instill accountability in team members\\r\\nResolve conflicts, promote work sharing, and motivate teams toward common goals\\r\\nMake timely and sound business decisions that support the business objectives\\r\\nManage multiple projects, tasks, and resources through effective organization\\r\\n\\r\\nBasic Qualifications:\\r\\nTypically requires a Bachelor's degree in Science, Technology, Engineering or Mathematics (STEM) and a minimum of 8 years of prior relevant experience unless prohibited by local laws/regulations\\r\\nAgile Scrum Master certification or ability to obtain within 6 months of hire\\r\\nExperience in Agile, Scrum, Kanban and/or project management methodologies\\r\\nExperience with application lifecycle management tools/platforms such as Jira, Confluence, Azure Devops or similar tools\\r\\nExperience with Gherkin, Behave, Python or similar applications\\r\\nActive and transferable U.S. government issued Secret security clearance is required prior to start date. U.S. citizenship is required, as only U.S. citizens are eligible for a security clearance.\\r\\n\\r\\nPreferred Qualifications:\\r\\nExperience with satellite mission management and command and control\\r\\nExperience with PERL or other scripting language\\r\\nExperience with C++, Java, MATLAB or other programming languages\\r\\nExperience with DOORS\\r\\nExperience with Model Based Systems Engineering (MBSE)\\r\\nGood oral and written communication skills\\r\\nExperience in systems engineering functions including large scale system integration, large Scale system test and verification\\r\\nExperience with the Microsoft Office suite of tools (e.g. Word, Excel, and PowerPoint)\\r\\n\\r\\n\",\n",
       "   {'entities': [[11, 23, 'JOB TITLE'],\n",
       "     [99, 107, 'LOCATION'],\n",
       "     [671, 676, 'MUST HAVE'],\n",
       "     [1998, 1999, 'EXPERIENCE'],\n",
       "     [2180, 2185, 'MUST HAVE'],\n",
       "     [2187, 2193, 'GOOD TO HAVE'],\n",
       "     [2201, 2219, 'GOOD TO HAVE'],\n",
       "     [2308, 2312, 'MUST HAVE'],\n",
       "     [2314, 2324, 'MUST HAVE'],\n",
       "     [2332, 2338, 'GOOD TO HAVE'],\n",
       "     [2373, 2380, 'MUST HAVE'],\n",
       "     [2382, 2388, 'MUST HAVE'],\n",
       "     [2390, 2396, 'GOOD TO HAVE'],\n",
       "     [2785, 2788, 'GOOD TO HAVE'],\n",
       "     [2790, 2794, 'GOOD TO HAVE'],\n",
       "     [2796, 2802, 'GOOD TO HAVE'],\n",
       "     [3144, 3148, 'GOOD TO HAVE'],\n",
       "     [3150, 3155, 'GOOD TO HAVE'],\n",
       "     [3161, 3171, 'GOOD TO HAVE']]}],\n",
       "  ['7. \\r\\n\\r\\nJob Title: Senior Data Scientist\\r\\n\\r\\nAbout the job\\r\\nCandidates must have a DoD issued Top Secret clearance with SCI\\r\\n\\r\\n\\r\\nThe candidate shall provide support for the Client’s machine learning model development and deployment efforts specific to its cyber focus and targets. Additionally, The candidate will need to have extensive experience with developing and implementing machine learning methodologies to triage large commercial cyber datasets.\\r\\n\\r\\n \\r\\nThe candidate shall provide support to include the following tasks:\\r\\n• The candidate shall implement machine learning methodologies to triage large commercial datasets.\\r\\n• The candidate shall identify topical, spatial, and time-based trends of interest within large amounts of commercial cyber data.\\r\\n• The candidate shall work with data science models testing, optimization, validation, and tests automation.\\r\\n• The candidate shall be able to integrate machine learning models into software through inference engines, machine learning pipelines, and other approaches.\\r\\n• The candidate shall communicate and work effectively with cross-functional team members including but not limited to data analysts, data scientists, external stakeholders, management, and software solutions integrators.\\r\\n \\r\\nRequired skills and demonstrated experience\\r\\n\\r\\nThe candidate shall have the following required skills, certifications and demonstrated experience:\\r\\n• Demonstrated experience tuning hyper-parameters of existing machine learning models for domain-specific data sets.\\r\\n• Demonstrated experience implementing, evaluating, and extending state-of-the-art, data science methods, data labeling, ETL, and other data standardization practices.\\r\\n• Demonstrated experience integrating user-orientated model evaluation.\\r\\n• Demonstrated experience working with data science models testing, optimization, validation, and tests automation.\\r\\n• Demonstrated experience leveraging model management capabilities to track version control and maintain information about best-performing models, such as MLFLOW or similar.\\r\\n• Demonstrated experience programming in Python.\\r\\n• Demonstrated experience programming in other scripting languages, such as Bash.\\r\\n• Demonstrated experience applying deep learning and machine learning processing libraries, including PyTorch, TensorFlow, Keras, and scikit.\\r\\n• Demonstrated experience using Linux and Windows operating systems.\\r\\n• Demonstrated experience using CUDA and NVIDIA GPU accelerated libraries for AI, machine-learning, and deep learning.\\r\\n• Demonstrated experience with implementing data science workflows in cloud-based platforms (e.g., AWS, Azure, etc.).\\r\\n \\r\\nHighly desired skills and demonstrated experience\\r\\n\\r\\nSkills and demonstrated experiences that are highly desired but not required to perform the work include:\\r\\n• Demonstrated experience developing and deploying machine learning models based on cybersecurity related workflows.\\r\\n• Demonstrated experience working in Client’s mission environment.\\r\\n• Demonstrated experience developing and working with cyber data (e.g. netflow, pcap, credential, ip scans, etc.).\\r\\n\\r\\n',\n",
       "   {'entities': [[18, 39, 'JOB TITLE'],\n",
       "     [379, 395, 'MUST HAVE'],\n",
       "     [812, 819, 'GOOD TO HAVE'],\n",
       "     [821, 833, 'GOOD TO HAVE'],\n",
       "     [835, 845, 'GOOD TO HAVE'],\n",
       "     [995, 1005, 'GOOD TO HAVE'],\n",
       "     [1547, 1559, 'GOOD TO HAVE'],\n",
       "     [1642, 1645, 'GOOD TO HAVE'],\n",
       "     [2096, 2103, 'MUST HAVE'],\n",
       "     [2290, 2297, 'GOOD TO HAVE'],\n",
       "     [2299, 2309, 'GOOD TO HAVE'],\n",
       "     [2311, 2316, 'GOOD TO HAVE'],\n",
       "     [2322, 2329, 'GOOD TO HAVE'],\n",
       "     [2449, 2452, 'GOOD TO HAVE'],\n",
       "     [2620, 2623, 'GOOD TO HAVE'],\n",
       "     [2625, 2630, 'GOOD TO HAVE']]}],\n",
       "  ['8. \\r\\n\\r\\nTitle: \\r\\n\\r\\nSenior AI Engineer\\r\\n\\r\\nAbout the job\\r\\nLocation: KING OF PRUSSIA, PA, USA\\r\\n\\r\\nSalary: Depends on Experience\\r\\n\\r\\nDescription\\r\\n\\r\\nSr. AI Engineer is responsible for designing and implementing AI based solutions. The ideal candidate can demonstrate a high-level of technical expertise, and will have excellent planning, coordination and communication skills.\\r\\n\\r\\nPrimary Duties & Responsibilities\\r\\n\\r\\nImplement AI processes across the organization.\\r\\nAssist with setting up on-prem and cloud infrastructure required for implementing AI solutions.\\r\\nBuild AI models to make predictions.\\r\\nConvert models to APIs for other processes to use.\\r\\nCollaborate with Data Science team to provide them the output from AI products.\\r\\nCollaborate with other development teams as required to achieve business results.\\r\\nReview business requirements and provide recommendations where architectural oversight is needed.\\r\\nRecommend best practices and standards for AI products, services and applications.\\r\\n\\r\\nQualifications\\r\\n\\r\\nAt least 7 years of overall software development experience writing software products and/or services.\\r\\nProven experience building enterprise level AI products and services.\\r\\nProven experience designing, implementing and using Azure cloud services/ framework to support AI products and services.\\r\\nProven experience with machine learning and model building.\\r\\nExperience with NLP and image processing.\\r\\nExperience with Azure Speech, Azure AI Vision/ Custom Vision, Azure Document Intelligence, Azure Conversational AI services will be a plus.\\r\\nExperience with UiPath a plus.\\r\\nAbility to work the full life-cycle of a software development project.\\r\\nExcellent programming skills in C#/ .NET and other related Microsoft technologies.\\r\\nStrong experience with SQL/ NoSQL databases.\\r\\nStrong object oriented programming and design skills.\\r\\nDemonstrated problem-solving skills.\\r\\nWilling to learn new technologies.\\r\\nDeliver high quality software without compromise.\\r\\nExperience with the healthcare industry and workers’ compensation is a plus.\\r\\nExcellent communication skills and the ability to work across multiple teams.\\r\\nExcellent documentation skills in order to plan, create, track and sustain AI engineering work.\\r\\n\\r\\n',\n",
       "   {'entities': [[18, 36, 'MUST HAVE'],\n",
       "     [65, 80, 'LOCATION'],\n",
       "     [82, 84, 'LOCATION'],\n",
       "     [86, 89, 'LOCATION'],\n",
       "     [611, 615, 'MUST HAVE'],\n",
       "     [1021, 1022, 'EXPERIENCE'],\n",
       "     [1386, 1389, 'MUST HAVE'],\n",
       "     [1394, 1411, 'MUST HAVE'],\n",
       "     [1429, 1441, 'GOOD TO HAVE'],\n",
       "     [1443, 1473, 'GOOD TO HAVE'],\n",
       "     [1475, 1502, 'GOOD TO HAVE'],\n",
       "     [1504, 1527, 'GOOD TO HAVE'],\n",
       "     [1690, 1692, 'GOOD TO HAVE'],\n",
       "     [1694, 1698, 'GOOD TO HAVE'],\n",
       "     [1765, 1769, 'GOOD TO HAVE']]}],\n",
       "  ['9. \\r\\n\\r\\nJob Title: Test Manager\\r\\n\\r\\nThe Successful Candidate Will Have a Background In:\\r\\n\\r\\n\\r\\n\\r\\nExperience in software and/or process testing.\\r\\nExperience with test automation tools.\\r\\nExperience within Digital Image Processing or Business Process Services environment.\\r\\nKnowledge of software QA methodologies, tools, and processes.\\r\\nAPI level test automation expertise.\\r\\n\\r\\n\\r\\nResponsibilities:\\r\\n\\r\\nDevelop, implement, and maintain testing strategies and plans to ensure solution quality.\\r\\nEvaluate, select, and integrate the best test automation tools to improve testing efficiency.\\r\\nWork closely with various internal and client teams to understand solution requirements and define test coverage.\\r\\nOversee the execution of manual and automated test cases ensuring they are completed on time and fully documented.\\r\\nSupport test script development and the optimization of those scripts.\\r\\nProvide regular updates on test progress, risks, and quality metrics to stakeholders.\\r\\nLead ongoing test reviews to gather insights and drive continuous improvement.\\r\\nCoordinate the execution of periodic client business continuity tests.\\r\\nSupport client driven testing projects as needed.\\r\\n\\r\\n\\r\\nQualifications:\\r\\n\\r\\nMinimum 5 years of experience in software and/or process testing.\\r\\nStrong organization and documentation skills\\r\\nExperience with test automation tools.\\r\\nStrong knowledge of software QA methodologies, tools, and processes.\\r\\nExceptional analytical and problem-solving skills.\\r\\nExcellent communication and interpersonal skills.\\r\\nAbility to work in a fast-paced environment and handle multiple priorities.\\r\\n\\r\\n\\r\\nSkills Requirements:\\r\\n\\r\\nBachelor’s degree in Computer Science, Information Technology, or a related field.\\r\\nExperience within Digital Image Processing or Business Process Services environment.\\r\\nFamiliarity with Agile/Scrum development processes.\\r\\nAPI level test automation expertise\\r\\nExperience with data file transmission formats (XML and JSON)\\r\\nGeneral understanding of cloud resources (AWS and/or Azure)\\r\\n\\r\\n',\n",
       "   {'entities': [[18, 30, 'JOB TITLE'],\n",
       "     [207, 223, 'GOOD TO HAVE'],\n",
       "     [227, 243, 'GOOD TO HAVE'],\n",
       "     [289, 291, 'MUST HAVE'],\n",
       "     [330, 333, 'MUST HAVE'],\n",
       "     [1203, 1204, 'EXPERIENCE'],\n",
       "     [1813, 1824, 'MUST HAVE'],\n",
       "     [1934, 1937, 'MUST HAVE'],\n",
       "     [1942, 1946, 'MUST HAVE'],\n",
       "     [1991, 1994, 'GOOD TO HAVE'],\n",
       "     [2002, 2007, 'GOOD TO HAVE']]}],\n",
       "  [\"10. \\r\\n\\r\\nJob Title: Product Manager, Cloud\\r\\n\\r\\nAbout the role:\\r\\n\\r\\nModular is building a next-generation AI infrastructure platform that connects the many application frameworks and hardware backends, simplifying deployment for AI production teams and accelerating innovation for AI researchers and hardware developers. We are looking for someone to drive the development and success of Modular’s compute platform – defining and developing a cloud-based AI inference system that provides state-of-the-art performance, scalability, usability, and program portability to customers. In this role, you will work closely with our cloud development team and interface with the world’s leading AI companies. Join our world-leading product team and redefine how AI infrastructure is built and deployed.\\r\\n\\r\\n\\r\\nLOCATION: Candidates based in the US or Canada are welcome to apply. You can work remotely from home.\\r\\n\\r\\n\\r\\nWhat you will do:\\r\\n\\r\\nDefine a cloud product strategy that aligns with Modular’s overall business goals and objectives.\\r\\nWork closely with Modular’s development team and world leading AI application developers to define, design, and develop a state-of-the-art cloud-based AI inference system.\\r\\nCreate and maintain product roadmaps, feature prioritization, and release schedules.\\r\\nCollaborate with marketing, design, and other cross-functional teams to ensure the success of the cloud-based AI inference system.\\r\\nConduct market research and gather customer feedback to identify new opportunities and drive innovation.\\r\\nHave a growth and leadership mindset, with an attitude that seeks to learn more from our customers, team, and the broader market.\\r\\n\\r\\n\\r\\nWhat you bring to the table:\\r\\n\\r\\n7+ years product management, including significant time working on a technical product within Cloud, AI, or Data, or experience as a former software/AI developer.\\r\\nKnowledge of cloud technologies and architectures, and/or past experience working on AI infrastructure/cloud products and platforms like TensorFlow, PyTorch, TensorRT, CUDA, Triton Inference Server, VertexAI, Sagemaker is a must.\\r\\nExperience driving product vision, go-to-market strategy, and design discussions.\\r\\nExperience creating strategic product roadmaps and working with cross-functional teams.\\r\\nExperience with market research and customer feedback to drive product innovation.\\r\\nStrong analytical and problem-solving skills.\\r\\n\\r\\n\\r\\nHelpful experience (not required):\\r\\n\\r\\nMaster's degree or Ph.D. in Computer Science or related technical field.\\r\\nExperience developing, training, and/or deploying machine learning models into production environments.\\r\\nKnowledge of multiple functional areas (e.g., Product Management, Engineering, UX/UI, or Marketing).\\r\\nAbility to influence multiple stakeholders without direct authority.\\r\\nHave worked in or around startups before or have a strong understanding of the nature of fast-moving, highly dynamic teams.\\r\\n\",\n",
       "   {'entities': [[19, 34, 'JOB TITLE'],\n",
       "     [1688, 1690, 'EXPERIENCE'],\n",
       "     [1782, 1787, 'MUST HAVE'],\n",
       "     [1789, 1791, 'MUST HAVE'],\n",
       "     [1828, 1850, 'GOOD TO HAVE'],\n",
       "     [1989, 1999, 'GOOD TO HAVE'],\n",
       "     [2001, 2008, 'GOOD TO HAVE'],\n",
       "     [2010, 2018, 'GOOD TO HAVE'],\n",
       "     [2020, 2024, 'GOOD TO HAVE'],\n",
       "     [2026, 2049, 'GOOD TO HAVE'],\n",
       "     [2051, 2059, 'GOOD TO HAVE'],\n",
       "     [2061, 2070, 'GOOD TO HAVE'],\n",
       "     [2552, 2568, 'MUST HAVE']]}],\n",
       "  [\"SAP BTP Architect In simple terms, the SAP BTP Architect is like the mastermind behind making sure a company's SAP software works seamlessly. They focus on the SAP Business Technology Platform (BTP), specifically its Integration Suite. This involves designing, implementing, and overseeing the ongoing management of advanced SAP solutions. Working closely with different teams, the architect figures out what the business needs, creates a plan for how to make it happen using SAP BTP, and then ensures that the plan is put into action smoothly. They also keep an eye on the latest SAP BTP technologies and trends, providing guidance to other teams. The architect is a key player in managing technical risks, making sure projects stay on track, and documenting everything so that others can understand and use the solutions. They should have a strong background in SAP implementation and architecture, deep knowledge of SAP BTP technologies, and excellent communication and leadership skills. Experience with agile development and DevOps practices, as well as relevant SAP certifications, is a big plus.\\r\\n\\r\\n\",\n",
       "   {'entities': [[0, 17, 'JOB TITLE'],\n",
       "     [194, 197, 'MUST HAVE'],\n",
       "     [1008, 1013, 'MUST HAVE'],\n",
       "     [1030, 1036, 'MUST HAVE']]}],\n",
       "  [\"SAP Logistic Solution architect – Offshore – 7A/6BSAP Logistic Solution Architect role involves overseeing and optimizing the way a company uses SAP software for its logistics and supply chain operations. This includes ensuring that the SAP solutions align with business goals, are efficient, secure, and stay up-to-date with the latest technologies.As part of the job, the architect defines the scope of new software releases, evaluates and challenges proposed solutions from vendors, and actively collaborates with various teams and stakeholders. They play a key role in translating business needs into IT requirements, configuring and building technical solutions, and testing them to ensure they meet quality standards.The architect is also responsible for maintaining the integrity of the software platforms, preventing unnecessary customization, and staying informed about digital market trends. With a solid background in SAP SD (Sales and Distribution) and integration with other modules, they bring expertise in areas like SAP S/4 HANA configuration, cloud concepts, and agile methodologies.The technical background and experience required for the role include a deep understanding of SAP technologies, multiple end-to-end implementation experiences, and the ability to manage vendor releases and assess solution designs. Overall, the SAP Logistic Solution Architect is a seasoned professional who ensures that the company's logistics and supply chain processes run smoothly and effectively with the help of SAP solutions.In simple terms, this job is about finding a skilled individual, possibly from a known company, who's an expert in Drupal (the last 2-3 versions). This person will be a one-person team responsible for designing and setting up an internal portal, which is like a private website for a company's internal use.The job involves creating and integrating various elements on the portal, such as web parts and plugins, and ensuring everything works smoothly. The person should be proficient in technologies like MySQL, PHP, HTML, CSS, and JavaScript. They'll need to have excellent communication skills, both written and verbal.The work hours are from 12 PM to 9 PM, and the job includes deploying the portal in testing and production environments. It's a long-term commitment, possibly starting immediately, and there's an option for a contract-to-hire arrangement.\\r\\n\\r\\n\",\n",
       "   {'entities': [[0, 31, 'JOB TITLE'],\n",
       "     [34, 42, 'LOCATION'],\n",
       "     [237, 240, 'MUST HAVE'],\n",
       "     [937, 942, 'MUST HAVE'],\n",
       "     [947, 959, 'MUST HAVE'],\n",
       "     [1032, 1044, 'MUST HAVE'],\n",
       "     [1060, 1065, 'GOOD TO HAVE'],\n",
       "     [1080, 1085, 'GOOD TO HAVE'],\n",
       "     [1646, 1652, 'MUST HAVE'],\n",
       "     [2036, 2041, 'MUST HAVE'],\n",
       "     [2043, 2046, 'MUST HAVE'],\n",
       "     [2048, 2052, 'MUST HAVE'],\n",
       "     [2054, 2057, 'MUST HAVE'],\n",
       "     [2063, 2074, 'MUST HAVE']]}],\n",
       "  [\"Drupal expertIn simple terms, this job is about finding a skilled individual, possibly from a known company, who's an expert in Drupal (the last 2-3 versions). This person will be a one-person team responsible for designing and setting up an internal portal, which is like a private website for a company's internal use.The job involves creating and integrating various elements on the portal, such as web parts and plugins, and ensuring everything works smoothly. The person should be proficient in technologies like MySQL, PHP, HTML, CSS, and JavaScript. They'll need to have excellent communication skills, both written and verbal.The work hours are from 12 PM to 9 PM, and the job includes deploying the portal in testing and production environments. It's a long-term commitment, possibly starting immediately, and there's an option for a contract-to-hire arrangement.\\r\\n\\r\\n\",\n",
       "   {'entities': [[0, 15, 'JOB TITLE'],\n",
       "     [128, 134, 'MUST HAVE'],\n",
       "     [518, 523, 'MUST HAVE'],\n",
       "     [525, 528, 'MUST HAVE'],\n",
       "     [530, 534, 'MUST HAVE'],\n",
       "     [536, 539, 'MUST HAVE'],\n",
       "     [545, 556, 'MUST HAVE']]}],\n",
       "  ['Databricks Solution Architect RoleThe client is seeking a Databricks Solution Architect withexperience in a technical role, focusing on data governance, data warehousing, and setting up DataBricks as a Service model. The role requires expertise in production deployment of data governance solutions and hands-on experience with cloud data lakes. The architect should have a strong background in designing and implementing data warehousing technologies, with deep specialty expertise in scaling big data workloads for optimal performance and cost-effectiveness, including technologies like Delta Lake. Additionally, the architect will support customers by creating reference architectures, how-to guides, and demo applications, collaborating with Enterprise Accounts, and integrating Databricks with third-party applications to align with customer architectures. Experience in designing and implementing cloud-based architectures (AWS, Azure, or GCP) is crucial, along with excellent communication skills.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 29, 'JOB TITLE'],\n",
       "     [186, 196, 'MUST HAVE'],\n",
       "     [334, 345, 'GOOD TO HAVE'],\n",
       "     [589, 600, 'MUST HAVE'],\n",
       "     [903, 928, 'MUST HAVE'],\n",
       "     [930, 933, 'GOOD TO HAVE'],\n",
       "     [935, 940, 'GOOD TO HAVE'],\n",
       "     [945, 948, 'GOOD TO HAVE']]}],\n",
       "  ['GCP DATA ENGINEERThe client is in need of a GCP Data Engineer with expertise in GCP data analytics services, including Dataflow, Data Fusion, BigQuery, and data storage. The primary responsibility is to develop a Google Kubernetes Engine (GKE) to efficiently connect and manage data from various sources. The role also involves conducting unit tests and validating the graph lineage, ensuring the integrity and accuracy of data processing and transformation within the GCP environment.\\r\\n',\n",
       "   {'entities': [[0, 20, 'JOB TITLE'],\n",
       "     [44, 47, 'MUST HAVE'],\n",
       "     [119, 127, 'GOOD TO HAVE'],\n",
       "     [129, 140, 'GOOD TO HAVE'],\n",
       "     [142, 150, 'GOOD TO HAVE'],\n",
       "     [220, 230, 'MUST HAVE']]}],\n",
       "  ['SNOWFLAKE ADMINThe client is seeking a Snowflake Admin with a wide range of responsibilities to ensure the effective and optimized utilization of Snowflake technology. The role includes configuring and customizing Snowflake based on user stories and acceptance criteria, as well as providing technical documentation and design documents. The Snowflake Admin will be involved in setting up and migrating data warehouses, designing complex data models, and optimizing Snowflake for performance. They will also work directly with customers to demonstrate best practices, provide guidance for technical challenges, and lead the implementation of high-volume data analytics and machine learning solutions in the cloud. The role entails designing features, integrating APIs, and ensuring high-quality, reliable software to meet customer requirements and business goals.\\r\\n',\n",
       "   {'entities': [[0, 18, 'JOB TITLE'],\n",
       "     [146, 155, 'MUST HAVE'],\n",
       "     [408, 418, 'GOOD TO HAVE'],\n",
       "     [438, 449, 'GOOD TO HAVE'],\n",
       "     [673, 689, 'MUST HAVE'],\n",
       "     [763, 767, 'GOOD TO HAVE']]}],\n",
       "  [\"FULL STACK DEVELOPERThe client is looking for a Full Stack Developer responsible for creating and maintaining both the user interface (UI) components using Angular and the server-side network components. This role requires expertise in developing APIs using Node.js, following microservices architecture, and integrating APIs from various technologies, including Java, Python, and third-party sources. The developer should have a strong focus on building high-performance applications through the creation of testable, reusable, and efficient code. Additionally, the candidate should be capable of handling production deployments and promptly addressing defects while having a clear understanding of various platform components such as caching, proxies, and routing to ensure the system's functionality and performance.\\r\\n\",\n",
       "   {'entities': [[0, 10, 'JOB TITLE'],\n",
       "     [156, 163, 'MUST HAVE'],\n",
       "     [258, 265, 'MUST HAVE'],\n",
       "     [363, 368, 'GOOD TO HAVE'],\n",
       "     [369, 375, 'GOOD TO HAVE'],\n",
       "     [736, 743, 'GOOD TO HAVE'],\n",
       "     [745, 752, 'GOOD TO HAVE']]}],\n",
       "  ['AWS MSKThe client is seeking an AWS professional with a strong background in Kafka Streams Development and expertise in AWS MSK (Managed Streaming for Apache Kafka). The primary focus is on developing cloud-native applications on the AWS platform that involve real-time streaming data pipelines and near real-time big data analytics. The ideal candidate should possess hands-on experience in working with Kafka Schemas and the use of the Schema Registry, as well as configuring, optimizing, and ensuring fault tolerance within Kafka clusters. Additionally, the role involves a strong understanding of Kafka client configuration and the ability to troubleshoot related issues to ensure the smooth operation of Kafka ecosystems within the AWS environment.\\r\\n',\n",
       "   {'entities': [[32, 35, 'MUST HAVE'],\n",
       "     [77, 82, 'MUST HAVE'],\n",
       "     [120, 127, 'JOB TITLE'],\n",
       "     [314, 333, 'GOOD TO HAVE'],\n",
       "     [438, 453, 'GOOD TO HAVE'],\n",
       "     [466, 477, 'GOOD TO HAVE'],\n",
       "     [479, 489, 'GOOD TO HAVE'],\n",
       "     [504, 519, 'GOOD TO HAVE']]}],\n",
       "  ['SHAREPOINT DEVELOPERThe client is looking for a SharePoint Developer to design and create an Intranet Portal Collaboration Site with a focus on branding and user experience. The developer will be responsible for building a new landing page, department templates, newsletters, banners, and ensuring mobile-friendly videos. The project involves creating, deploying, and administering a new intranet portal site using SharePoint. The developer will also design website elements using SharePoint Framework SPx, HTML5, JavaScript, Typescript, jQuery, CSS, SQL, Bootstrap, and responsive design to ensure compatibility across all devices. Additionally, tasks include building web components, managing content, permissions, and using tools like SharePoint Designer and PowerApps/Flow for workflow forms. Experience with data migration tools to transfer data between SharePoint Online and other SharePoint versions is also required.\\r\\n',\n",
       "   {'entities': [[0, 23, 'JOB TITLE'],\n",
       "     [502, 505, 'MUST HAVE'],\n",
       "     [507, 512, 'MUST HAVE'],\n",
       "     [514, 524, 'MUST HAVE'],\n",
       "     [526, 536, 'GOOD TO HAVE'],\n",
       "     [538, 544, 'GOOD TO HAVE'],\n",
       "     [546, 549, 'GOOD TO HAVE'],\n",
       "     [551, 554, 'MUST HAVE'],\n",
       "     [556, 565, 'GOOD TO HAVE']]}],\n",
       "  ['AI LLM DATA SCIENCE:The client is seeking an AI LLM Data Scientist with extensive experience in data science, machine learning, and a strong focus on NLP technologies. a deep understanding of Transformer Encoder Networks. They should be adept at applying deep learning and generative modeling techniques, particularly in the field of Artificial Intelligence and generative models like GANs, VAEs, and transformer-based architectures. The role involves designing and implementing state-of-the-art generative models for various NLP tasks, collaborating with cross-functional teams, and staying updated on the latest advancements in generative AI and LLM. While contributions to the research community are a plus, the primary responsibilities include evaluating and preprocessing large-scale datasets, ensuring data quality and integrity, developing data pipelines for training and evaluation, and deploying and optimizing generative models in production environments. The client also values the ability to articulate model effects to business stakeholders, develop guardrails for LLMs, and collaborate with software engineers for scalability and efficiency. Additionally, the candidate should have the capacity to provide guidance to junior data scientists and contribute to the growth and success of the data science team.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 19, 'JOB TITLE'],\n",
       "     [110, 126, 'MUST HAVE'],\n",
       "     [150, 153, 'MUST HAVE'],\n",
       "     [192, 203, 'GOOD TO HAVE'],\n",
       "     [385, 389, 'GOOD TO HAVE'],\n",
       "     [391, 395, 'GOOD TO HAVE'],\n",
       "     [641, 643, 'MUST HAVE'],\n",
       "     [648, 652, 'MUST HAVE']]}],\n",
       "  ['AZURE MLOPS & DEVOPS:The client is seeking an Azure MLOps and DevOps expert with specific qualifications. The ideal candidate should be proficient in Terraform and demonstrate a willingness to quickly adapt to client-specific frameworks and standards. They should have expertise in Azure services and deployments and have experience in MLOps using tools like Argo CD and DevOps with Jenkins. The role involves deploying AI, ML, and LLM-related solutions and service provisioning, as well as deploying various applications such as Node.js, React, Angular, and Java. Experience in creating Kubernetes clusters, nodes, setup, and Docker images is essential. Knowledge of Python and the ability to implement monitoring and observability solutions are also required. Strong written and verbal communication skills are important, and the candidate should be capable of working as an individual contributor. In summary, the client is looking for a versatile professional who can excel in Azure MLOps and DevOps while being adaptable to specific client requirements and standards.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 20, 'JOB TITLE'],\n",
       "     [46, 51, 'MUST HAVE'],\n",
       "     [364, 366, 'GOOD TO HAVE'],\n",
       "     [371, 377, 'GOOD TO HAVE'],\n",
       "     [383, 391, 'MUST HAVE'],\n",
       "     [530, 537, 'GOOD TO HAVE'],\n",
       "     [539, 544, 'GOOD TO HAVE'],\n",
       "     [546, 553, 'GOOD TO HAVE'],\n",
       "     [559, 564, 'GOOD TO HAVE'],\n",
       "     [588, 598, 'MUST HAVE'],\n",
       "     [609, 614, 'GOOD TO HAVE'],\n",
       "     [627, 633, 'MUST HAVE'],\n",
       "     [668, 674, 'MUST HAVE']]}],\n",
       "  ['AWS DEVOPS ENGINEERThe client is looking for an experienced AWS DevOps Engineer including a minimum of 2 implementations of DevOps on the AWS cloud platform. The candidate should have a strong background in building and maintaining CI/CD pipelines on AWS using tools like Jenkins. Additionally, the role requires expertise in Python, Lambda integration, Airflow, and MLflow, with a specific focus on automating ML model operations. Knowledge of technologies such as JavaScript, Kubernetes, Docker, and Python is important for this position. Effective oral and written communication skills are also essential. In summary, the client needs a skilled DevOps Engineer with AWS experience who can contribute to the automation and management of CI/CD pipelines and ML model operations on the AWS platform.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 10, 'JOB TITLE'],\n",
       "     [60, 63, 'MUST HAVE'],\n",
       "     [103, 104, 'EXPERIENCE'],\n",
       "     [232, 237, 'MUST HAVE'],\n",
       "     [326, 332, 'MUST HAVE'],\n",
       "     [334, 352, 'MUST HAVE'],\n",
       "     [354, 361, 'MUST HAVE'],\n",
       "     [367, 373, 'MUST HAVE'],\n",
       "     [466, 476, 'GOOD TO HAVE'],\n",
       "     [478, 488, 'GOOD TO HAVE'],\n",
       "     [490, 496, 'GOOD TO HAVE'],\n",
       "     [502, 508, 'GOOD TO HAVE']]}],\n",
       "  ['SNOWFLAKE DATA ENGINEER:The client is looking for a Snowflake Data Engineer  IT experience, hands-on experience as a Snowflake Data Engineer. The candidate should have expertise in implementing end-to-end Snowflake cloud data warehousing solutions on Amazon Web Services (AWS), and experience with Snowflake migration projects on AWS. Proficiency in programming languages such as Python, Pyspark, Scala, and Scalaspark is required, and a strong understanding of the BIG DATA ecosystem, including Hadoop, HIVE, structured data processing, semi-structured data processing, and unstructured data processing, is essential. The candidate should also have hands-on experience with Snowflake utilities, Snowpark API, SnowSQL, and SnowPipe, as well as expertise in Snowflake data modeling, ELT using Snowflake SQL, implementing complex stored procedures, and standard data warehousing and ETL concepts. Familiarity with data security and access controls, RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, and performance tuning is important. Good communication skills, strong analytical abilities, and effective troubleshooting skills are also required for this role. In summary, the client is seeking an experienced Snowflake Data Engineer with a comprehensive skill set to work on Snowflake cloud data warehousing projects with a focus on AWS.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 23, 'JOB TITLE'],\n",
       "     [52, 61, 'MUST HAVE'],\n",
       "     [272, 275, 'GOOD TO HAVE'],\n",
       "     [380, 386, 'MUST HAVE'],\n",
       "     [388, 395, 'MUST HAVE'],\n",
       "     [397, 402, 'GOOD TO HAVE'],\n",
       "     [408, 418, 'GOOD TO HAVE'],\n",
       "     [496, 502, 'MUST HAVE'],\n",
       "     [504, 508, 'GOOD TO HAVE'],\n",
       "     [696, 708, 'GOOD TO HAVE'],\n",
       "     [710, 717, 'GOOD TO HAVE'],\n",
       "     [723, 731, 'GOOD TO HAVE'],\n",
       "     [962, 965, 'MUST HAVE'],\n",
       "     [975, 979, 'MUST HAVE']]}],\n",
       "  ['SAP HANA LEADThe client is seeking an experienced SAP HANA Lead of SAP HANA development experience. This role requires leadership and team-handling experience within the HANA domain and mandates a minimum of 2 End to End S/4 HANA implementations. The ideal candidate should have extensive hands-on experience in SAP HANA design and modeling, with a focus on calculation views, stored procedures, scalar and table functions, and XSO Data. Proficiency in SAP Native HANA SQL script is essential. The candidate should also have project experience in modeling concepts, including creating calculation views, SAP HANA SQL, SQL Script and Procedures, currency conversion, and translating business rules into decision tables. Collaboration with functional consultants to develop optimal solutions is a key aspect of this role, requiring a general understanding of functional processes. Effective verbal and written communication skills are also crucial for this position. In summary, the client is looking for a seasoned SAP HANA professional with leadership experience, strong modeling skills, and the ability to work closely with functional consultants to deliver effective solutions.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 8, 'JOB TITLE'],\n",
       "     [50, 53, 'MUST HAVE'],\n",
       "     [208, 209, 'EXPERIENCE'],\n",
       "     [396, 402, 'GOOD TO HAVE'],\n",
       "     [407, 422, 'GOOD TO HAVE'],\n",
       "     [428, 431, 'GOOD TO HAVE'],\n",
       "     [469, 472, 'MUST HAVE'],\n",
       "     [645, 664, 'GOOD TO HAVE']]}],\n",
       "  ['SAP CPI/PI/POThe client is seeking an experienced SAP CPI/PI/PO professional of SAP PO development experience, including a minimum of 2 End to End SAP CPI implementations. This role requires team leadership and handling experience within the SAP CPI/PI/PO domain. The ideal candidate should have a strong working knowledge of SAP CPI, PI, SAP Architecture, and interfacing concepts. They should be proficient in developing interfaces using SAP PI standard adaptors such as OData, RFC, Proxies, IDOC, SOAP, REST, and JDBC. Moreover, the candidate should have a solid background in integrating third-party systems using APIs. Strong skills in graphical mapping, Java mapping, XSLT mapping, and user-defined function (UDF) development are essential. Experience in ALE/IDoc, Proxies, and RFCs within the SAP ECC environment is also required. This role involves close collaboration with functional consultants and third-party vendors to develop effective solutions, making a general understanding of functional processes important. Effective verbal and written communication skills are crucial for this position. In summary, the client is looking for a seasoned SAP professional capable of leading teams, developing interfaces, and collaborating with stakeholders to deliver optimal solutions in the SAP CPI/PI/PO landscape.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 16, 'JOB TITLE'],\n",
       "     [134, 135, 'EXPERIENCE'],\n",
       "     [339, 342, 'MUST HAVE'],\n",
       "     [473, 478, 'GOOD TO HAVE'],\n",
       "     [480, 483, 'GOOD TO HAVE'],\n",
       "     [485, 492, 'GOOD TO HAVE'],\n",
       "     [494, 498, 'GOOD TO HAVE'],\n",
       "     [500, 504, 'MUST HAVE'],\n",
       "     [506, 510, 'MUST HAVE'],\n",
       "     [516, 521, 'MUST HAVE'],\n",
       "     [660, 672, 'MUST HAVE'],\n",
       "     [674, 686, 'GOOD TO HAVE'],\n",
       "     [761, 769, 'GOOD TO HAVE'],\n",
       "     [771, 778, 'GOOD TO HAVE'],\n",
       "     [784, 788, 'GOOD TO HAVE']]}],\n",
       "  ['STIIM DEVELOPERThe client is looking for a STIIM Developer IT experience who possesses a specific skill set. The ideal candidate should be proficient in Python and have a strong command of the Google Cloud stack, including BigQuery, Dataproc, and Dataflow. Additionally, expertise in Linux scripting and a good understanding of big data technologies, particularly PySpark, are required. Strong analytical skills and the ability to write complex SQL queries based on business requirements are essential. Effective communication skills are also a prerequisite for this role. In summary, the client needs a seasoned IT professional who can effectively leverage Python and Google Cloud tools for data processing and analysis, coupled with strong analytical, database, and communication skills.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 18, 'JOB TITLE'],\n",
       "     [223, 231, 'GOOD TO HAVE'],\n",
       "     [284, 289, 'MUST HAVE'],\n",
       "     [364, 371, 'GOOD TO HAVE'],\n",
       "     [445, 448, 'MUST HAVE'],\n",
       "     [658, 664, 'MUST HAVE']]}],\n",
       "  ['AZURE CLOUD ADMINThe client is looking for an Azure Cloud Admin with expertise in designing and building cloud infrastructure on Microsoft Azure. The responsibilities include developing hybrid cloud reference architecture, defining target state cloud architecture, and experience with private and public cloud architectures. The candidate should have hands-on experience in migrating workloads from on-premises to Azure, provisioning infrastructure, and executing cutover plans to minimize system downtime. Proficiency in workload migration using automation tools like CloudEndure, Azure Application Migration Service, and Azure Database Migration Service is essential. Knowledge of infrastructure provisioning and management tools like Ansible, Chef, Puppet, Terraform, and CloudFormation is required. Familiarity with Cloud Adoption Frameworks and Azure Well-Architected Framework, as well as container technologies like Docker and Kubernetes, is expected. Understanding of DevOps processes and CI/CD tools, microservices, and serverless architecture is a plus. The candidate should also have networking knowledge in Azure services, RedHat Linux and Windows OS expertise, strong problem-solving skills, and the ability to implement high availability (HA) and disaster recovery (DR) concepts on Azure. Creating proof-of-concepts to demonstrate the viability of solutions is part of the role. In summary, the client is seeking an Azure Cloud Admin with a comprehensive skill set to design, build, and manage cloud infrastructure on the Azure platform while ensuring efficient and secure operations.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 20, 'JOB TITLE'],\n",
       "     [46, 51, 'MUST HAVE'],\n",
       "     [569, 580, 'GOOD TO HAVE'],\n",
       "     [582, 599, 'GOOD TO HAVE'],\n",
       "     [600, 617, 'GOOD TO HAVE'],\n",
       "     [623, 655, 'GOOD TO HAVE'],\n",
       "     [923, 929, 'MUST HAVE'],\n",
       "     [934, 944, 'MUST HAVE'],\n",
       "     [976, 982, 'MUST HAVE'],\n",
       "     [997, 1002, 'MUST HAVE'],\n",
       "     [1135, 1141, 'GOOD TO HAVE'],\n",
       "     [1152, 1159, 'GOOD TO HAVE']]}],\n",
       "  ['DATA ENGINEERThe client is seeking a Data Engineer with 5-8 years of experience in architecting and building large-scale, distributed big data solutions, with a focus on at least 2-3 Big Data implementation projects. The ideal candidate should have knowledge and experience with cloud platforms, particularly AWS cloud services such as EC2, EMR, RDS, and Redshift. A strong background in the Hadoop ecosystem, including HDFS, Hive, Spark, Sqoop, Kafka, NiFi, and real-time streaming technologies, is essential, and experience with the Cloudera distribution is preferred. Proficiency in programming languages like Python, PySpark, and Java is a must. Excellent communication skills, strong analytical abilities, and troubleshooting skills are expected. Experience in team leadership and workload management is also desirable, and any experience with tools like Informatica BDM and StreamSets would be a plus. In summary, the client is looking for an experienced Data Engineer with a strong background in big data technologies and cloud platforms to design and implement data solutions for their organization.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 16, 'JOB TITLE'],\n",
       "     [56, 59, 'EXPERIENCE'],\n",
       "     [309, 312, 'MUST HAVE'],\n",
       "     [336, 339, 'GOOD TO HAVE'],\n",
       "     [341, 344, 'GOOD TO HAVE'],\n",
       "     [346, 349, 'GOOD TO HAVE'],\n",
       "     [355, 364, 'GOOD TO HAVE'],\n",
       "     [392, 398, 'MUST HAVE'],\n",
       "     [420, 424, 'GOOD TO HAVE'],\n",
       "     [426, 430, 'GOOD TO HAVE'],\n",
       "     [432, 437, 'GOOD TO HAVE'],\n",
       "     [439, 444, 'GOOD TO HAVE'],\n",
       "     [446, 451, 'GOOD TO HAVE'],\n",
       "     [453, 457, 'GOOD TO HAVE'],\n",
       "     [613, 619, 'MUST HAVE'],\n",
       "     [621, 628, 'MUST HAVE'],\n",
       "     [634, 638, 'GOOD TO HAVE']]}],\n",
       "  ['SNOWFLAKE ARCHITECTThe client is in need of a Snowflake Architect The key responsibilities involve designing end-to-end data and analytics solutions using cloud-native technologies, with a focus on AWS and Snowflake. The architect will be responsible for designing enterprise data ingestion, workflows, and ETL/ELT patterns. They will also play a crucial role in designing and supporting solutions that incorporate data visualization and BI tools like Tableau. Defining database structure requirements, including aspects of recovery, security, backup, and capabilities, is an essential part of this role. Experience in Snowflake migration projects, as well as proficiency in Python/Pyspark or Scala/Scalaspark programming languages, is required. Strong expertise in the BIG DATA ecosystem, including Hadoop, HIVE, structured data processing, semi-structured data processing, and unstructured data processing, is vital. Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning, and troubleshooting is expected. Excellent communication, design documentation, analytical, technical, and solution prototyping skills are also essential. In summary, the client is looking for a highly experienced Snowflake Architect to design and implement complex data and analytics solutions in a cloud-native environment.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 22, 'JOB TITLE'],\n",
       "     [46, 55, 'MUST HAVE'],\n",
       "     [198, 201, 'MUST HAVE'],\n",
       "     [675, 689, 'MUST HAVE'],\n",
       "     [693, 709, 'GOOD TO HAVE'],\n",
       "     [800, 806, 'MUST HAVE'],\n",
       "     [808, 812, 'GOOD TO HAVE'],\n",
       "     [934, 939, 'GOOD TO HAVE'],\n",
       "     [949, 952, 'GOOD TO HAVE'],\n",
       "     [954, 960, 'GOOD TO HAVE'],\n",
       "     [962, 966, 'GOOD TO HAVE']]}],\n",
       "  ['SNOWFLAKE CLOUD DWHThe client is seeking a Snowflake Engineer. The key responsibilities include end-to-end implementation of the Snowflake cloud data warehouse, expertise in Snowflake data modeling, and using Snowflake SQL for ELT processes. The engineer should be skilled in developing complex stored procedures and have a strong understanding of data warehousing and ETL concepts. Hands-on experience with Snowflake utilities, including SnowSQL and SnowPipe, and deploying Snowflake features is essential. Data migration from RDBMS to the Snowflake cloud data warehouse is also a part of the role. The candidate should have a deep understanding of both relational and NoSQL data stores, including star and snowflake schemas and dimensional modeling. Data security and access controls design is a significant aspect of this position. Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning, and troubleshooting is required. Strong analytical, technical, and problem-solving skills, along with excellent communication abilities, are essential for this role. Overall, the client is looking for a highly skilled Snowflake Engineer to work on their cloud data warehousing solutions.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 22, 'JOB TITLE'],\n",
       "     [209, 218, 'MUST HAVE'],\n",
       "     [219, 222, 'MUST HAVE'],\n",
       "     [227, 230, 'MUST HAVE'],\n",
       "     [439, 446, 'GOOD TO HAVE'],\n",
       "     [451, 459, 'GOOD TO HAVE'],\n",
       "     [670, 675, 'MUST HAVE'],\n",
       "     [850, 855, 'GOOD TO HAVE'],\n",
       "     [878, 882, 'GOOD TO HAVE']]}],\n",
       "  ['CLOUDERA ADMINThe client is seeking a Cloudera Administrator with extensive experience in managing large-scale Enterprise Hadoop environments. The responsibilities include designing, capacity planning, setting up clusters, ensuring security, performance tuning, and ongoing monitoring. The administrator should have in-depth knowledge of the Cloudera CDH distribution and be capable of installing, configuring, and monitoring all services within the CDH stack. Proficiency in core Cloudera Hadoop services, such as HDFS, MapReduce, Kafka, Spark, Hive, Impala, HBASE, Kudu, Sqoop, and Oozie, is crucial. Additionally, the role involves the administration and support of RHEL Linux operating systems, databases, and hardware in an enterprise setting. Expertise in system administration, programming skills, storage capacity management, debugging, and performance tuning is essential, along with proficiency in shell scripting. Experience with supporting Pepperdata installations is also a valuable skill for this position. Overall, the client is looking for a highly skilled Cloudera Administrator to manage and optimize their Hadoop infrastructure.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 17, 'JOB TITLE'],\n",
       "     [38, 46, 'MUST HAVE'],\n",
       "     [351, 354, 'GOOD TO HAVE'],\n",
       "     [490, 496, 'MUST HAVE'],\n",
       "     [515, 519, 'GOOD TO HAVE'],\n",
       "     [521, 530, 'GOOD TO HAVE'],\n",
       "     [532, 537, 'GOOD TO HAVE'],\n",
       "     [539, 544, 'GOOD TO HAVE'],\n",
       "     [546, 550, 'GOOD TO HAVE'],\n",
       "     [552, 558, 'GOOD TO HAVE'],\n",
       "     [560, 565, 'GOOD TO HAVE'],\n",
       "     [567, 571, 'GOOD TO HAVE'],\n",
       "     [573, 578, 'GOOD TO HAVE'],\n",
       "     [584, 589, 'GOOD TO HAVE'],\n",
       "     [674, 679, 'MUST HAVE'],\n",
       "     [952, 962, 'GOOD TO HAVE']]}],\n",
       "  ['DATA SCIENTISTThe client is seeking a Data Scientist with to join their team. The Data Scientist will play a crucial role in helping clients understand the value of AI and advanced analytics for their business. The responsibilities include working closely within a data scientist team, developing and expanding knowledge within projects, and engaging customers to optimize their data monetization processes using AI techniques such as NLP, Neural Net, and Deep Learning. The ideal candidate should have strong experience in Data Sciences and Machine Learning, including the development of client models like Supervised and Unsupervised learning, as well as expertise in Natural Language Processing and linguistic analysis. The Data Scientist should be a thought leader capable of conceptualizing a big vision and defining plans for machine learning, metrics, and deliverables. Strong technical proficiency in programming, design techniques, and various technologies is essential, along with excellent communication skills and the ability to lead and manage small teams. Overall, the client is looking for a highly skilled and innovative Data Scientist to help drive AI solutions for their clients.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 17, 'JOB TITLE'],\n",
       "     [435, 438, 'MUST HAVE'],\n",
       "     [440, 450, 'GOOD TO HAVE'],\n",
       "     [456, 470, 'GOOD TO HAVE'],\n",
       "     [542, 558, 'MUST HAVE']]}],\n",
       "  ['AUTOMATION TESTERThe client is looking for an Automation Tester who will be responsible for implementing automation using JAVA, Selenium Webdriver, Selenium Grid, Maven, and Cucumber. The key responsibilities include understanding and examining test requirements, designing and implementing an automation framework, setting up Selenium test environments, creating test cases with Selenium commands and element locators, and using JUnit/TestNG annotations and Java programming to escalate test cases. The tester is also responsible for maintaining automation resources, preparing test cases in the preferred language, continuously enhancing test case scripts to develop robust test scripts, and resolving bugs and assigning new issues to the development team once tests are executed. In summary, the client is seeking an Automation Tester with expertise in Selenium automation and related technologies to ensure the quality and reliability of their software through automated testing.\\r\\n\\r\\n',\n",
       "   {'entities': [[46, 63, 'JOB TITLE'],\n",
       "     [122, 126, 'MUST HAVE'],\n",
       "     [128, 136, 'MUST HAVE'],\n",
       "     [148, 161, 'GOOD TO HAVE'],\n",
       "     [163, 168, 'GOOD TO HAVE'],\n",
       "     [174, 187, 'GOOD TO HAVE'],\n",
       "     [430, 442, 'GOOD TO HAVE']]}],\n",
       "  ['DEVOPS ENGINEERThe client is seeking a DevOps Engineer with a primary focus on Kubernetes and container orchestration. The role involves designing, implementing, and maintaining scalable and reliable cloud infrastructure using Kubernetes, as well as developing and managing CI/CD pipelines for efficient software delivery with automated testing, deployment, and monitoring. Collaboration with development teams to optimize application deployments and troubleshoot issues related to Kubernetes and containerized environments is key. The engineer should also be responsible for implementing and maintaining monitoring and alerting systems for Kubernetes clusters and applications, ensuring high availability and performance. Infrastructure-as-code practices using tools like Terraform or CloudFormation for managing Kubernetes infrastructure is expected. Staying up-to-date with emerging technologies and trends related to Kubernetes and containerization is crucial. Qualifications include 3+ years of DevOps experience with a strong focus on Kubernetes, a deep understanding of Kubernetes concepts, proficiency in scripting and programming languages, experience with cloud platforms, knowledge of CI/CD concepts and tools, familiarity with infrastructure-as-code, and expertise in containerization technologies. Strong troubleshooting and problem-solving skills, leadership qualities, and effective communication and collaboration with cross-functional teams are also important. In summary, the client is looking for a DevOps Engineer who can lead Kubernetes and container orchestration efforts, automate processes, and ensure the reliability and scalability of cloud infrastructure.\\r\\n\\r\\n',\n",
       "   {'entities': [[39, 54, 'JOB TITLE'],\n",
       "     [79, 89, 'MUST HAVE'],\n",
       "     [274, 279, 'MUST HAVE'],\n",
       "     [652, 660, 'GOOD TO HAVE'],\n",
       "     [773, 782, 'GOOD TO HAVE'],\n",
       "     [786, 800, 'GOOD TO HAVE'],\n",
       "     [988, 990, 'EXPERIENCE'],\n",
       "     [1000, 1006, 'MUST HAVE']]}],\n",
       "  [\"Back-end Data Base Engineer:The client is seeking a Back-End Database Engineer, particularly excelling in Python, Flask, and Django framework for Rest API development, Swagger, and the automation of unit testing. This role requires excellent knowledge of SQL, particularly using Postgres, and experience with No-SQL databases is desirable. The engineer should be capable of setting up AWS for projects and integrating various services such as EC2, S3, Lambda, EKS, and other serverless services. Proficiency in using Bitbucket for source code management, familiarity with Agile methodologies and Software Development Life Cycle (SDLC), and knowledge of networking and web application development components are beneficial. Familiarity with front-end technologies like Angular and React is desirable, and the engineer should have strong communication skills and the ability to work as an individual contributor. Understanding how to integrate multiple data sources and databases into a unified system, knowledge of Python's threading limitations and multi-process architecture, and expertise in writing reusable, testable, and efficient code are vital. In summary, the client is looking for a Back-End Database Engineer with a strong Python and database development background, who can work on various aspects of back-end development and integration with front-end technologies.\\r\\n\\r\\n\",\n",
       "   {'entities': [[0, 27, 'JOB TITLE'],\n",
       "     [106, 112, 'MUST HAVE'],\n",
       "     [114, 119, 'GOOD TO HAVE'],\n",
       "     [125, 131, 'GOOD TO HAVE'],\n",
       "     [255, 258, 'MUST HAVE'],\n",
       "     [279, 287, 'GOOD TO HAVE'],\n",
       "     [309, 315, 'MUST HAVE'],\n",
       "     [385, 388, 'MUST HAVE'],\n",
       "     [443, 446, 'GOOD TO HAVE'],\n",
       "     [448, 450, 'GOOD TO HAVE'],\n",
       "     [452, 458, 'GOOD TO HAVE'],\n",
       "     [460, 464, 'GOOD TO HAVE'],\n",
       "     [572, 577, 'GOOD TO HAVE'],\n",
       "     [768, 775, 'GOOD TO HAVE'],\n",
       "     [780, 785, 'GOOD TO HAVE']]}],\n",
       "  ['QA ENGINEERThe client is in search of a QA Engineer with strong experience in quality assurance and testing. This role encompasses various aspects of testing, including API testing, writing effective SQL queries, and test automation. Performance testing is also part of the responsibilities. The engineer should be familiar with test management tools such as JIRA and ALM, as well as experienced in both Waterfall and Agile methodologies and the Software Testing Lifecycle. Knowledge of CRM and the Transportation domain is considered a valuable asset. The ability to analyze defects, provide root-cause analysis, and create and execute test scenarios, test cases, and test data is essential. Effective communication, both verbal and written, is required, along with proficiency in interacting with customers and stakeholders. The QA Engineer should possess strong analytical, problem-solving, and troubleshooting skills, with the ability to adapt quickly to new technologies, methodologies, and systems. Flexibility to work in shifts and as a functional tester is also expected. In summary, the client is seeking a versatile QA Engineer with experience in multiple testing aspects who can effectively ensure the quality of their software products and adapt to changing requirements.\\r\\n\\r\\n',\n",
       "   {'entities': [[40, 51, 'JOB TITLE'],\n",
       "     [169, 172, 'GOOD TO HAVE'],\n",
       "     [200, 203, 'MUST HAVE'],\n",
       "     [359, 363, 'GOOD TO HAVE'],\n",
       "     [368, 371, 'GOOD TO HAVE'],\n",
       "     [404, 413, 'MUST HAVE'],\n",
       "     [418, 423, 'MUST HAVE']]}],\n",
       "  ['FRONT END DEVELOPER:The client is looking for an experienced Front-End Developer with a who can play a significant role in implementing designs, creating Low-Level Design (LLD), and working on full-stack development using the latest versions of technologies like Angular, Express JS, and TypeScript. This role requires expertise in unit testing automation, code deployment, and in-depth knowledge of frontend and integration technologies such as HTML, CSS, JavaScript, JSON, SOAP, and REST. The developer should excel at building highly scalable, resilient, and secure applications and have experience with databases including MySQL, Postgres, MarkLogic (NoSQL), and Elastic. Familiarity with Swagger API documentation, API integrations in various technologies like Java, Python, and third-party APIs is essential. Security implementation using Single Sign-On (SSO) is also a requirement. Strong communication skills are crucial, as this role involves direct client interaction, requirement discussions, and providing demonstrations. The developer should be capable of working independently to solve problems, handle production deployments, and address production defects within SLAs. In summary, the client is seeking a Front-End Developer who can contribute to the development of scalable and secure applications, work directly with clients, and demonstrate expertise in various front-end and integration technologies.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 19, 'JOB TITLE'],\n",
       "     [263, 270, 'GOOD TO HAVE'],\n",
       "     [272, 282, 'GOOD TO HAVE'],\n",
       "     [288, 299, 'GOOD TO HAVE'],\n",
       "     [446, 450, 'MUST HAVE'],\n",
       "     [452, 455, 'MUST HAVE'],\n",
       "     [457, 467, 'MUST HAVE'],\n",
       "     [469, 473, 'MUST HAVE'],\n",
       "     [475, 479, 'GOOD TO HAVE'],\n",
       "     [485, 490, 'GOOD TO HAVE'],\n",
       "     [627, 632, 'MUST HAVE'],\n",
       "     [634, 642, 'GOOD TO HAVE'],\n",
       "     [644, 653, 'GOOD TO HAVE'],\n",
       "     [655, 660, 'MUST HAVE'],\n",
       "     [720, 723, 'GOOD TO HAVE'],\n",
       "     [766, 770, 'GOOD TO HAVE'],\n",
       "     [772, 778, 'GOOD TO HAVE']]}],\n",
       "  ['BUSINESS ANALYST:The client is seeking a Business Analyst who will serve as a vital link between business and technology leaders, conducting workshops with business stakeholders and demonstrating strong expertise in requirements gathering and grooming, potentially even taking on a Product Owner role. Effective communication skills, especially when interacting with the executive leadership team, are a crucial requirement. Knowledge and understanding of the Life Sciences industry and regulatory processes are essential, with additional value placed on familiarity with Analytics and Machine Learning. The role involves ensuring the update of all Software Development Life Cycle (SDLC) and validation deliverables in Asset Management and JIRA, including documenting User Stories. The Business Analyst is responsible for collecting and documenting requirements for a new Regulatory Analytics platform, reports, dashboards, and models. They will also support the definition of requirements for experiments based on business-prioritized use cases and assist in test planning and script creation when necessary. Maintaining the Product Backlog, generating status reports, and working closely with cross-functional teams are integral aspects of the role. In summary, the client is looking for a Business Analyst with a deep understanding of both business and technology, the ability to gather and document requirements effectively, and a strong grasp of Life Sciences and regulatory processes, with the potential to contribute to analytics and machine learning initiatives.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 16, 'JOB TITLE'],\n",
       "     [572, 581, 'MUST HAVE'],\n",
       "     [586, 603, 'MUST HAVE'],\n",
       "     [682, 686, 'GOOD TO HAVE'],\n",
       "     [740, 744, 'GOOD TO HAVE']]}],\n",
       "  [\"AWS ADMINThe client is in search of an AWS Administrator with a focus on managing Amazon Web Services (AWS) cloud infrastructure, particularly within computing services such as Amazon Machine Image (AMI) and Elastic Compute Cloud (EC2). This role involves networking expertise in AWS services, including Virtual Private Cloud (VPC) and Amazon Route53, and proficiency in AWS Administration and Security Services, such as Identity and Access Management (IAM) and CloudWatch. Key responsibilities encompass creating and managing EC2 instances and working in VPC environments using AMIs, as well as maintaining backups and snapshots. Monitoring and optimizing EC2 performance using CloudWatch is essential, along with experience in system health monitoring and handling performance optimization. The ideal candidate should have strong knowledge of both RedHat Linux and Windows OS, excellent problem-solving skills, and the ability to manage multiple projects simultaneously, adapting to changing business needs. This role includes installing, configuring, and supporting Linux and Windows Servers on Infrastructure as a Service (IaaS), as well as monitoring cloud instances' performance and collaborating with internal teams to provide effective infrastructure support. The client is seeking someone with 4 to 5 years of experience in infrastructure and Linux/Windows operating system support, and who can work effectively in a 24/7 uptime environment. Strong verbal and written communication skills are a must. In summary, the client is looking for an AWS Administrator capable of efficiently managing AWS infrastructure, ensuring its performance and security, and collaborating effectively with a variety of teams.\\r\\n\\r\\n\",\n",
       "   {'entities': [[39, 56, 'JOB TITLE'],\n",
       "     [103, 106, 'MUST HAVE'],\n",
       "     [199, 202, 'GOOD TO HAVE'],\n",
       "     [231, 234, 'GOOD TO HAVE'],\n",
       "     [327, 330, 'GOOD TO HAVE'],\n",
       "     [453, 456, 'GOOD TO HAVE'],\n",
       "     [462, 473, 'GOOD TO HAVE'],\n",
       "     [850, 856, 'MUST HAVE'],\n",
       "     [1308, 1309, 'EXPERIENCE'],\n",
       "     [1352, 1365, 'GOOD TO HAVE']]}],\n",
       "  ['AWS SOLUTION ARCHITECTThe client is seeking an AWS Solution Architect with a specialized skill set in designing and developing big data solutions within the AWS Cloud environment. The ideal candidate should have expertise in utilizing AWS services such as Redshift, S3, EC2, and RDS, along with programming in Python and using data integration tools like Talend. Additionally, the architect should be proficient in designing data processing and visualization with Python and Tableau, optimizing and tuning Redshift databases, scripting in Shell or Python, and working with REST APIs for data extraction. Knowledge of CI/CD and DevOps practices is essential, as is the ability to maintain and enhance data integration pipelines for performance. Experience in data migration from on-premises to AWS, familiarity with Cloudera, and domain knowledge in Sales and Digital Marketing are valuable assets. The role also involves working with cross-functional teams, adhering to Agile methodologies, and driving technical requirements sessions to ensure the quality of software development. Strong communication skills, attention to detail, analytical thinking, and problem-solving capabilities are highly sought after in the ideal candidate. In summary, the client is looking for an AWS Solution Architect who can design and implement complex data solutions in AWS, manage data pipelines, and work collaboratively with various teams to meet business needs.\\r\\n\\r\\n',\n",
       "   {'entities': [[47, 69, 'JOB TITLE'],\n",
       "     [235, 238, 'MUST HAVE'],\n",
       "     [256, 264, 'GOOD TO HAVE'],\n",
       "     [266, 268, 'GOOD TO HAVE'],\n",
       "     [270, 273, 'GOOD TO HAVE'],\n",
       "     [279, 282, 'GOOD TO HAVE'],\n",
       "     [310, 316, 'MUST HAVE'],\n",
       "     [355, 362, 'GOOD TO HAVE'],\n",
       "     [475, 482, 'GOOD TO HAVE'],\n",
       "     [573, 582, 'GOOD TO HAVE'],\n",
       "     [815, 823, 'GOOD TO HAVE'],\n",
       "     [970, 975, 'GOOD TO HAVE']]}],\n",
       "  ['MULESOFT DEVELOPER:The client is looking for a MuleSoft Developer with a hands-on application development experience in MuleSoft ESB/API, particularly focusing on Mule version 3 onwards, and MuleSoft CloudHub. Proficiency in Mule 4 is a mandatory requirement, with certification considered a plus. The ideal candidate should have expertise in integrating Java frameworks, including Spring Integration, and working with web services, REST APIs, and MuleSoft APIs. Experience with CI/CD pipelines is important for this role. Additionally, any familiarity with the MuleSoft Real-Time Processing Framework (RTF) is a valuable asset. In summary, the client is seeking a MuleSoft Developer with strong experience in MuleSoft versions 3 and 4, Java integration, and API development, who can contribute to application development and integration within their organization.\\r\\n\\r\\n',\n",
       "   {'entities': [[0, 18, 'JOB TITLE'],\n",
       "     [129, 136, 'MUST HAVE'],\n",
       "     [191, 209, 'GOOD TO HAVE'],\n",
       "     [355, 359, 'MUST HAVE'],\n",
       "     [382, 388, 'GOOD TO HAVE'],\n",
       "     [433, 437, 'GOOD TO HAVE'],\n",
       "     [448, 456, 'GOOD TO HAVE'],\n",
       "     [603, 606, 'GOOD TO HAVE']]}],\n",
       "  ['BIG DATA DEVELOPER:The client is seeking a Big Data Developer with in designing, developing, and deploying applications within Big Data ecosystems. The ideal candidate should have strong hands-on expertise in key Big Data technologies such as Hadoop, Spark, Scala, Elastic, and Hive/Impala SQL, along with experience in writing Shell scripts and familiarity with data loading tools like Sqoop, Flume, and Kafka. Knowledge of workflow schedulers like Oozie, as well as NoSQL databases (Hbase, MongoDB) and traditional RDBMS (Oracle), is essential. Additional value is placed on experience with StreamSets pipeline development in both cloud and on-premises environments. The role requires strong analytical and design skills, the ability to translate business requirements into efficient technical solutions, and excellent troubleshooting skills. An eagerness to learn new technologies and effective communication skills are also highly desired in this position. In summary, the client is looking for a skilled Big Data Developer to contribute to the design, development, and deployment of applications within large-scale enterprise environments, with a strong emphasis on Big Data technologies and tools.\\r\\n\\t\\r\\n',\n",
       "   {'entities': [[0, 18, 'JOB TITLE'],\n",
       "     [243, 249, 'MUST HAVE'],\n",
       "     [251, 256, 'GOOD TO HAVE'],\n",
       "     [258, 263, 'GOOD TO HAVE'],\n",
       "     [265, 272, 'GOOD TO HAVE'],\n",
       "     [278, 289, 'GOOD TO HAVE'],\n",
       "     [387, 392, 'GOOD TO HAVE'],\n",
       "     [394, 399, 'GOOD TO HAVE'],\n",
       "     [405, 411, 'GOOD TO HAVE'],\n",
       "     [450, 455, 'GOOD TO HAVE'],\n",
       "     [468, 473, 'MUST HAVE']]}],\n",
       "  ['BIG QUERY ENGINEERThe client is in need of a BigQuery Engineer with experience in Google Cloud Platform (GCP) data analytics services, particularly in using BigQuery, Dataflow, and data storage solutions. The primary responsibility for this role involves developing data analytics processes, implementing KPIs, and managing metadata storage. The engineer is expected to handle complex data transformations, conduct unit testing, and validate business logics. In essence, the client is looking for a professional who can effectively utilize GCP data analytics tools to support data processing, analytics, and KPI implementation within their organization.\\r\\n\\r\\n',\n",
       "   {'entities': [[45, 62, 'JOB TITLE'],\n",
       "     [105, 108, 'MUST HAVE'],\n",
       "     [157, 165, 'GOOD TO HAVE'],\n",
       "     [167, 175, 'GOOD TO HAVE'],\n",
       "     [181, 193, 'GOOD TO HAVE'],\n",
       "     [305, 309, 'MUST HAVE']]}],\n",
       "  ['AWS ENGINEERThe client is seeking an AWS Engineer with a strong background in data warehousing, data lakes, data engineering, and data management. This role requires experience with specific AWS services such as Amazon Glue, Redshift, Lambda, and API Gateway, as well as familiarity with other AWS services like EMR, EC2, and S3. The ideal candidate should have a proven track record in Python and/or Java, along with expertise in Elastic Search. The role involves building scripts and processes for ETL (Extract, Transform, Load), data enrichment, alerting, and indexing for high-rate event streams. Designing and developing scalable code within a Big Data environment is a key responsibility, and experience with tools like Scoop, Flume, Oozie, or Zookeeper is beneficial. Hands-on experience with Apache Hive and Apache Spark is crucial, and a background in the life sciences industry is highly advantageous for this role. In summary, the client is looking for an AWS Engineer proficient in data management and processing within an AWS ecosystem, particularly in the context of high-rate event streams and Big Data.\\r\\n\\r\\n',\n",
       "   {'entities': [[37, 49, 'JOB TITLE'],\n",
       "     [191, 194, 'MUST HAVE'],\n",
       "     [212, 223, 'GOOD TO HAVE'],\n",
       "     [225, 233, 'GOOD TO HAVE'],\n",
       "     [235, 241, 'GOOD TO HAVE'],\n",
       "     [247, 250, 'GOOD TO HAVE'],\n",
       "     [251, 258, 'GOOD TO HAVE'],\n",
       "     [312, 315, 'GOOD TO HAVE'],\n",
       "     [317, 320, 'GOOD TO HAVE'],\n",
       "     [326, 329, 'GOOD TO HAVE'],\n",
       "     [387, 393, 'MUST HAVE'],\n",
       "     [401, 405, 'GOOD TO HAVE'],\n",
       "     [500, 503, 'MUST HAVE'],\n",
       "     [726, 731, 'GOOD TO HAVE'],\n",
       "     [733, 739, 'GOOD TO HAVE'],\n",
       "     [740, 745, 'GOOD TO HAVE'],\n",
       "     [750, 759, 'GOOD TO HAVE'],\n",
       "     [807, 811, 'GOOD TO HAVE'],\n",
       "     [823, 828, 'GOOD TO HAVE']]}],\n",
       "  ['PYTHON DEVELOPERThe client is seeking an experienced Python Developer with a minimum hands-on development experience. The responsibilities of this role include helping to design and implement functional requirements, building efficient back-end features in Python, integrating front-end components into applications, managing testing and bug fixes, and preparing technical documentation. Collaboration with UX/UI designers for code implementation, suggesting software enhancements, and ensuring improvements are also part of the role. The client requires familiarity with Python frameworks like Django, Flask, and Bottle, as well as experience with Amazon Web Services (AWS) and REST APIs. Knowledge of databases (particularly Postgres) and SQL is essential, and proficiency in JavaScript and the AngularJs/Angular framework is considered a plus. The candidate should also be well-versed in source code management tools like Bitbucket/GIT, have experience with Agile methodology and the Software Development Life Cycle (SDLC), and possess strong attention to detail. The role additionally calls for an understanding of integrating multiple data sources and databases into a unified system, familiarity with event-driven programming in Python, and the ability to write reusable, testable, and efficient code\\r\\n\\r\\n',\n",
       "   {'entities': [[53, 69, 'JOB TITLE'],\n",
       "     [407, 412, 'GOOD TO HAVE'],\n",
       "     [595, 601, 'GOOD TO HAVE'],\n",
       "     [603, 608, 'GOOD TO HAVE'],\n",
       "     [614, 620, 'GOOD TO HAVE'],\n",
       "     [670, 673, 'MUST HAVE'],\n",
       "     [679, 689, 'GOOD TO HAVE'],\n",
       "     [741, 744, 'MUST HAVE'],\n",
       "     [778, 788, 'MUST HAVE'],\n",
       "     [797, 814, 'GOOD TO HAVE'],\n",
       "     [925, 938, 'GOOD TO HAVE'],\n",
       "     [961, 966, 'GOOD TO HAVE'],\n",
       "     [1020, 1024, 'GOOD TO HAVE']]}],\n",
       "  ['SNOWFLAKE ENGINEERThe requirement for a Snowflake Engineer is seeking an individual with, specifically with at least 4 years as a Snowflake Engineer. This role emphasizes the design and implementation of large-scale, distributed big data solutions, with a track record of creating end-to-end Big Data Lakes integrated with Business Intelligence. The candidate should have expertise in Snowflake cloud data warehousing, including data modeling, ELT using Snowflake SQL, stored procedures, and ETL concepts. Proficiency in Snowflake utilities, data migration from RDBMS to Snowflake, and a deep understanding of relational and NoSQL data stores is crucial. Data security and access control design, as well as strong technical, analytical, and troubleshooting skills, are essential. Effective communication skills are also required for this role.\\r\\n\\r\\n',\n",
       "   {'entities': [[40, 58, 'JOB TITLE'],\n",
       "     [117, 118, 'EXPERIENCE'],\n",
       "     [130, 139, 'MUST HAVE'],\n",
       "     [434, 442, 'GOOD TO HAVE'],\n",
       "     [464, 467, 'MUST HAVE'],\n",
       "     [492, 495, 'MUST HAVE'],\n",
       "     [625, 630, 'MUST HAVE']]}],\n",
       "  ['DATA GOVERNANCE LEADThe requirement for a Data Governance Lead primarily seeks an experienced business analyst with over 12 years of expertise in medical device regulatory registrations, particularly focusing on guidelines such as 510K, EU MDR, and IVDR. This role involves a deep understanding of product quality processes, including CAPA, NCMR, SCAR, and medical device incident reporting. Additionally, the candidate should possess proficiency in PL/SQL, enabling them to conduct source data analysis and collaborate with both IT and business teams within the software development life cycle. The role emphasizes the need for hands-on experience within medical device companies, where the candidate would design and implement processes for managing product regulatory compliance and medical device product registration solutions, including EU MDR, IVDR, 510-K, and UDI solutions using Product Lifecycle Management (PLM). Strong communication, teamwork, and interpersonal skills are essential for success in this position.\\r\\n\\r\\n',\n",
       "   {'entities': [[42, 62, 'JOB TITLE'],\n",
       "     [121, 123, 'EXPERIENCE'],\n",
       "     [231, 235, 'GOOD TO HAVE'],\n",
       "     [237, 243, 'GOOD TO HAVE'],\n",
       "     [249, 254, 'GOOD TO HAVE'],\n",
       "     [335, 339, 'GOOD TO HAVE'],\n",
       "     [341, 345, 'GOOD TO HAVE'],\n",
       "     [347, 351, 'GOOD TO HAVE'],\n",
       "     [450, 456, 'MUST HAVE'],\n",
       "     [843, 849, 'GOOD TO HAVE'],\n",
       "     [851, 855, 'GOOD TO HAVE'],\n",
       "     [857, 862, 'GOOD TO HAVE'],\n",
       "     [868, 871, 'GOOD TO HAVE'],\n",
       "     [918, 921, 'GOOD TO HAVE']]}],\n",
       "  ['RELTIO MDMThe requirement for a Reltio MDM (Master Data Management) professional involves designing and implementing complex data management solutions using the Reltio platform to meet specific organizational needs. This role requires understanding business requirements, translating them into technical specifications, and creating design documents. Collaboration with various team members, including business analysts and project managers, is essential for successful project delivery. Additionally, the role involves code reviews, ensuring code quality, troubleshooting technical issues, and leading discussions with clients and internal teams. The ideal candidate should have a solid understanding of data management concepts, experience with the Reltio platform, proficiency in cloud-based systems, and strong communication and collaboration skills.\\r\\n\\r\\n',\n",
       "   {'entities': [[32, 42, 'JOB TITLE'],\n",
       "     [161, 167, 'MUST HAVE'],\n",
       "     [705, 720, 'MUST HAVE'],\n",
       "     [783, 794, 'GOOD TO HAVE']]}],\n",
       "  ['AI ARCHITECTThe AI Architect role entails designing and delivering machine learning (ML) architecture patterns suitable for both native and hybrid cloud environments. Responsibilities include researching and recommending technical approaches for solving complex ML model training and deployment challenges in enterprise applications. This role requires hands-on programming and architectural expertise in languages like Python, Java, R, or SCALA, with a minimum of 6+ years of experience in enterprise application development. The AI Architect must have a strong background in implementing and deploying ML solutions, hands-on experience with statistical packages and ML libraries, and a deep understanding of statistical analysis and modeling. Additionally, they should be well-versed in various data storage technologies, have experience in solution architecture roles, and be familiar with open source software. Effective problem-solving and excellent communication skills are crucial. The ideal candidate should have a demonstrated technical expertise in AI, ML, and deep learning and a background in integrating these technologies into large-scale enterprise applications, particularly in cloud environments like Amazon Web Services and Microsoft Azure. Specialization in AI/ML stack components and developing best practices for ML life-cycle capabilities is also expected, including data collection, preparation, feature engineering, model management, MLOps, deployment, monitoring, and tuning.\\r\\n\\r\\n\\r\\n',\n",
       "   {'entities': [[16, 28, 'JOB TITLE'],\n",
       "     [67, 83, 'MUST HAVE'],\n",
       "     [420, 426, 'GOOD TO HAVE'],\n",
       "     [428, 432, 'GOOD TO HAVE'],\n",
       "     [434, 436, 'GOOD TO HAVE'],\n",
       "     [440, 445, 'GOOD TO HAVE'],\n",
       "     [465, 467, 'EXPERIENCE'],\n",
       "     [1059, 1061, 'MUST HAVE'],\n",
       "     [1063, 1066, 'MUST HAVE'],\n",
       "     [1071, 1084, 'MUST HAVE'],\n",
       "     [1218, 1237, 'GOOD TO HAVE'],\n",
       "     [1242, 1258, 'GOOD TO HAVE'],\n",
       "     [1458, 1463, 'GOOD TO HAVE']]}]]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320fd3a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.454962Z",
     "iopub.status.busy": "2024-01-02T12:42:11.454642Z",
     "iopub.status.idle": "2024-01-02T12:42:11.458854Z",
     "shell.execute_reply": "2024-01-02T12:42:11.457988Z"
    },
    "papermill": {
     "duration": 0.018798,
     "end_time": "2024-01-02T12:42:11.460796",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.441998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations = jd_data.get('annotations', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f19c83e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.486264Z",
     "iopub.status.busy": "2024-01-02T12:42:11.485452Z",
     "iopub.status.idle": "2024-01-02T12:42:11.541502Z",
     "shell.execute_reply": "2024-01-02T12:42:11.540535Z"
    },
    "papermill": {
     "duration": 0.074694,
     "end_time": "2024-01-02T12:42:11.547324",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.472630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Job Title: AWS Administrator Location: GA, Atlanta, (Remote) Duration: 1 Year Key Responsibilities: Establish configuration, compliance, and audit program to effectively manage AWS accounts and resources. Review cloud resources and cost drivers, compute, storage, network, managed services, database and service licenses, marketplace, and accounts. As part of our Managed Services offering for clients; monitor rightsizing, elasticity, storage optimization, and identify unused resources. Maintain cloud-based servers patching vulnerabilities, backup/restore operations, provision new servers, configure firewalls, configure monitoring systems. Manage account containerization and tagging for internal cost centers and end client billing feeds. Monitor cost allocations, security compliance, budgets and set alerts across accounts, workloads, and users. Operational monitoring to identify and address issues. Use AWS Config to monitor and track resource configuration. Set standards for resource configurations, evaluate configuration compliance, and risk, and remediate configuration drift. Use AWS CloudTrail for compliance audits by recording and storing event logs for actions made within AWS accounts. Set controls to monitor to assure compliance. Familiarity with AWS security best practices and hands-on experience in implementing security controls and compliance requirements with services like AWS Security Hub, Guard Duty, and AWS Audit Manager. Experience with monitoring tools, middleware software and ITSM tools a plus. Qualifications: Bachelor’s degree in computer science or a related field from an accredited college or university. Eight (8) years of Cloud Administration Experience. Excellent technical knowledge of IT Infrastructure, including switches, routers, server operating systems and hardware, storage arrays, and security applications. Strong system administration experience in Windows and Linux environments. Experience with automation using an established framework (SaltStack, Puppet, Chef, Ansible, etc.). Strong technical knowledge of current protocols, operating systems, and standards. Able to read and understand technical manuals and procedural documentation.  Experience in public cloud environments, including AWS and/or Azure. Experience working in an ITIL-driven environment and working knowledge of ITIL principles and processes. ',\n",
       "  {'entities': [[11, 28, 'JOB TITLE'],\n",
       "    [39, 41, 'LOCATION'],\n",
       "    [43, 50, 'LOCATION'],\n",
       "    [53, 59, 'LOCATION'],\n",
       "    [71, 72, 'EXPERIENCE'],\n",
       "    [177, 180, 'MUST HAVE'],\n",
       "    [205, 211, 'GOOD TO HAVE'],\n",
       "    [403, 410, 'GOOD TO HAVE'],\n",
       "    [424, 434, 'GOOD TO HAVE'],\n",
       "    [604, 613, 'GOOD TO HAVE'],\n",
       "    [771, 779, 'GOOD TO HAVE'],\n",
       "    [1100, 1110, 'MUST HAVE'],\n",
       "    [1514, 1518, 'GOOD TO HAVE'],\n",
       "    [1906, 1913, 'MUST HAVE'],\n",
       "    [1918, 1923, 'MUST HAVE'],\n",
       "    [1997, 2006, 'GOOD TO HAVE'],\n",
       "    [2008, 2014, 'GOOD TO HAVE'],\n",
       "    [2016, 2020, 'GOOD TO HAVE'],\n",
       "    [2022, 2029, 'GOOD TO HAVE'],\n",
       "    [2260, 2266, 'GOOD TO HAVE']]}],\n",
       " ['Python Developer responsibilities include: Writing effective, scalable code Developing back-end components to improve responsiveness and overall performance Integrating user-facing elements into applications Job brief We are looking for a Python Developer to join our engineering team and help us develop and maintain various software products. Python Developer responsibilities include writing and testing code, debugging programs and integrating applications with third-party web services. To be successful in this role, you should have experience using server-side logic and work well in a team. Ultimately, you’ll build highly responsive web applications that align with our business needs. Responsibilities Write effective, scalable code Develop back-end components to improve responsiveness and overall performance Integrate user-facing elements into applications Test and debug programs Improve functionality of existing systems Implement security and data protection solutions Assess and prioritize feature requests Coordinate with internal teams to understand user requirements and provide technical solutions Requirements and skills Work experience as a Python Developer Expertise in at least one popular Python framework (like Django, Flask or Pyramid) Knowledge of object-relational mapping (ORM) Familiarity with front-end technologies (like JavaScript and HTML5) Team spirit Good problem-solving skills BSc in Computer Science, Engineering or relevant field ',\n",
       "  {'entities': [[0, 16, 'JOB TITLE'],\n",
       "    [239, 245, 'MUST HAVE'],\n",
       "    [399, 406, 'GOOD TO HAVE'],\n",
       "    [413, 422, 'GOOD TO HAVE'],\n",
       "    [642, 658, 'MUST HAVE'],\n",
       "    [1238, 1244, 'MUST HAVE'],\n",
       "    [1246, 1251, 'GOOD TO HAVE'],\n",
       "    [1255, 1262, 'GOOD TO HAVE'],\n",
       "    [1355, 1365, 'GOOD TO HAVE']]}],\n",
       " ['Title : Full Stack Java Developer Job Type : Contract  Location :Onsite(Jersy City,NJ) Client : Goldman Sachs Implementation Partner : NTT DATA Job Description Required Skills: Bachelor’s or master’s degree in computer science, Engineering, or a related field. ',\n",
       "  {'entities': [[8, 18, 'JOB TITLE'],\n",
       "    [72, 82, 'LOCATION'],\n",
       "    [83, 85, 'LOCATION']]}],\n",
       " ['Core Java developer with 5+ years’ experience in developing, maintaining, and supporting software applications using Java/J2EE, Spring framework, and other related technologies. Must have: Good hands-on experience in Java and J2EE technologies and well versed with features in Java 8 and above. Proficient in Collections, Multi-Threading concepts and should be hands-on. • Experience in UI technologies like Angular, REACT Experience in Spring Boot Hands on development background and recent experience developing and implementing Java based Web Services, primarily in a REST model. Experience building scalable and distributed micro-services. Experience with major open-source tools and frameworks such as Spring, Hibernate, Spring JPA Good understanding of Data structure and Algorithms Experience in messaging tools like Kafka Familiar with Agile software development methodologies Experience with build and development tools like Gradle and Maven Experience in using GIT projects. Experience with Linux / Unix environments Excellent problem-solving skills and communication skills Good to have: Good understanding of SQL and working with relational databases Familiarity with cloud-based data platforms and services (e.g., Snowflake, AWS, Google Cloud, Azure). Good understating on data pipelines using ETL/ELT frameworks and tools (e.g., Informatica, Apache Spark). Familiarity with data modeling techniques and experience with data modeling tools ',\n",
       "  {'entities': [[25, 27, 'EXPERIENCE'],\n",
       "    [128, 134, 'MUST HAVE'],\n",
       "    [217, 221, 'MUST HAVE'],\n",
       "    [226, 230, 'MUST HAVE'],\n",
       "    [408, 415, 'GOOD TO HAVE'],\n",
       "    [417, 422, 'GOOD TO HAVE'],\n",
       "    [571, 575, 'GOOD TO HAVE'],\n",
       "    [707, 713, 'GOOD TO HAVE'],\n",
       "    [715, 724, 'GOOD TO HAVE'],\n",
       "    [726, 736, 'GOOD TO HAVE'],\n",
       "    [824, 829, 'GOOD TO HAVE'],\n",
       "    [844, 849, 'GOOD TO HAVE'],\n",
       "    [1001, 1006, 'MUST HAVE'],\n",
       "    [1121, 1124, 'MUST HAVE'],\n",
       "    [1238, 1241, 'GOOD TO HAVE'],\n",
       "    [1243, 1255, 'GOOD TO HAVE'],\n",
       "    [1257, 1262, 'GOOD TO HAVE'],\n",
       "    [1356, 1368, 'GOOD TO HAVE']]}],\n",
       " ['Job Role: .Net Developer Must needed skills: ETRM (Energy trading and risk management) Location : Portland, Oregon (Hybrid Role) Weekly 2 days onsite Job Description: The ETRM Techno Functional Consultant will be responsible for providing technical and functional expertise in the Energy Trading and Risk Management (ETRM) domain. The Consultant will be responsible for the design, development, testing, and implementation of ETRM solutions. The Consultant will be required to analyze business requirements, develop technical solutions, and provide support to the end users. The Consultant will be responsible for troubleshooting and resolving technical issues related to the ETRM system. The Consultant will be required to provide training and guidance to the end users on the use of the ETRM system. Skills required. – Proficiency with ETRM Endur application, .Net, C#, SQL Server, Oracle PL/SQL ',\n",
       "  {'entities': [[10, 14, 'JOB TITLE'],\n",
       "    [45, 49, 'MUST HAVE'],\n",
       "    [98, 106, 'LOCATION'],\n",
       "    [108, 114, 'LOCATION'],\n",
       "    [862, 866, 'MUST HAVE'],\n",
       "    [868, 870, 'MUST HAVE'],\n",
       "    [872, 875, 'GOOD TO HAVE'],\n",
       "    [884, 890, 'GOOD TO HAVE']]}],\n",
       " ['Job Title: product owner.Primary Responsibilities Translate high-level strategy & product direction into features, epics and user stories Understand the business value of requirements profoundly and convey it internally externally Consult and work with project stakeholders to understand business drivers and translate them into functional and non-functional requirements Elicit, capture, analyze, refine and document business requirements and user stories Communicate business and customer value to the team in a manner that sparks innovative thought Work closely with Agile team members Work to define the acceptance criteria for user stories Accept/reject the delivered code from within each Sprint Responsible for making ongoing decisions to continue development down the current path Create Capabilities and Features, collaborating with IT and business stakeholders to groom these components to a state of ready for the Agile teams Take responsibility for the development of ongoing enhancements, creating and prioritizing user stories within the Agile teams\\r\\n\\r\\nAbout You\\r\\n\\r\\n8+ years of experience with Agile and Scrum practices\\r\\nAgile Certified Practitioner (ACP) preferred\\r\\nExperience managing software projects built in .NET, XML and SQL Server preferred (C#?)\\r\\nExperience working with Jira\\r\\nExperience with data governance products (e.g. Varonis, CyberArk), Security/Networking and knowledge of interfacing to storage platforms (Windows, NetApp, EMC, NAS, NFS) is desirable\\r\\nProficient with project management techniques and tools\\r\\nYou can identify, define, analyze, prioritize and refine requirements, communicating milestones and visions to the development team\\r\\nYou can represent the customer with the ability to prioritize tradeoffs, clarify requirements, and work to drive user stories to acceptance throughout the Agile software development life cycle\\r\\nYou have intermediate level of proficiency with Excel and other Microsoft Products\\r\\nYou have experience writing and translating business requirements\\r\\nYou are an analytical person who can control the details but still see the full picture\\r\\nExperience and comfort solving problems in an ambiguous environment where there is constant change\\r\\nYou have the tenacity to thrive in a dynamic and fast-paced environment\\r\\nExperience in the following technologies is a plus: o Storage platforms (Windows, NetApp, EMC lsilon, NAS, NFS), o Device connectors (Unix, Windows, SSCM, ServiceNow, CMDB, Active Directory), o 3rd party interfaces (OneDrive, SharePoint, Microsoft System Center, Active Directory, PeopleSoft, Office 365), o Databases (SQL Server, Oracle or Sybase), o Map Reduce Frameworks (Hadoop)\\r\\n\\r\\n',\n",
       "  {'entities': [[11, 18, 'JOB TITLE'],\n",
       "    [570, 575, 'MUST HAVE'],\n",
       "    [1080, 1082, 'EXPERIENCE'],\n",
       "    [1165, 1168, 'GOOD TO HAVE'],\n",
       "    [1228, 1232, 'GOOD TO HAVE'],\n",
       "    [1234, 1237, 'GOOD TO HAVE'],\n",
       "    [1242, 1245, 'GOOD TO HAVE'],\n",
       "    [1294, 1298, 'MUST HAVE'],\n",
       "    [1367, 1386, 'MUST HAVE'],\n",
       "    [1916, 1921, 'GOOD TO HAVE'],\n",
       "    [2363, 2369, 'GOOD TO HAVE'],\n",
       "    [2371, 2381, 'GOOD TO HAVE'],\n",
       "    [2383, 2386, 'GOOD TO HAVE'],\n",
       "    [2388, 2391, 'GOOD TO HAVE'],\n",
       "    [2415, 2419, 'GOOD TO HAVE'],\n",
       "    [2421, 2428, 'GOOD TO HAVE'],\n",
       "    [2430, 2434, 'GOOD TO HAVE'],\n",
       "    [2436, 2446, 'GOOD TO HAVE'],\n",
       "    [2448, 2452, 'GOOD TO HAVE'],\n",
       "    [2454, 2470, 'GOOD TO HAVE'],\n",
       "    [2497, 2505, 'GOOD TO HAVE'],\n",
       "    [2507, 2517, 'GOOD TO HAVE'],\n",
       "    [2519, 2542, 'GOOD TO HAVE'],\n",
       "    [2544, 2560, 'GOOD TO HAVE'],\n",
       "    [2562, 2572, 'GOOD TO HAVE'],\n",
       "    [2574, 2584, 'GOOD TO HAVE'],\n",
       "    [2600, 2603, 'GOOD TO HAVE'],\n",
       "    [2612, 2618, 'GOOD TO HAVE'],\n",
       "    [2656, 2662, 'GOOD TO HAVE']]}],\n",
       " [\"Job Title: Scrum Master\\r\\n\\r\\nCountry:\\r\\nUnited States of America\\r\\n\\r\\nPosition Role Type:\\r\\nOnsite\\r\\n\\r\\nAt Raytheon, the foundation of everything we do is rooted in our values and a higher calling – to help our nation and allies defend freedoms and deter aggression. We bring the strength of more than 100 years of experience and renowned engineering expertise to meet the needs of today’s mission and stay ahead of tomorrow’s threat. Our team solves tough, meaningful problems that create a safer, more secure world.\\r\\n\\r\\nJob Summary:\\r\\nRaytheon is looking for Principal Systems Engineer - Scrum Master. In this role, you will lead the Integrated Product Team (IPT) through proper Agile practices within the overall program agile strategy. You will conduct full Agile processes for the Integration and Test IPT including sprint planning, grooming, stand-ups, demos and retrospectives to ensure products integrate with the ground segment and value is provided to the customer and program goals are met. You will work with the team to maintain a healthy backlog of well written stories that are development ready and will work with the team to maintain effective story point estimation. In addition to their Scrum Master role, you will also assist the Integration & Test Lead and Product Owners with the overall integration and test system build and checkout as well as the integrated factory testing portion of the program.\\r\\n\\r\\nResponsibilities to Anticipate:\\r\\nWork with Integration and Test Team and Product Management to create effective story point estimates\\r\\nCommunicate expectations and instill accountability in team members\\r\\nResolve conflicts, promote work sharing, and motivate teams toward common goals\\r\\nMake timely and sound business decisions that support the business objectives\\r\\nManage multiple projects, tasks, and resources through effective organization\\r\\n\\r\\nBasic Qualifications:\\r\\nTypically requires a Bachelor's degree in Science, Technology, Engineering or Mathematics (STEM) and a minimum of 8 years of prior relevant experience unless prohibited by local laws/regulations\\r\\nAgile Scrum Master certification or ability to obtain within 6 months of hire\\r\\nExperience in Agile, Scrum, Kanban and/or project management methodologies\\r\\nExperience with application lifecycle management tools/platforms such as Jira, Confluence, Azure Devops or similar tools\\r\\nExperience with Gherkin, Behave, Python or similar applications\\r\\nActive and transferable U.S. government issued Secret security clearance is required prior to start date. U.S. citizenship is required, as only U.S. citizens are eligible for a security clearance.\\r\\n\\r\\nPreferred Qualifications:\\r\\nExperience with satellite mission management and command and control\\r\\nExperience with PERL or other scripting language\\r\\nExperience with C++, Java, MATLAB or other programming languages\\r\\nExperience with DOORS\\r\\nExperience with Model Based Systems Engineering (MBSE)\\r\\nGood oral and written communication skills\\r\\nExperience in systems engineering functions including large scale system integration, large Scale system test and verification\\r\\nExperience with the Microsoft Office suite of tools (e.g. Word, Excel, and PowerPoint)\\r\\n\\r\\n\",\n",
       "  {'entities': [[11, 23, 'JOB TITLE'],\n",
       "    [99, 107, 'LOCATION'],\n",
       "    [671, 676, 'MUST HAVE'],\n",
       "    [1998, 1999, 'EXPERIENCE'],\n",
       "    [2180, 2185, 'MUST HAVE'],\n",
       "    [2187, 2193, 'GOOD TO HAVE'],\n",
       "    [2201, 2219, 'GOOD TO HAVE'],\n",
       "    [2308, 2312, 'MUST HAVE'],\n",
       "    [2314, 2324, 'MUST HAVE'],\n",
       "    [2332, 2338, 'GOOD TO HAVE'],\n",
       "    [2373, 2380, 'MUST HAVE'],\n",
       "    [2382, 2388, 'MUST HAVE'],\n",
       "    [2390, 2396, 'GOOD TO HAVE'],\n",
       "    [2785, 2788, 'GOOD TO HAVE'],\n",
       "    [2790, 2794, 'GOOD TO HAVE'],\n",
       "    [2796, 2802, 'GOOD TO HAVE'],\n",
       "    [3144, 3148, 'GOOD TO HAVE'],\n",
       "    [3150, 3155, 'GOOD TO HAVE'],\n",
       "    [3161, 3171, 'GOOD TO HAVE']]}],\n",
       " ['7. \\r\\n\\r\\nJob Title: Senior Data Scientist\\r\\n\\r\\nAbout the job\\r\\nCandidates must have a DoD issued Top Secret clearance with SCI\\r\\n\\r\\n\\r\\nThe candidate shall provide support for the Client’s machine learning model development and deployment efforts specific to its cyber focus and targets. Additionally, The candidate will need to have extensive experience with developing and implementing machine learning methodologies to triage large commercial cyber datasets.\\r\\n\\r\\n \\r\\nThe candidate shall provide support to include the following tasks:\\r\\n• The candidate shall implement machine learning methodologies to triage large commercial datasets.\\r\\n• The candidate shall identify topical, spatial, and time-based trends of interest within large amounts of commercial cyber data.\\r\\n• The candidate shall work with data science models testing, optimization, validation, and tests automation.\\r\\n• The candidate shall be able to integrate machine learning models into software through inference engines, machine learning pipelines, and other approaches.\\r\\n• The candidate shall communicate and work effectively with cross-functional team members including but not limited to data analysts, data scientists, external stakeholders, management, and software solutions integrators.\\r\\n \\r\\nRequired skills and demonstrated experience\\r\\n\\r\\nThe candidate shall have the following required skills, certifications and demonstrated experience:\\r\\n• Demonstrated experience tuning hyper-parameters of existing machine learning models for domain-specific data sets.\\r\\n• Demonstrated experience implementing, evaluating, and extending state-of-the-art, data science methods, data labeling, ETL, and other data standardization practices.\\r\\n• Demonstrated experience integrating user-orientated model evaluation.\\r\\n• Demonstrated experience working with data science models testing, optimization, validation, and tests automation.\\r\\n• Demonstrated experience leveraging model management capabilities to track version control and maintain information about best-performing models, such as MLFLOW or similar.\\r\\n• Demonstrated experience programming in Python.\\r\\n• Demonstrated experience programming in other scripting languages, such as Bash.\\r\\n• Demonstrated experience applying deep learning and machine learning processing libraries, including PyTorch, TensorFlow, Keras, and scikit.\\r\\n• Demonstrated experience using Linux and Windows operating systems.\\r\\n• Demonstrated experience using CUDA and NVIDIA GPU accelerated libraries for AI, machine-learning, and deep learning.\\r\\n• Demonstrated experience with implementing data science workflows in cloud-based platforms (e.g., AWS, Azure, etc.).\\r\\n \\r\\nHighly desired skills and demonstrated experience\\r\\n\\r\\nSkills and demonstrated experiences that are highly desired but not required to perform the work include:\\r\\n• Demonstrated experience developing and deploying machine learning models based on cybersecurity related workflows.\\r\\n• Demonstrated experience working in Client’s mission environment.\\r\\n• Demonstrated experience developing and working with cyber data (e.g. netflow, pcap, credential, ip scans, etc.).\\r\\n\\r\\n',\n",
       "  {'entities': [[18, 39, 'JOB TITLE'],\n",
       "    [379, 395, 'MUST HAVE'],\n",
       "    [812, 819, 'GOOD TO HAVE'],\n",
       "    [821, 833, 'GOOD TO HAVE'],\n",
       "    [835, 845, 'GOOD TO HAVE'],\n",
       "    [995, 1005, 'GOOD TO HAVE'],\n",
       "    [1547, 1559, 'GOOD TO HAVE'],\n",
       "    [1642, 1645, 'GOOD TO HAVE'],\n",
       "    [2096, 2103, 'MUST HAVE'],\n",
       "    [2290, 2297, 'GOOD TO HAVE'],\n",
       "    [2299, 2309, 'GOOD TO HAVE'],\n",
       "    [2311, 2316, 'GOOD TO HAVE'],\n",
       "    [2322, 2329, 'GOOD TO HAVE'],\n",
       "    [2449, 2452, 'GOOD TO HAVE'],\n",
       "    [2620, 2623, 'GOOD TO HAVE'],\n",
       "    [2625, 2630, 'GOOD TO HAVE']]}],\n",
       " ['8. \\r\\n\\r\\nTitle: \\r\\n\\r\\nSenior AI Engineer\\r\\n\\r\\nAbout the job\\r\\nLocation: KING OF PRUSSIA, PA, USA\\r\\n\\r\\nSalary: Depends on Experience\\r\\n\\r\\nDescription\\r\\n\\r\\nSr. AI Engineer is responsible for designing and implementing AI based solutions. The ideal candidate can demonstrate a high-level of technical expertise, and will have excellent planning, coordination and communication skills.\\r\\n\\r\\nPrimary Duties & Responsibilities\\r\\n\\r\\nImplement AI processes across the organization.\\r\\nAssist with setting up on-prem and cloud infrastructure required for implementing AI solutions.\\r\\nBuild AI models to make predictions.\\r\\nConvert models to APIs for other processes to use.\\r\\nCollaborate with Data Science team to provide them the output from AI products.\\r\\nCollaborate with other development teams as required to achieve business results.\\r\\nReview business requirements and provide recommendations where architectural oversight is needed.\\r\\nRecommend best practices and standards for AI products, services and applications.\\r\\n\\r\\nQualifications\\r\\n\\r\\nAt least 7 years of overall software development experience writing software products and/or services.\\r\\nProven experience building enterprise level AI products and services.\\r\\nProven experience designing, implementing and using Azure cloud services/ framework to support AI products and services.\\r\\nProven experience with machine learning and model building.\\r\\nExperience with NLP and image processing.\\r\\nExperience with Azure Speech, Azure AI Vision/ Custom Vision, Azure Document Intelligence, Azure Conversational AI services will be a plus.\\r\\nExperience with UiPath a plus.\\r\\nAbility to work the full life-cycle of a software development project.\\r\\nExcellent programming skills in C#/ .NET and other related Microsoft technologies.\\r\\nStrong experience with SQL/ NoSQL databases.\\r\\nStrong object oriented programming and design skills.\\r\\nDemonstrated problem-solving skills.\\r\\nWilling to learn new technologies.\\r\\nDeliver high quality software without compromise.\\r\\nExperience with the healthcare industry and workers’ compensation is a plus.\\r\\nExcellent communication skills and the ability to work across multiple teams.\\r\\nExcellent documentation skills in order to plan, create, track and sustain AI engineering work.\\r\\n\\r\\n',\n",
       "  {'entities': [[18, 36, 'MUST HAVE'],\n",
       "    [65, 80, 'LOCATION'],\n",
       "    [82, 84, 'LOCATION'],\n",
       "    [86, 89, 'LOCATION'],\n",
       "    [611, 615, 'MUST HAVE'],\n",
       "    [1021, 1022, 'EXPERIENCE'],\n",
       "    [1386, 1389, 'MUST HAVE'],\n",
       "    [1394, 1411, 'MUST HAVE'],\n",
       "    [1429, 1441, 'GOOD TO HAVE'],\n",
       "    [1443, 1473, 'GOOD TO HAVE'],\n",
       "    [1475, 1502, 'GOOD TO HAVE'],\n",
       "    [1504, 1527, 'GOOD TO HAVE'],\n",
       "    [1690, 1692, 'GOOD TO HAVE'],\n",
       "    [1694, 1698, 'GOOD TO HAVE'],\n",
       "    [1765, 1769, 'GOOD TO HAVE']]}],\n",
       " ['9. \\r\\n\\r\\nJob Title: Test Manager\\r\\n\\r\\nThe Successful Candidate Will Have a Background In:\\r\\n\\r\\n\\r\\n\\r\\nExperience in software and/or process testing.\\r\\nExperience with test automation tools.\\r\\nExperience within Digital Image Processing or Business Process Services environment.\\r\\nKnowledge of software QA methodologies, tools, and processes.\\r\\nAPI level test automation expertise.\\r\\n\\r\\n\\r\\nResponsibilities:\\r\\n\\r\\nDevelop, implement, and maintain testing strategies and plans to ensure solution quality.\\r\\nEvaluate, select, and integrate the best test automation tools to improve testing efficiency.\\r\\nWork closely with various internal and client teams to understand solution requirements and define test coverage.\\r\\nOversee the execution of manual and automated test cases ensuring they are completed on time and fully documented.\\r\\nSupport test script development and the optimization of those scripts.\\r\\nProvide regular updates on test progress, risks, and quality metrics to stakeholders.\\r\\nLead ongoing test reviews to gather insights and drive continuous improvement.\\r\\nCoordinate the execution of periodic client business continuity tests.\\r\\nSupport client driven testing projects as needed.\\r\\n\\r\\n\\r\\nQualifications:\\r\\n\\r\\nMinimum 5 years of experience in software and/or process testing.\\r\\nStrong organization and documentation skills\\r\\nExperience with test automation tools.\\r\\nStrong knowledge of software QA methodologies, tools, and processes.\\r\\nExceptional analytical and problem-solving skills.\\r\\nExcellent communication and interpersonal skills.\\r\\nAbility to work in a fast-paced environment and handle multiple priorities.\\r\\n\\r\\n\\r\\nSkills Requirements:\\r\\n\\r\\nBachelor’s degree in Computer Science, Information Technology, or a related field.\\r\\nExperience within Digital Image Processing or Business Process Services environment.\\r\\nFamiliarity with Agile/Scrum development processes.\\r\\nAPI level test automation expertise\\r\\nExperience with data file transmission formats (XML and JSON)\\r\\nGeneral understanding of cloud resources (AWS and/or Azure)\\r\\n\\r\\n',\n",
       "  {'entities': [[18, 30, 'JOB TITLE'],\n",
       "    [207, 223, 'GOOD TO HAVE'],\n",
       "    [227, 243, 'GOOD TO HAVE'],\n",
       "    [289, 291, 'MUST HAVE'],\n",
       "    [330, 333, 'MUST HAVE'],\n",
       "    [1203, 1204, 'EXPERIENCE'],\n",
       "    [1813, 1824, 'MUST HAVE'],\n",
       "    [1934, 1937, 'MUST HAVE'],\n",
       "    [1942, 1946, 'MUST HAVE'],\n",
       "    [1991, 1994, 'GOOD TO HAVE'],\n",
       "    [2002, 2007, 'GOOD TO HAVE']]}],\n",
       " [\"10. \\r\\n\\r\\nJob Title: Product Manager, Cloud\\r\\n\\r\\nAbout the role:\\r\\n\\r\\nModular is building a next-generation AI infrastructure platform that connects the many application frameworks and hardware backends, simplifying deployment for AI production teams and accelerating innovation for AI researchers and hardware developers. We are looking for someone to drive the development and success of Modular’s compute platform – defining and developing a cloud-based AI inference system that provides state-of-the-art performance, scalability, usability, and program portability to customers. In this role, you will work closely with our cloud development team and interface with the world’s leading AI companies. Join our world-leading product team and redefine how AI infrastructure is built and deployed.\\r\\n\\r\\n\\r\\nLOCATION: Candidates based in the US or Canada are welcome to apply. You can work remotely from home.\\r\\n\\r\\n\\r\\nWhat you will do:\\r\\n\\r\\nDefine a cloud product strategy that aligns with Modular’s overall business goals and objectives.\\r\\nWork closely with Modular’s development team and world leading AI application developers to define, design, and develop a state-of-the-art cloud-based AI inference system.\\r\\nCreate and maintain product roadmaps, feature prioritization, and release schedules.\\r\\nCollaborate with marketing, design, and other cross-functional teams to ensure the success of the cloud-based AI inference system.\\r\\nConduct market research and gather customer feedback to identify new opportunities and drive innovation.\\r\\nHave a growth and leadership mindset, with an attitude that seeks to learn more from our customers, team, and the broader market.\\r\\n\\r\\n\\r\\nWhat you bring to the table:\\r\\n\\r\\n7+ years product management, including significant time working on a technical product within Cloud, AI, or Data, or experience as a former software/AI developer.\\r\\nKnowledge of cloud technologies and architectures, and/or past experience working on AI infrastructure/cloud products and platforms like TensorFlow, PyTorch, TensorRT, CUDA, Triton Inference Server, VertexAI, Sagemaker is a must.\\r\\nExperience driving product vision, go-to-market strategy, and design discussions.\\r\\nExperience creating strategic product roadmaps and working with cross-functional teams.\\r\\nExperience with market research and customer feedback to drive product innovation.\\r\\nStrong analytical and problem-solving skills.\\r\\n\\r\\n\\r\\nHelpful experience (not required):\\r\\n\\r\\nMaster's degree or Ph.D. in Computer Science or related technical field.\\r\\nExperience developing, training, and/or deploying machine learning models into production environments.\\r\\nKnowledge of multiple functional areas (e.g., Product Management, Engineering, UX/UI, or Marketing).\\r\\nAbility to influence multiple stakeholders without direct authority.\\r\\nHave worked in or around startups before or have a strong understanding of the nature of fast-moving, highly dynamic teams.\\r\\n\",\n",
       "  {'entities': [[19, 34, 'JOB TITLE'],\n",
       "    [1688, 1690, 'EXPERIENCE'],\n",
       "    [1782, 1787, 'MUST HAVE'],\n",
       "    [1789, 1791, 'MUST HAVE'],\n",
       "    [1828, 1850, 'GOOD TO HAVE'],\n",
       "    [1989, 1999, 'GOOD TO HAVE'],\n",
       "    [2001, 2008, 'GOOD TO HAVE'],\n",
       "    [2010, 2018, 'GOOD TO HAVE'],\n",
       "    [2020, 2024, 'GOOD TO HAVE'],\n",
       "    [2026, 2049, 'GOOD TO HAVE'],\n",
       "    [2051, 2059, 'GOOD TO HAVE'],\n",
       "    [2061, 2070, 'GOOD TO HAVE'],\n",
       "    [2552, 2568, 'MUST HAVE']]}],\n",
       " [\"SAP BTP Architect In simple terms, the SAP BTP Architect is like the mastermind behind making sure a company's SAP software works seamlessly. They focus on the SAP Business Technology Platform (BTP), specifically its Integration Suite. This involves designing, implementing, and overseeing the ongoing management of advanced SAP solutions. Working closely with different teams, the architect figures out what the business needs, creates a plan for how to make it happen using SAP BTP, and then ensures that the plan is put into action smoothly. They also keep an eye on the latest SAP BTP technologies and trends, providing guidance to other teams. The architect is a key player in managing technical risks, making sure projects stay on track, and documenting everything so that others can understand and use the solutions. They should have a strong background in SAP implementation and architecture, deep knowledge of SAP BTP technologies, and excellent communication and leadership skills. Experience with agile development and DevOps practices, as well as relevant SAP certifications, is a big plus.\\r\\n\\r\\n\",\n",
       "  {'entities': [[0, 17, 'JOB TITLE'],\n",
       "    [194, 197, 'MUST HAVE'],\n",
       "    [1008, 1013, 'MUST HAVE'],\n",
       "    [1030, 1036, 'MUST HAVE']]}],\n",
       " [\"SAP Logistic Solution architect – Offshore – 7A/6BSAP Logistic Solution Architect role involves overseeing and optimizing the way a company uses SAP software for its logistics and supply chain operations. This includes ensuring that the SAP solutions align with business goals, are efficient, secure, and stay up-to-date with the latest technologies.As part of the job, the architect defines the scope of new software releases, evaluates and challenges proposed solutions from vendors, and actively collaborates with various teams and stakeholders. They play a key role in translating business needs into IT requirements, configuring and building technical solutions, and testing them to ensure they meet quality standards.The architect is also responsible for maintaining the integrity of the software platforms, preventing unnecessary customization, and staying informed about digital market trends. With a solid background in SAP SD (Sales and Distribution) and integration with other modules, they bring expertise in areas like SAP S/4 HANA configuration, cloud concepts, and agile methodologies.The technical background and experience required for the role include a deep understanding of SAP technologies, multiple end-to-end implementation experiences, and the ability to manage vendor releases and assess solution designs. Overall, the SAP Logistic Solution Architect is a seasoned professional who ensures that the company's logistics and supply chain processes run smoothly and effectively with the help of SAP solutions.In simple terms, this job is about finding a skilled individual, possibly from a known company, who's an expert in Drupal (the last 2-3 versions). This person will be a one-person team responsible for designing and setting up an internal portal, which is like a private website for a company's internal use.The job involves creating and integrating various elements on the portal, such as web parts and plugins, and ensuring everything works smoothly. The person should be proficient in technologies like MySQL, PHP, HTML, CSS, and JavaScript. They'll need to have excellent communication skills, both written and verbal.The work hours are from 12 PM to 9 PM, and the job includes deploying the portal in testing and production environments. It's a long-term commitment, possibly starting immediately, and there's an option for a contract-to-hire arrangement.\\r\\n\\r\\n\",\n",
       "  {'entities': [[0, 31, 'JOB TITLE'],\n",
       "    [34, 42, 'LOCATION'],\n",
       "    [237, 240, 'MUST HAVE'],\n",
       "    [937, 942, 'MUST HAVE'],\n",
       "    [947, 959, 'MUST HAVE'],\n",
       "    [1032, 1044, 'MUST HAVE'],\n",
       "    [1060, 1065, 'GOOD TO HAVE'],\n",
       "    [1080, 1085, 'GOOD TO HAVE'],\n",
       "    [1646, 1652, 'MUST HAVE'],\n",
       "    [2036, 2041, 'MUST HAVE'],\n",
       "    [2043, 2046, 'MUST HAVE'],\n",
       "    [2048, 2052, 'MUST HAVE'],\n",
       "    [2054, 2057, 'MUST HAVE'],\n",
       "    [2063, 2074, 'MUST HAVE']]}],\n",
       " [\"Drupal expertIn simple terms, this job is about finding a skilled individual, possibly from a known company, who's an expert in Drupal (the last 2-3 versions). This person will be a one-person team responsible for designing and setting up an internal portal, which is like a private website for a company's internal use.The job involves creating and integrating various elements on the portal, such as web parts and plugins, and ensuring everything works smoothly. The person should be proficient in technologies like MySQL, PHP, HTML, CSS, and JavaScript. They'll need to have excellent communication skills, both written and verbal.The work hours are from 12 PM to 9 PM, and the job includes deploying the portal in testing and production environments. It's a long-term commitment, possibly starting immediately, and there's an option for a contract-to-hire arrangement.\\r\\n\\r\\n\",\n",
       "  {'entities': [[0, 15, 'JOB TITLE'],\n",
       "    [128, 134, 'MUST HAVE'],\n",
       "    [518, 523, 'MUST HAVE'],\n",
       "    [525, 528, 'MUST HAVE'],\n",
       "    [530, 534, 'MUST HAVE'],\n",
       "    [536, 539, 'MUST HAVE'],\n",
       "    [545, 556, 'MUST HAVE']]}],\n",
       " ['Databricks Solution Architect RoleThe client is seeking a Databricks Solution Architect withexperience in a technical role, focusing on data governance, data warehousing, and setting up DataBricks as a Service model. The role requires expertise in production deployment of data governance solutions and hands-on experience with cloud data lakes. The architect should have a strong background in designing and implementing data warehousing technologies, with deep specialty expertise in scaling big data workloads for optimal performance and cost-effectiveness, including technologies like Delta Lake. Additionally, the architect will support customers by creating reference architectures, how-to guides, and demo applications, collaborating with Enterprise Accounts, and integrating Databricks with third-party applications to align with customer architectures. Experience in designing and implementing cloud-based architectures (AWS, Azure, or GCP) is crucial, along with excellent communication skills.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 29, 'JOB TITLE'],\n",
       "    [186, 196, 'MUST HAVE'],\n",
       "    [334, 345, 'GOOD TO HAVE'],\n",
       "    [589, 600, 'MUST HAVE'],\n",
       "    [903, 928, 'MUST HAVE'],\n",
       "    [930, 933, 'GOOD TO HAVE'],\n",
       "    [935, 940, 'GOOD TO HAVE'],\n",
       "    [945, 948, 'GOOD TO HAVE']]}],\n",
       " ['GCP DATA ENGINEERThe client is in need of a GCP Data Engineer with expertise in GCP data analytics services, including Dataflow, Data Fusion, BigQuery, and data storage. The primary responsibility is to develop a Google Kubernetes Engine (GKE) to efficiently connect and manage data from various sources. The role also involves conducting unit tests and validating the graph lineage, ensuring the integrity and accuracy of data processing and transformation within the GCP environment.\\r\\n',\n",
       "  {'entities': [[0, 20, 'JOB TITLE'],\n",
       "    [44, 47, 'MUST HAVE'],\n",
       "    [119, 127, 'GOOD TO HAVE'],\n",
       "    [129, 140, 'GOOD TO HAVE'],\n",
       "    [142, 150, 'GOOD TO HAVE'],\n",
       "    [220, 230, 'MUST HAVE']]}],\n",
       " ['SNOWFLAKE ADMINThe client is seeking a Snowflake Admin with a wide range of responsibilities to ensure the effective and optimized utilization of Snowflake technology. The role includes configuring and customizing Snowflake based on user stories and acceptance criteria, as well as providing technical documentation and design documents. The Snowflake Admin will be involved in setting up and migrating data warehouses, designing complex data models, and optimizing Snowflake for performance. They will also work directly with customers to demonstrate best practices, provide guidance for technical challenges, and lead the implementation of high-volume data analytics and machine learning solutions in the cloud. The role entails designing features, integrating APIs, and ensuring high-quality, reliable software to meet customer requirements and business goals.\\r\\n',\n",
       "  {'entities': [[0, 18, 'JOB TITLE'],\n",
       "    [146, 155, 'MUST HAVE'],\n",
       "    [408, 418, 'GOOD TO HAVE'],\n",
       "    [438, 449, 'GOOD TO HAVE'],\n",
       "    [673, 689, 'MUST HAVE'],\n",
       "    [763, 767, 'GOOD TO HAVE']]}],\n",
       " [\"FULL STACK DEVELOPERThe client is looking for a Full Stack Developer responsible for creating and maintaining both the user interface (UI) components using Angular and the server-side network components. This role requires expertise in developing APIs using Node.js, following microservices architecture, and integrating APIs from various technologies, including Java, Python, and third-party sources. The developer should have a strong focus on building high-performance applications through the creation of testable, reusable, and efficient code. Additionally, the candidate should be capable of handling production deployments and promptly addressing defects while having a clear understanding of various platform components such as caching, proxies, and routing to ensure the system's functionality and performance.\\r\\n\",\n",
       "  {'entities': [[0, 10, 'JOB TITLE'],\n",
       "    [156, 163, 'MUST HAVE'],\n",
       "    [258, 265, 'MUST HAVE'],\n",
       "    [363, 368, 'GOOD TO HAVE'],\n",
       "    [369, 375, 'GOOD TO HAVE'],\n",
       "    [736, 743, 'GOOD TO HAVE'],\n",
       "    [745, 752, 'GOOD TO HAVE']]}],\n",
       " ['AWS MSKThe client is seeking an AWS professional with a strong background in Kafka Streams Development and expertise in AWS MSK (Managed Streaming for Apache Kafka). The primary focus is on developing cloud-native applications on the AWS platform that involve real-time streaming data pipelines and near real-time big data analytics. The ideal candidate should possess hands-on experience in working with Kafka Schemas and the use of the Schema Registry, as well as configuring, optimizing, and ensuring fault tolerance within Kafka clusters. Additionally, the role involves a strong understanding of Kafka client configuration and the ability to troubleshoot related issues to ensure the smooth operation of Kafka ecosystems within the AWS environment.\\r\\n',\n",
       "  {'entities': [[32, 35, 'MUST HAVE'],\n",
       "    [77, 82, 'MUST HAVE'],\n",
       "    [120, 127, 'JOB TITLE'],\n",
       "    [314, 333, 'GOOD TO HAVE'],\n",
       "    [438, 453, 'GOOD TO HAVE'],\n",
       "    [466, 477, 'GOOD TO HAVE'],\n",
       "    [479, 489, 'GOOD TO HAVE'],\n",
       "    [504, 519, 'GOOD TO HAVE']]}],\n",
       " ['SHAREPOINT DEVELOPERThe client is looking for a SharePoint Developer to design and create an Intranet Portal Collaboration Site with a focus on branding and user experience. The developer will be responsible for building a new landing page, department templates, newsletters, banners, and ensuring mobile-friendly videos. The project involves creating, deploying, and administering a new intranet portal site using SharePoint. The developer will also design website elements using SharePoint Framework SPx, HTML5, JavaScript, Typescript, jQuery, CSS, SQL, Bootstrap, and responsive design to ensure compatibility across all devices. Additionally, tasks include building web components, managing content, permissions, and using tools like SharePoint Designer and PowerApps/Flow for workflow forms. Experience with data migration tools to transfer data between SharePoint Online and other SharePoint versions is also required.\\r\\n',\n",
       "  {'entities': [[0, 23, 'JOB TITLE'],\n",
       "    [502, 505, 'MUST HAVE'],\n",
       "    [507, 512, 'MUST HAVE'],\n",
       "    [514, 524, 'MUST HAVE'],\n",
       "    [526, 536, 'GOOD TO HAVE'],\n",
       "    [538, 544, 'GOOD TO HAVE'],\n",
       "    [546, 549, 'GOOD TO HAVE'],\n",
       "    [551, 554, 'MUST HAVE'],\n",
       "    [556, 565, 'GOOD TO HAVE']]}],\n",
       " ['AI LLM DATA SCIENCE:The client is seeking an AI LLM Data Scientist with extensive experience in data science, machine learning, and a strong focus on NLP technologies. a deep understanding of Transformer Encoder Networks. They should be adept at applying deep learning and generative modeling techniques, particularly in the field of Artificial Intelligence and generative models like GANs, VAEs, and transformer-based architectures. The role involves designing and implementing state-of-the-art generative models for various NLP tasks, collaborating with cross-functional teams, and staying updated on the latest advancements in generative AI and LLM. While contributions to the research community are a plus, the primary responsibilities include evaluating and preprocessing large-scale datasets, ensuring data quality and integrity, developing data pipelines for training and evaluation, and deploying and optimizing generative models in production environments. The client also values the ability to articulate model effects to business stakeholders, develop guardrails for LLMs, and collaborate with software engineers for scalability and efficiency. Additionally, the candidate should have the capacity to provide guidance to junior data scientists and contribute to the growth and success of the data science team.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 19, 'JOB TITLE'],\n",
       "    [110, 126, 'MUST HAVE'],\n",
       "    [150, 153, 'MUST HAVE'],\n",
       "    [192, 203, 'GOOD TO HAVE'],\n",
       "    [385, 389, 'GOOD TO HAVE'],\n",
       "    [391, 395, 'GOOD TO HAVE'],\n",
       "    [641, 643, 'MUST HAVE'],\n",
       "    [648, 652, 'MUST HAVE']]}],\n",
       " ['AZURE MLOPS & DEVOPS:The client is seeking an Azure MLOps and DevOps expert with specific qualifications. The ideal candidate should be proficient in Terraform and demonstrate a willingness to quickly adapt to client-specific frameworks and standards. They should have expertise in Azure services and deployments and have experience in MLOps using tools like Argo CD and DevOps with Jenkins. The role involves deploying AI, ML, and LLM-related solutions and service provisioning, as well as deploying various applications such as Node.js, React, Angular, and Java. Experience in creating Kubernetes clusters, nodes, setup, and Docker images is essential. Knowledge of Python and the ability to implement monitoring and observability solutions are also required. Strong written and verbal communication skills are important, and the candidate should be capable of working as an individual contributor. In summary, the client is looking for a versatile professional who can excel in Azure MLOps and DevOps while being adaptable to specific client requirements and standards.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 20, 'JOB TITLE'],\n",
       "    [46, 51, 'MUST HAVE'],\n",
       "    [364, 366, 'GOOD TO HAVE'],\n",
       "    [371, 377, 'GOOD TO HAVE'],\n",
       "    [383, 391, 'MUST HAVE'],\n",
       "    [530, 537, 'GOOD TO HAVE'],\n",
       "    [539, 544, 'GOOD TO HAVE'],\n",
       "    [546, 553, 'GOOD TO HAVE'],\n",
       "    [559, 564, 'GOOD TO HAVE'],\n",
       "    [588, 598, 'MUST HAVE'],\n",
       "    [609, 614, 'GOOD TO HAVE'],\n",
       "    [627, 633, 'MUST HAVE'],\n",
       "    [668, 674, 'MUST HAVE']]}],\n",
       " ['AWS DEVOPS ENGINEERThe client is looking for an experienced AWS DevOps Engineer including a minimum of 2 implementations of DevOps on the AWS cloud platform. The candidate should have a strong background in building and maintaining CI/CD pipelines on AWS using tools like Jenkins. Additionally, the role requires expertise in Python, Lambda integration, Airflow, and MLflow, with a specific focus on automating ML model operations. Knowledge of technologies such as JavaScript, Kubernetes, Docker, and Python is important for this position. Effective oral and written communication skills are also essential. In summary, the client needs a skilled DevOps Engineer with AWS experience who can contribute to the automation and management of CI/CD pipelines and ML model operations on the AWS platform.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 10, 'JOB TITLE'],\n",
       "    [60, 63, 'MUST HAVE'],\n",
       "    [103, 104, 'EXPERIENCE'],\n",
       "    [232, 237, 'MUST HAVE'],\n",
       "    [326, 332, 'MUST HAVE'],\n",
       "    [334, 352, 'MUST HAVE'],\n",
       "    [354, 361, 'MUST HAVE'],\n",
       "    [367, 373, 'MUST HAVE'],\n",
       "    [466, 476, 'GOOD TO HAVE'],\n",
       "    [478, 488, 'GOOD TO HAVE'],\n",
       "    [490, 496, 'GOOD TO HAVE'],\n",
       "    [502, 508, 'GOOD TO HAVE']]}],\n",
       " ['SNOWFLAKE DATA ENGINEER:The client is looking for a Snowflake Data Engineer  IT experience, hands-on experience as a Snowflake Data Engineer. The candidate should have expertise in implementing end-to-end Snowflake cloud data warehousing solutions on Amazon Web Services (AWS), and experience with Snowflake migration projects on AWS. Proficiency in programming languages such as Python, Pyspark, Scala, and Scalaspark is required, and a strong understanding of the BIG DATA ecosystem, including Hadoop, HIVE, structured data processing, semi-structured data processing, and unstructured data processing, is essential. The candidate should also have hands-on experience with Snowflake utilities, Snowpark API, SnowSQL, and SnowPipe, as well as expertise in Snowflake data modeling, ELT using Snowflake SQL, implementing complex stored procedures, and standard data warehousing and ETL concepts. Familiarity with data security and access controls, RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, and performance tuning is important. Good communication skills, strong analytical abilities, and effective troubleshooting skills are also required for this role. In summary, the client is seeking an experienced Snowflake Data Engineer with a comprehensive skill set to work on Snowflake cloud data warehousing projects with a focus on AWS.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 23, 'JOB TITLE'],\n",
       "    [52, 61, 'MUST HAVE'],\n",
       "    [272, 275, 'GOOD TO HAVE'],\n",
       "    [380, 386, 'MUST HAVE'],\n",
       "    [388, 395, 'MUST HAVE'],\n",
       "    [397, 402, 'GOOD TO HAVE'],\n",
       "    [408, 418, 'GOOD TO HAVE'],\n",
       "    [496, 502, 'MUST HAVE'],\n",
       "    [504, 508, 'GOOD TO HAVE'],\n",
       "    [696, 708, 'GOOD TO HAVE'],\n",
       "    [710, 717, 'GOOD TO HAVE'],\n",
       "    [723, 731, 'GOOD TO HAVE'],\n",
       "    [962, 965, 'MUST HAVE'],\n",
       "    [975, 979, 'MUST HAVE']]}],\n",
       " ['SAP HANA LEADThe client is seeking an experienced SAP HANA Lead of SAP HANA development experience. This role requires leadership and team-handling experience within the HANA domain and mandates a minimum of 2 End to End S/4 HANA implementations. The ideal candidate should have extensive hands-on experience in SAP HANA design and modeling, with a focus on calculation views, stored procedures, scalar and table functions, and XSO Data. Proficiency in SAP Native HANA SQL script is essential. The candidate should also have project experience in modeling concepts, including creating calculation views, SAP HANA SQL, SQL Script and Procedures, currency conversion, and translating business rules into decision tables. Collaboration with functional consultants to develop optimal solutions is a key aspect of this role, requiring a general understanding of functional processes. Effective verbal and written communication skills are also crucial for this position. In summary, the client is looking for a seasoned SAP HANA professional with leadership experience, strong modeling skills, and the ability to work closely with functional consultants to deliver effective solutions.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 8, 'JOB TITLE'],\n",
       "    [50, 53, 'MUST HAVE'],\n",
       "    [208, 209, 'EXPERIENCE'],\n",
       "    [396, 402, 'GOOD TO HAVE'],\n",
       "    [407, 422, 'GOOD TO HAVE'],\n",
       "    [428, 431, 'GOOD TO HAVE'],\n",
       "    [469, 472, 'MUST HAVE'],\n",
       "    [645, 664, 'GOOD TO HAVE']]}],\n",
       " ['SAP CPI/PI/POThe client is seeking an experienced SAP CPI/PI/PO professional of SAP PO development experience, including a minimum of 2 End to End SAP CPI implementations. This role requires team leadership and handling experience within the SAP CPI/PI/PO domain. The ideal candidate should have a strong working knowledge of SAP CPI, PI, SAP Architecture, and interfacing concepts. They should be proficient in developing interfaces using SAP PI standard adaptors such as OData, RFC, Proxies, IDOC, SOAP, REST, and JDBC. Moreover, the candidate should have a solid background in integrating third-party systems using APIs. Strong skills in graphical mapping, Java mapping, XSLT mapping, and user-defined function (UDF) development are essential. Experience in ALE/IDoc, Proxies, and RFCs within the SAP ECC environment is also required. This role involves close collaboration with functional consultants and third-party vendors to develop effective solutions, making a general understanding of functional processes important. Effective verbal and written communication skills are crucial for this position. In summary, the client is looking for a seasoned SAP professional capable of leading teams, developing interfaces, and collaborating with stakeholders to deliver optimal solutions in the SAP CPI/PI/PO landscape.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 16, 'JOB TITLE'],\n",
       "    [134, 135, 'EXPERIENCE'],\n",
       "    [339, 342, 'MUST HAVE'],\n",
       "    [473, 478, 'GOOD TO HAVE'],\n",
       "    [480, 483, 'GOOD TO HAVE'],\n",
       "    [485, 492, 'GOOD TO HAVE'],\n",
       "    [494, 498, 'GOOD TO HAVE'],\n",
       "    [500, 504, 'MUST HAVE'],\n",
       "    [506, 510, 'MUST HAVE'],\n",
       "    [516, 521, 'MUST HAVE'],\n",
       "    [660, 672, 'MUST HAVE'],\n",
       "    [674, 686, 'GOOD TO HAVE'],\n",
       "    [761, 769, 'GOOD TO HAVE'],\n",
       "    [771, 778, 'GOOD TO HAVE'],\n",
       "    [784, 788, 'GOOD TO HAVE']]}],\n",
       " ['STIIM DEVELOPERThe client is looking for a STIIM Developer IT experience who possesses a specific skill set. The ideal candidate should be proficient in Python and have a strong command of the Google Cloud stack, including BigQuery, Dataproc, and Dataflow. Additionally, expertise in Linux scripting and a good understanding of big data technologies, particularly PySpark, are required. Strong analytical skills and the ability to write complex SQL queries based on business requirements are essential. Effective communication skills are also a prerequisite for this role. In summary, the client needs a seasoned IT professional who can effectively leverage Python and Google Cloud tools for data processing and analysis, coupled with strong analytical, database, and communication skills.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 18, 'JOB TITLE'],\n",
       "    [223, 231, 'GOOD TO HAVE'],\n",
       "    [284, 289, 'MUST HAVE'],\n",
       "    [364, 371, 'GOOD TO HAVE'],\n",
       "    [445, 448, 'MUST HAVE'],\n",
       "    [658, 664, 'MUST HAVE']]}],\n",
       " ['AZURE CLOUD ADMINThe client is looking for an Azure Cloud Admin with expertise in designing and building cloud infrastructure on Microsoft Azure. The responsibilities include developing hybrid cloud reference architecture, defining target state cloud architecture, and experience with private and public cloud architectures. The candidate should have hands-on experience in migrating workloads from on-premises to Azure, provisioning infrastructure, and executing cutover plans to minimize system downtime. Proficiency in workload migration using automation tools like CloudEndure, Azure Application Migration Service, and Azure Database Migration Service is essential. Knowledge of infrastructure provisioning and management tools like Ansible, Chef, Puppet, Terraform, and CloudFormation is required. Familiarity with Cloud Adoption Frameworks and Azure Well-Architected Framework, as well as container technologies like Docker and Kubernetes, is expected. Understanding of DevOps processes and CI/CD tools, microservices, and serverless architecture is a plus. The candidate should also have networking knowledge in Azure services, RedHat Linux and Windows OS expertise, strong problem-solving skills, and the ability to implement high availability (HA) and disaster recovery (DR) concepts on Azure. Creating proof-of-concepts to demonstrate the viability of solutions is part of the role. In summary, the client is seeking an Azure Cloud Admin with a comprehensive skill set to design, build, and manage cloud infrastructure on the Azure platform while ensuring efficient and secure operations.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 20, 'JOB TITLE'],\n",
       "    [46, 51, 'MUST HAVE'],\n",
       "    [569, 580, 'GOOD TO HAVE'],\n",
       "    [582, 599, 'GOOD TO HAVE'],\n",
       "    [600, 617, 'GOOD TO HAVE'],\n",
       "    [623, 655, 'GOOD TO HAVE'],\n",
       "    [923, 929, 'MUST HAVE'],\n",
       "    [934, 944, 'MUST HAVE'],\n",
       "    [976, 982, 'MUST HAVE'],\n",
       "    [997, 1002, 'MUST HAVE'],\n",
       "    [1135, 1141, 'GOOD TO HAVE'],\n",
       "    [1152, 1159, 'GOOD TO HAVE']]}],\n",
       " ['DATA ENGINEERThe client is seeking a Data Engineer with 5-8 years of experience in architecting and building large-scale, distributed big data solutions, with a focus on at least 2-3 Big Data implementation projects. The ideal candidate should have knowledge and experience with cloud platforms, particularly AWS cloud services such as EC2, EMR, RDS, and Redshift. A strong background in the Hadoop ecosystem, including HDFS, Hive, Spark, Sqoop, Kafka, NiFi, and real-time streaming technologies, is essential, and experience with the Cloudera distribution is preferred. Proficiency in programming languages like Python, PySpark, and Java is a must. Excellent communication skills, strong analytical abilities, and troubleshooting skills are expected. Experience in team leadership and workload management is also desirable, and any experience with tools like Informatica BDM and StreamSets would be a plus. In summary, the client is looking for an experienced Data Engineer with a strong background in big data technologies and cloud platforms to design and implement data solutions for their organization.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 16, 'JOB TITLE'],\n",
       "    [56, 59, 'EXPERIENCE'],\n",
       "    [309, 312, 'MUST HAVE'],\n",
       "    [336, 339, 'GOOD TO HAVE'],\n",
       "    [341, 344, 'GOOD TO HAVE'],\n",
       "    [346, 349, 'GOOD TO HAVE'],\n",
       "    [355, 364, 'GOOD TO HAVE'],\n",
       "    [392, 398, 'MUST HAVE'],\n",
       "    [420, 424, 'GOOD TO HAVE'],\n",
       "    [426, 430, 'GOOD TO HAVE'],\n",
       "    [432, 437, 'GOOD TO HAVE'],\n",
       "    [439, 444, 'GOOD TO HAVE'],\n",
       "    [446, 451, 'GOOD TO HAVE'],\n",
       "    [453, 457, 'GOOD TO HAVE'],\n",
       "    [613, 619, 'MUST HAVE'],\n",
       "    [621, 628, 'MUST HAVE'],\n",
       "    [634, 638, 'GOOD TO HAVE']]}],\n",
       " ['SNOWFLAKE ARCHITECTThe client is in need of a Snowflake Architect The key responsibilities involve designing end-to-end data and analytics solutions using cloud-native technologies, with a focus on AWS and Snowflake. The architect will be responsible for designing enterprise data ingestion, workflows, and ETL/ELT patterns. They will also play a crucial role in designing and supporting solutions that incorporate data visualization and BI tools like Tableau. Defining database structure requirements, including aspects of recovery, security, backup, and capabilities, is an essential part of this role. Experience in Snowflake migration projects, as well as proficiency in Python/Pyspark or Scala/Scalaspark programming languages, is required. Strong expertise in the BIG DATA ecosystem, including Hadoop, HIVE, structured data processing, semi-structured data processing, and unstructured data processing, is vital. Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning, and troubleshooting is expected. Excellent communication, design documentation, analytical, technical, and solution prototyping skills are also essential. In summary, the client is looking for a highly experienced Snowflake Architect to design and implement complex data and analytics solutions in a cloud-native environment.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 22, 'JOB TITLE'],\n",
       "    [46, 55, 'MUST HAVE'],\n",
       "    [198, 201, 'MUST HAVE'],\n",
       "    [675, 689, 'MUST HAVE'],\n",
       "    [693, 709, 'GOOD TO HAVE'],\n",
       "    [800, 806, 'MUST HAVE'],\n",
       "    [808, 812, 'GOOD TO HAVE'],\n",
       "    [934, 939, 'GOOD TO HAVE'],\n",
       "    [949, 952, 'GOOD TO HAVE'],\n",
       "    [954, 960, 'GOOD TO HAVE'],\n",
       "    [962, 966, 'GOOD TO HAVE']]}],\n",
       " ['SNOWFLAKE CLOUD DWHThe client is seeking a Snowflake Engineer. The key responsibilities include end-to-end implementation of the Snowflake cloud data warehouse, expertise in Snowflake data modeling, and using Snowflake SQL for ELT processes. The engineer should be skilled in developing complex stored procedures and have a strong understanding of data warehousing and ETL concepts. Hands-on experience with Snowflake utilities, including SnowSQL and SnowPipe, and deploying Snowflake features is essential. Data migration from RDBMS to the Snowflake cloud data warehouse is also a part of the role. The candidate should have a deep understanding of both relational and NoSQL data stores, including star and snowflake schemas and dimensional modeling. Data security and access controls design is a significant aspect of this position. Proficiency in RDBMS, complex SQL, PL/SQL, Unix Shell Scripting, performance tuning, and troubleshooting is required. Strong analytical, technical, and problem-solving skills, along with excellent communication abilities, are essential for this role. Overall, the client is looking for a highly skilled Snowflake Engineer to work on their cloud data warehousing solutions.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 22, 'JOB TITLE'],\n",
       "    [209, 218, 'MUST HAVE'],\n",
       "    [219, 222, 'MUST HAVE'],\n",
       "    [227, 230, 'MUST HAVE'],\n",
       "    [439, 446, 'GOOD TO HAVE'],\n",
       "    [451, 459, 'GOOD TO HAVE'],\n",
       "    [670, 675, 'MUST HAVE'],\n",
       "    [850, 855, 'GOOD TO HAVE'],\n",
       "    [878, 882, 'GOOD TO HAVE']]}],\n",
       " ['CLOUDERA ADMINThe client is seeking a Cloudera Administrator with extensive experience in managing large-scale Enterprise Hadoop environments. The responsibilities include designing, capacity planning, setting up clusters, ensuring security, performance tuning, and ongoing monitoring. The administrator should have in-depth knowledge of the Cloudera CDH distribution and be capable of installing, configuring, and monitoring all services within the CDH stack. Proficiency in core Cloudera Hadoop services, such as HDFS, MapReduce, Kafka, Spark, Hive, Impala, HBASE, Kudu, Sqoop, and Oozie, is crucial. Additionally, the role involves the administration and support of RHEL Linux operating systems, databases, and hardware in an enterprise setting. Expertise in system administration, programming skills, storage capacity management, debugging, and performance tuning is essential, along with proficiency in shell scripting. Experience with supporting Pepperdata installations is also a valuable skill for this position. Overall, the client is looking for a highly skilled Cloudera Administrator to manage and optimize their Hadoop infrastructure.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 17, 'JOB TITLE'],\n",
       "    [38, 46, 'MUST HAVE'],\n",
       "    [351, 354, 'GOOD TO HAVE'],\n",
       "    [490, 496, 'MUST HAVE'],\n",
       "    [515, 519, 'GOOD TO HAVE'],\n",
       "    [521, 530, 'GOOD TO HAVE'],\n",
       "    [532, 537, 'GOOD TO HAVE'],\n",
       "    [539, 544, 'GOOD TO HAVE'],\n",
       "    [546, 550, 'GOOD TO HAVE'],\n",
       "    [552, 558, 'GOOD TO HAVE'],\n",
       "    [560, 565, 'GOOD TO HAVE'],\n",
       "    [567, 571, 'GOOD TO HAVE'],\n",
       "    [573, 578, 'GOOD TO HAVE'],\n",
       "    [584, 589, 'GOOD TO HAVE'],\n",
       "    [674, 679, 'MUST HAVE'],\n",
       "    [952, 962, 'GOOD TO HAVE']]}],\n",
       " ['DATA SCIENTISTThe client is seeking a Data Scientist with to join their team. The Data Scientist will play a crucial role in helping clients understand the value of AI and advanced analytics for their business. The responsibilities include working closely within a data scientist team, developing and expanding knowledge within projects, and engaging customers to optimize their data monetization processes using AI techniques such as NLP, Neural Net, and Deep Learning. The ideal candidate should have strong experience in Data Sciences and Machine Learning, including the development of client models like Supervised and Unsupervised learning, as well as expertise in Natural Language Processing and linguistic analysis. The Data Scientist should be a thought leader capable of conceptualizing a big vision and defining plans for machine learning, metrics, and deliverables. Strong technical proficiency in programming, design techniques, and various technologies is essential, along with excellent communication skills and the ability to lead and manage small teams. Overall, the client is looking for a highly skilled and innovative Data Scientist to help drive AI solutions for their clients.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 17, 'JOB TITLE'],\n",
       "    [435, 438, 'MUST HAVE'],\n",
       "    [440, 450, 'GOOD TO HAVE'],\n",
       "    [456, 470, 'GOOD TO HAVE'],\n",
       "    [542, 558, 'MUST HAVE']]}],\n",
       " ['AUTOMATION TESTERThe client is looking for an Automation Tester who will be responsible for implementing automation using JAVA, Selenium Webdriver, Selenium Grid, Maven, and Cucumber. The key responsibilities include understanding and examining test requirements, designing and implementing an automation framework, setting up Selenium test environments, creating test cases with Selenium commands and element locators, and using JUnit/TestNG annotations and Java programming to escalate test cases. The tester is also responsible for maintaining automation resources, preparing test cases in the preferred language, continuously enhancing test case scripts to develop robust test scripts, and resolving bugs and assigning new issues to the development team once tests are executed. In summary, the client is seeking an Automation Tester with expertise in Selenium automation and related technologies to ensure the quality and reliability of their software through automated testing.\\r\\n\\r\\n',\n",
       "  {'entities': [[46, 63, 'JOB TITLE'],\n",
       "    [122, 126, 'MUST HAVE'],\n",
       "    [128, 136, 'MUST HAVE'],\n",
       "    [148, 161, 'GOOD TO HAVE'],\n",
       "    [163, 168, 'GOOD TO HAVE'],\n",
       "    [174, 187, 'GOOD TO HAVE'],\n",
       "    [430, 442, 'GOOD TO HAVE']]}],\n",
       " ['DEVOPS ENGINEERThe client is seeking a DevOps Engineer with a primary focus on Kubernetes and container orchestration. The role involves designing, implementing, and maintaining scalable and reliable cloud infrastructure using Kubernetes, as well as developing and managing CI/CD pipelines for efficient software delivery with automated testing, deployment, and monitoring. Collaboration with development teams to optimize application deployments and troubleshoot issues related to Kubernetes and containerized environments is key. The engineer should also be responsible for implementing and maintaining monitoring and alerting systems for Kubernetes clusters and applications, ensuring high availability and performance. Infrastructure-as-code practices using tools like Terraform or CloudFormation for managing Kubernetes infrastructure is expected. Staying up-to-date with emerging technologies and trends related to Kubernetes and containerization is crucial. Qualifications include 3+ years of DevOps experience with a strong focus on Kubernetes, a deep understanding of Kubernetes concepts, proficiency in scripting and programming languages, experience with cloud platforms, knowledge of CI/CD concepts and tools, familiarity with infrastructure-as-code, and expertise in containerization technologies. Strong troubleshooting and problem-solving skills, leadership qualities, and effective communication and collaboration with cross-functional teams are also important. In summary, the client is looking for a DevOps Engineer who can lead Kubernetes and container orchestration efforts, automate processes, and ensure the reliability and scalability of cloud infrastructure.\\r\\n\\r\\n',\n",
       "  {'entities': [[39, 54, 'JOB TITLE'],\n",
       "    [79, 89, 'MUST HAVE'],\n",
       "    [274, 279, 'MUST HAVE'],\n",
       "    [652, 660, 'GOOD TO HAVE'],\n",
       "    [773, 782, 'GOOD TO HAVE'],\n",
       "    [786, 800, 'GOOD TO HAVE'],\n",
       "    [988, 990, 'EXPERIENCE'],\n",
       "    [1000, 1006, 'MUST HAVE']]}],\n",
       " [\"Back-end Data Base Engineer:The client is seeking a Back-End Database Engineer, particularly excelling in Python, Flask, and Django framework for Rest API development, Swagger, and the automation of unit testing. This role requires excellent knowledge of SQL, particularly using Postgres, and experience with No-SQL databases is desirable. The engineer should be capable of setting up AWS for projects and integrating various services such as EC2, S3, Lambda, EKS, and other serverless services. Proficiency in using Bitbucket for source code management, familiarity with Agile methodologies and Software Development Life Cycle (SDLC), and knowledge of networking and web application development components are beneficial. Familiarity with front-end technologies like Angular and React is desirable, and the engineer should have strong communication skills and the ability to work as an individual contributor. Understanding how to integrate multiple data sources and databases into a unified system, knowledge of Python's threading limitations and multi-process architecture, and expertise in writing reusable, testable, and efficient code are vital. In summary, the client is looking for a Back-End Database Engineer with a strong Python and database development background, who can work on various aspects of back-end development and integration with front-end technologies.\\r\\n\\r\\n\",\n",
       "  {'entities': [[0, 27, 'JOB TITLE'],\n",
       "    [106, 112, 'MUST HAVE'],\n",
       "    [114, 119, 'GOOD TO HAVE'],\n",
       "    [125, 131, 'GOOD TO HAVE'],\n",
       "    [255, 258, 'MUST HAVE'],\n",
       "    [279, 287, 'GOOD TO HAVE'],\n",
       "    [309, 315, 'MUST HAVE'],\n",
       "    [385, 388, 'MUST HAVE'],\n",
       "    [443, 446, 'GOOD TO HAVE'],\n",
       "    [448, 450, 'GOOD TO HAVE'],\n",
       "    [452, 458, 'GOOD TO HAVE'],\n",
       "    [460, 464, 'GOOD TO HAVE'],\n",
       "    [572, 577, 'GOOD TO HAVE'],\n",
       "    [768, 775, 'GOOD TO HAVE'],\n",
       "    [780, 785, 'GOOD TO HAVE']]}],\n",
       " ['QA ENGINEERThe client is in search of a QA Engineer with strong experience in quality assurance and testing. This role encompasses various aspects of testing, including API testing, writing effective SQL queries, and test automation. Performance testing is also part of the responsibilities. The engineer should be familiar with test management tools such as JIRA and ALM, as well as experienced in both Waterfall and Agile methodologies and the Software Testing Lifecycle. Knowledge of CRM and the Transportation domain is considered a valuable asset. The ability to analyze defects, provide root-cause analysis, and create and execute test scenarios, test cases, and test data is essential. Effective communication, both verbal and written, is required, along with proficiency in interacting with customers and stakeholders. The QA Engineer should possess strong analytical, problem-solving, and troubleshooting skills, with the ability to adapt quickly to new technologies, methodologies, and systems. Flexibility to work in shifts and as a functional tester is also expected. In summary, the client is seeking a versatile QA Engineer with experience in multiple testing aspects who can effectively ensure the quality of their software products and adapt to changing requirements.\\r\\n\\r\\n',\n",
       "  {'entities': [[40, 51, 'JOB TITLE'],\n",
       "    [169, 172, 'GOOD TO HAVE'],\n",
       "    [200, 203, 'MUST HAVE'],\n",
       "    [359, 363, 'GOOD TO HAVE'],\n",
       "    [368, 371, 'GOOD TO HAVE'],\n",
       "    [404, 413, 'MUST HAVE'],\n",
       "    [418, 423, 'MUST HAVE']]}],\n",
       " ['FRONT END DEVELOPER:The client is looking for an experienced Front-End Developer with a who can play a significant role in implementing designs, creating Low-Level Design (LLD), and working on full-stack development using the latest versions of technologies like Angular, Express JS, and TypeScript. This role requires expertise in unit testing automation, code deployment, and in-depth knowledge of frontend and integration technologies such as HTML, CSS, JavaScript, JSON, SOAP, and REST. The developer should excel at building highly scalable, resilient, and secure applications and have experience with databases including MySQL, Postgres, MarkLogic (NoSQL), and Elastic. Familiarity with Swagger API documentation, API integrations in various technologies like Java, Python, and third-party APIs is essential. Security implementation using Single Sign-On (SSO) is also a requirement. Strong communication skills are crucial, as this role involves direct client interaction, requirement discussions, and providing demonstrations. The developer should be capable of working independently to solve problems, handle production deployments, and address production defects within SLAs. In summary, the client is seeking a Front-End Developer who can contribute to the development of scalable and secure applications, work directly with clients, and demonstrate expertise in various front-end and integration technologies.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 19, 'JOB TITLE'],\n",
       "    [263, 270, 'GOOD TO HAVE'],\n",
       "    [272, 282, 'GOOD TO HAVE'],\n",
       "    [288, 299, 'GOOD TO HAVE'],\n",
       "    [446, 450, 'MUST HAVE'],\n",
       "    [452, 455, 'MUST HAVE'],\n",
       "    [457, 467, 'MUST HAVE'],\n",
       "    [469, 473, 'MUST HAVE'],\n",
       "    [475, 479, 'GOOD TO HAVE'],\n",
       "    [485, 490, 'GOOD TO HAVE'],\n",
       "    [627, 632, 'MUST HAVE'],\n",
       "    [634, 642, 'GOOD TO HAVE'],\n",
       "    [644, 653, 'GOOD TO HAVE'],\n",
       "    [655, 660, 'MUST HAVE'],\n",
       "    [720, 723, 'GOOD TO HAVE'],\n",
       "    [766, 770, 'GOOD TO HAVE'],\n",
       "    [772, 778, 'GOOD TO HAVE']]}],\n",
       " ['BUSINESS ANALYST:The client is seeking a Business Analyst who will serve as a vital link between business and technology leaders, conducting workshops with business stakeholders and demonstrating strong expertise in requirements gathering and grooming, potentially even taking on a Product Owner role. Effective communication skills, especially when interacting with the executive leadership team, are a crucial requirement. Knowledge and understanding of the Life Sciences industry and regulatory processes are essential, with additional value placed on familiarity with Analytics and Machine Learning. The role involves ensuring the update of all Software Development Life Cycle (SDLC) and validation deliverables in Asset Management and JIRA, including documenting User Stories. The Business Analyst is responsible for collecting and documenting requirements for a new Regulatory Analytics platform, reports, dashboards, and models. They will also support the definition of requirements for experiments based on business-prioritized use cases and assist in test planning and script creation when necessary. Maintaining the Product Backlog, generating status reports, and working closely with cross-functional teams are integral aspects of the role. In summary, the client is looking for a Business Analyst with a deep understanding of both business and technology, the ability to gather and document requirements effectively, and a strong grasp of Life Sciences and regulatory processes, with the potential to contribute to analytics and machine learning initiatives.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 16, 'JOB TITLE'],\n",
       "    [572, 581, 'MUST HAVE'],\n",
       "    [586, 603, 'MUST HAVE'],\n",
       "    [682, 686, 'GOOD TO HAVE'],\n",
       "    [740, 744, 'GOOD TO HAVE']]}],\n",
       " [\"AWS ADMINThe client is in search of an AWS Administrator with a focus on managing Amazon Web Services (AWS) cloud infrastructure, particularly within computing services such as Amazon Machine Image (AMI) and Elastic Compute Cloud (EC2). This role involves networking expertise in AWS services, including Virtual Private Cloud (VPC) and Amazon Route53, and proficiency in AWS Administration and Security Services, such as Identity and Access Management (IAM) and CloudWatch. Key responsibilities encompass creating and managing EC2 instances and working in VPC environments using AMIs, as well as maintaining backups and snapshots. Monitoring and optimizing EC2 performance using CloudWatch is essential, along with experience in system health monitoring and handling performance optimization. The ideal candidate should have strong knowledge of both RedHat Linux and Windows OS, excellent problem-solving skills, and the ability to manage multiple projects simultaneously, adapting to changing business needs. This role includes installing, configuring, and supporting Linux and Windows Servers on Infrastructure as a Service (IaaS), as well as monitoring cloud instances' performance and collaborating with internal teams to provide effective infrastructure support. The client is seeking someone with 4 to 5 years of experience in infrastructure and Linux/Windows operating system support, and who can work effectively in a 24/7 uptime environment. Strong verbal and written communication skills are a must. In summary, the client is looking for an AWS Administrator capable of efficiently managing AWS infrastructure, ensuring its performance and security, and collaborating effectively with a variety of teams.\\r\\n\\r\\n\",\n",
       "  {'entities': [[39, 56, 'JOB TITLE'],\n",
       "    [103, 106, 'MUST HAVE'],\n",
       "    [199, 202, 'GOOD TO HAVE'],\n",
       "    [231, 234, 'GOOD TO HAVE'],\n",
       "    [327, 330, 'GOOD TO HAVE'],\n",
       "    [453, 456, 'GOOD TO HAVE'],\n",
       "    [462, 473, 'GOOD TO HAVE'],\n",
       "    [850, 856, 'MUST HAVE'],\n",
       "    [1308, 1309, 'EXPERIENCE'],\n",
       "    [1352, 1365, 'GOOD TO HAVE']]}],\n",
       " ['AWS SOLUTION ARCHITECTThe client is seeking an AWS Solution Architect with a specialized skill set in designing and developing big data solutions within the AWS Cloud environment. The ideal candidate should have expertise in utilizing AWS services such as Redshift, S3, EC2, and RDS, along with programming in Python and using data integration tools like Talend. Additionally, the architect should be proficient in designing data processing and visualization with Python and Tableau, optimizing and tuning Redshift databases, scripting in Shell or Python, and working with REST APIs for data extraction. Knowledge of CI/CD and DevOps practices is essential, as is the ability to maintain and enhance data integration pipelines for performance. Experience in data migration from on-premises to AWS, familiarity with Cloudera, and domain knowledge in Sales and Digital Marketing are valuable assets. The role also involves working with cross-functional teams, adhering to Agile methodologies, and driving technical requirements sessions to ensure the quality of software development. Strong communication skills, attention to detail, analytical thinking, and problem-solving capabilities are highly sought after in the ideal candidate. In summary, the client is looking for an AWS Solution Architect who can design and implement complex data solutions in AWS, manage data pipelines, and work collaboratively with various teams to meet business needs.\\r\\n\\r\\n',\n",
       "  {'entities': [[47, 69, 'JOB TITLE'],\n",
       "    [235, 238, 'MUST HAVE'],\n",
       "    [256, 264, 'GOOD TO HAVE'],\n",
       "    [266, 268, 'GOOD TO HAVE'],\n",
       "    [270, 273, 'GOOD TO HAVE'],\n",
       "    [279, 282, 'GOOD TO HAVE'],\n",
       "    [310, 316, 'MUST HAVE'],\n",
       "    [355, 362, 'GOOD TO HAVE'],\n",
       "    [475, 482, 'GOOD TO HAVE'],\n",
       "    [573, 582, 'GOOD TO HAVE'],\n",
       "    [815, 823, 'GOOD TO HAVE'],\n",
       "    [970, 975, 'GOOD TO HAVE']]}],\n",
       " ['MULESOFT DEVELOPER:The client is looking for a MuleSoft Developer with a hands-on application development experience in MuleSoft ESB/API, particularly focusing on Mule version 3 onwards, and MuleSoft CloudHub. Proficiency in Mule 4 is a mandatory requirement, with certification considered a plus. The ideal candidate should have expertise in integrating Java frameworks, including Spring Integration, and working with web services, REST APIs, and MuleSoft APIs. Experience with CI/CD pipelines is important for this role. Additionally, any familiarity with the MuleSoft Real-Time Processing Framework (RTF) is a valuable asset. In summary, the client is seeking a MuleSoft Developer with strong experience in MuleSoft versions 3 and 4, Java integration, and API development, who can contribute to application development and integration within their organization.\\r\\n\\r\\n',\n",
       "  {'entities': [[0, 18, 'JOB TITLE'],\n",
       "    [129, 136, 'MUST HAVE'],\n",
       "    [191, 209, 'GOOD TO HAVE'],\n",
       "    [355, 359, 'MUST HAVE'],\n",
       "    [382, 388, 'GOOD TO HAVE'],\n",
       "    [433, 437, 'GOOD TO HAVE'],\n",
       "    [448, 456, 'GOOD TO HAVE'],\n",
       "    [603, 606, 'GOOD TO HAVE']]}],\n",
       " ['BIG DATA DEVELOPER:The client is seeking a Big Data Developer with in designing, developing, and deploying applications within Big Data ecosystems. The ideal candidate should have strong hands-on expertise in key Big Data technologies such as Hadoop, Spark, Scala, Elastic, and Hive/Impala SQL, along with experience in writing Shell scripts and familiarity with data loading tools like Sqoop, Flume, and Kafka. Knowledge of workflow schedulers like Oozie, as well as NoSQL databases (Hbase, MongoDB) and traditional RDBMS (Oracle), is essential. Additional value is placed on experience with StreamSets pipeline development in both cloud and on-premises environments. The role requires strong analytical and design skills, the ability to translate business requirements into efficient technical solutions, and excellent troubleshooting skills. An eagerness to learn new technologies and effective communication skills are also highly desired in this position. In summary, the client is looking for a skilled Big Data Developer to contribute to the design, development, and deployment of applications within large-scale enterprise environments, with a strong emphasis on Big Data technologies and tools.\\r\\n\\t\\r\\n',\n",
       "  {'entities': [[0, 18, 'JOB TITLE'],\n",
       "    [243, 249, 'MUST HAVE'],\n",
       "    [251, 256, 'GOOD TO HAVE'],\n",
       "    [258, 263, 'GOOD TO HAVE'],\n",
       "    [265, 272, 'GOOD TO HAVE'],\n",
       "    [278, 289, 'GOOD TO HAVE'],\n",
       "    [387, 392, 'GOOD TO HAVE'],\n",
       "    [394, 399, 'GOOD TO HAVE'],\n",
       "    [405, 411, 'GOOD TO HAVE'],\n",
       "    [450, 455, 'GOOD TO HAVE'],\n",
       "    [468, 473, 'MUST HAVE']]}],\n",
       " ['BIG QUERY ENGINEERThe client is in need of a BigQuery Engineer with experience in Google Cloud Platform (GCP) data analytics services, particularly in using BigQuery, Dataflow, and data storage solutions. The primary responsibility for this role involves developing data analytics processes, implementing KPIs, and managing metadata storage. The engineer is expected to handle complex data transformations, conduct unit testing, and validate business logics. In essence, the client is looking for a professional who can effectively utilize GCP data analytics tools to support data processing, analytics, and KPI implementation within their organization.\\r\\n\\r\\n',\n",
       "  {'entities': [[45, 62, 'JOB TITLE'],\n",
       "    [105, 108, 'MUST HAVE'],\n",
       "    [157, 165, 'GOOD TO HAVE'],\n",
       "    [167, 175, 'GOOD TO HAVE'],\n",
       "    [181, 193, 'GOOD TO HAVE'],\n",
       "    [305, 309, 'MUST HAVE']]}],\n",
       " ['AWS ENGINEERThe client is seeking an AWS Engineer with a strong background in data warehousing, data lakes, data engineering, and data management. This role requires experience with specific AWS services such as Amazon Glue, Redshift, Lambda, and API Gateway, as well as familiarity with other AWS services like EMR, EC2, and S3. The ideal candidate should have a proven track record in Python and/or Java, along with expertise in Elastic Search. The role involves building scripts and processes for ETL (Extract, Transform, Load), data enrichment, alerting, and indexing for high-rate event streams. Designing and developing scalable code within a Big Data environment is a key responsibility, and experience with tools like Scoop, Flume, Oozie, or Zookeeper is beneficial. Hands-on experience with Apache Hive and Apache Spark is crucial, and a background in the life sciences industry is highly advantageous for this role. In summary, the client is looking for an AWS Engineer proficient in data management and processing within an AWS ecosystem, particularly in the context of high-rate event streams and Big Data.\\r\\n\\r\\n',\n",
       "  {'entities': [[37, 49, 'JOB TITLE'],\n",
       "    [191, 194, 'MUST HAVE'],\n",
       "    [212, 223, 'GOOD TO HAVE'],\n",
       "    [225, 233, 'GOOD TO HAVE'],\n",
       "    [235, 241, 'GOOD TO HAVE'],\n",
       "    [247, 250, 'GOOD TO HAVE'],\n",
       "    [251, 258, 'GOOD TO HAVE'],\n",
       "    [312, 315, 'GOOD TO HAVE'],\n",
       "    [317, 320, 'GOOD TO HAVE'],\n",
       "    [326, 329, 'GOOD TO HAVE'],\n",
       "    [387, 393, 'MUST HAVE'],\n",
       "    [401, 405, 'GOOD TO HAVE'],\n",
       "    [500, 503, 'MUST HAVE'],\n",
       "    [726, 731, 'GOOD TO HAVE'],\n",
       "    [733, 739, 'GOOD TO HAVE'],\n",
       "    [740, 745, 'GOOD TO HAVE'],\n",
       "    [750, 759, 'GOOD TO HAVE'],\n",
       "    [807, 811, 'GOOD TO HAVE'],\n",
       "    [823, 828, 'GOOD TO HAVE']]}],\n",
       " ['PYTHON DEVELOPERThe client is seeking an experienced Python Developer with a minimum hands-on development experience. The responsibilities of this role include helping to design and implement functional requirements, building efficient back-end features in Python, integrating front-end components into applications, managing testing and bug fixes, and preparing technical documentation. Collaboration with UX/UI designers for code implementation, suggesting software enhancements, and ensuring improvements are also part of the role. The client requires familiarity with Python frameworks like Django, Flask, and Bottle, as well as experience with Amazon Web Services (AWS) and REST APIs. Knowledge of databases (particularly Postgres) and SQL is essential, and proficiency in JavaScript and the AngularJs/Angular framework is considered a plus. The candidate should also be well-versed in source code management tools like Bitbucket/GIT, have experience with Agile methodology and the Software Development Life Cycle (SDLC), and possess strong attention to detail. The role additionally calls for an understanding of integrating multiple data sources and databases into a unified system, familiarity with event-driven programming in Python, and the ability to write reusable, testable, and efficient code\\r\\n\\r\\n',\n",
       "  {'entities': [[53, 69, 'JOB TITLE'],\n",
       "    [407, 412, 'GOOD TO HAVE'],\n",
       "    [595, 601, 'GOOD TO HAVE'],\n",
       "    [603, 608, 'GOOD TO HAVE'],\n",
       "    [614, 620, 'GOOD TO HAVE'],\n",
       "    [670, 673, 'MUST HAVE'],\n",
       "    [679, 689, 'GOOD TO HAVE'],\n",
       "    [741, 744, 'MUST HAVE'],\n",
       "    [778, 788, 'MUST HAVE'],\n",
       "    [797, 814, 'GOOD TO HAVE'],\n",
       "    [925, 938, 'GOOD TO HAVE'],\n",
       "    [961, 966, 'GOOD TO HAVE'],\n",
       "    [1020, 1024, 'GOOD TO HAVE']]}],\n",
       " ['SNOWFLAKE ENGINEERThe requirement for a Snowflake Engineer is seeking an individual with, specifically with at least 4 years as a Snowflake Engineer. This role emphasizes the design and implementation of large-scale, distributed big data solutions, with a track record of creating end-to-end Big Data Lakes integrated with Business Intelligence. The candidate should have expertise in Snowflake cloud data warehousing, including data modeling, ELT using Snowflake SQL, stored procedures, and ETL concepts. Proficiency in Snowflake utilities, data migration from RDBMS to Snowflake, and a deep understanding of relational and NoSQL data stores is crucial. Data security and access control design, as well as strong technical, analytical, and troubleshooting skills, are essential. Effective communication skills are also required for this role.\\r\\n\\r\\n',\n",
       "  {'entities': [[40, 58, 'JOB TITLE'],\n",
       "    [117, 118, 'EXPERIENCE'],\n",
       "    [130, 139, 'MUST HAVE'],\n",
       "    [434, 442, 'GOOD TO HAVE'],\n",
       "    [464, 467, 'MUST HAVE'],\n",
       "    [492, 495, 'MUST HAVE'],\n",
       "    [625, 630, 'MUST HAVE']]}],\n",
       " ['DATA GOVERNANCE LEADThe requirement for a Data Governance Lead primarily seeks an experienced business analyst with over 12 years of expertise in medical device regulatory registrations, particularly focusing on guidelines such as 510K, EU MDR, and IVDR. This role involves a deep understanding of product quality processes, including CAPA, NCMR, SCAR, and medical device incident reporting. Additionally, the candidate should possess proficiency in PL/SQL, enabling them to conduct source data analysis and collaborate with both IT and business teams within the software development life cycle. The role emphasizes the need for hands-on experience within medical device companies, where the candidate would design and implement processes for managing product regulatory compliance and medical device product registration solutions, including EU MDR, IVDR, 510-K, and UDI solutions using Product Lifecycle Management (PLM). Strong communication, teamwork, and interpersonal skills are essential for success in this position.\\r\\n\\r\\n',\n",
       "  {'entities': [[42, 62, 'JOB TITLE'],\n",
       "    [121, 123, 'EXPERIENCE'],\n",
       "    [231, 235, 'GOOD TO HAVE'],\n",
       "    [237, 243, 'GOOD TO HAVE'],\n",
       "    [249, 254, 'GOOD TO HAVE'],\n",
       "    [335, 339, 'GOOD TO HAVE'],\n",
       "    [341, 345, 'GOOD TO HAVE'],\n",
       "    [347, 351, 'GOOD TO HAVE'],\n",
       "    [450, 456, 'MUST HAVE'],\n",
       "    [843, 849, 'GOOD TO HAVE'],\n",
       "    [851, 855, 'GOOD TO HAVE'],\n",
       "    [857, 862, 'GOOD TO HAVE'],\n",
       "    [868, 871, 'GOOD TO HAVE'],\n",
       "    [918, 921, 'GOOD TO HAVE']]}],\n",
       " ['RELTIO MDMThe requirement for a Reltio MDM (Master Data Management) professional involves designing and implementing complex data management solutions using the Reltio platform to meet specific organizational needs. This role requires understanding business requirements, translating them into technical specifications, and creating design documents. Collaboration with various team members, including business analysts and project managers, is essential for successful project delivery. Additionally, the role involves code reviews, ensuring code quality, troubleshooting technical issues, and leading discussions with clients and internal teams. The ideal candidate should have a solid understanding of data management concepts, experience with the Reltio platform, proficiency in cloud-based systems, and strong communication and collaboration skills.\\r\\n\\r\\n',\n",
       "  {'entities': [[32, 42, 'JOB TITLE'],\n",
       "    [161, 167, 'MUST HAVE'],\n",
       "    [705, 720, 'MUST HAVE'],\n",
       "    [783, 794, 'GOOD TO HAVE']]}],\n",
       " ['AI ARCHITECTThe AI Architect role entails designing and delivering machine learning (ML) architecture patterns suitable for both native and hybrid cloud environments. Responsibilities include researching and recommending technical approaches for solving complex ML model training and deployment challenges in enterprise applications. This role requires hands-on programming and architectural expertise in languages like Python, Java, R, or SCALA, with a minimum of 6+ years of experience in enterprise application development. The AI Architect must have a strong background in implementing and deploying ML solutions, hands-on experience with statistical packages and ML libraries, and a deep understanding of statistical analysis and modeling. Additionally, they should be well-versed in various data storage technologies, have experience in solution architecture roles, and be familiar with open source software. Effective problem-solving and excellent communication skills are crucial. The ideal candidate should have a demonstrated technical expertise in AI, ML, and deep learning and a background in integrating these technologies into large-scale enterprise applications, particularly in cloud environments like Amazon Web Services and Microsoft Azure. Specialization in AI/ML stack components and developing best practices for ML life-cycle capabilities is also expected, including data collection, preparation, feature engineering, model management, MLOps, deployment, monitoring, and tuning.\\r\\n\\r\\n\\r\\n',\n",
       "  {'entities': [[16, 28, 'JOB TITLE'],\n",
       "    [67, 83, 'MUST HAVE'],\n",
       "    [420, 426, 'GOOD TO HAVE'],\n",
       "    [428, 432, 'GOOD TO HAVE'],\n",
       "    [434, 436, 'GOOD TO HAVE'],\n",
       "    [440, 445, 'GOOD TO HAVE'],\n",
       "    [465, 467, 'EXPERIENCE'],\n",
       "    [1059, 1061, 'MUST HAVE'],\n",
       "    [1063, 1066, 'MUST HAVE'],\n",
       "    [1071, 1084, 'MUST HAVE'],\n",
       "    [1218, 1237, 'GOOD TO HAVE'],\n",
       "    [1242, 1258, 'GOOD TO HAVE'],\n",
       "    [1458, 1463, 'GOOD TO HAVE']]}]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec36c334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.577475Z",
     "iopub.status.busy": "2024-01-02T12:42:11.576623Z",
     "iopub.status.idle": "2024-01-02T12:42:11.584445Z",
     "shell.execute_reply": "2024-01-02T12:42:11.583487Z"
    },
    "papermill": {
     "duration": 0.025266,
     "end_time": "2024-01-02T12:42:11.586522",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.561256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Job Title: AWS Administrator Location: GA, Atlanta, (Remote) Duration: 1 Year Key Responsibilities: Establish configuration, compliance, and audit program to effectively manage AWS accounts and resources. Review cloud resources and cost drivers, compute, storage, network, managed services, database and service licenses, marketplace, and accounts. As part of our Managed Services offering for clients; monitor rightsizing, elasticity, storage optimization, and identify unused resources. Maintain cloud-based servers patching vulnerabilities, backup/restore operations, provision new servers, configure firewalls, configure monitoring systems. Manage account containerization and tagging for internal cost centers and end client billing feeds. Monitor cost allocations, security compliance, budgets and set alerts across accounts, workloads, and users. Operational monitoring to identify and address issues. Use AWS Config to monitor and track resource configuration. Set standards for resource configurations, evaluate configuration compliance, and risk, and remediate configuration drift. Use AWS CloudTrail for compliance audits by recording and storing event logs for actions made within AWS accounts. Set controls to monitor to assure compliance. Familiarity with AWS security best practices and hands-on experience in implementing security controls and compliance requirements with services like AWS Security Hub, Guard Duty, and AWS Audit Manager. Experience with monitoring tools, middleware software and ITSM tools a plus. Qualifications: Bachelor’s degree in computer science or a related field from an accredited college or university. Eight (8) years of Cloud Administration Experience. Excellent technical knowledge of IT Infrastructure, including switches, routers, server operating systems and hardware, storage arrays, and security applications. Strong system administration experience in Windows and Linux environments. Experience with automation using an established framework (SaltStack, Puppet, Chef, Ansible, etc.). Strong technical knowledge of current protocols, operating systems, and standards. Able to read and understand technical manuals and procedural documentation.  Experience in public cloud environments, including AWS and/or Azure. Experience working in an ITIL-driven environment and working knowledge of ITIL principles and processes. ',\n",
       " {'entities': [[11, 28, 'JOB TITLE'],\n",
       "   [39, 41, 'LOCATION'],\n",
       "   [43, 50, 'LOCATION'],\n",
       "   [53, 59, 'LOCATION'],\n",
       "   [71, 72, 'EXPERIENCE'],\n",
       "   [177, 180, 'MUST HAVE'],\n",
       "   [205, 211, 'GOOD TO HAVE'],\n",
       "   [403, 410, 'GOOD TO HAVE'],\n",
       "   [424, 434, 'GOOD TO HAVE'],\n",
       "   [604, 613, 'GOOD TO HAVE'],\n",
       "   [771, 779, 'GOOD TO HAVE'],\n",
       "   [1100, 1110, 'MUST HAVE'],\n",
       "   [1514, 1518, 'GOOD TO HAVE'],\n",
       "   [1906, 1913, 'MUST HAVE'],\n",
       "   [1918, 1923, 'MUST HAVE'],\n",
       "   [1997, 2006, 'GOOD TO HAVE'],\n",
       "   [2008, 2014, 'GOOD TO HAVE'],\n",
       "   [2016, 2020, 'GOOD TO HAVE'],\n",
       "   [2022, 2029, 'GOOD TO HAVE'],\n",
       "   [2260, 2266, 'GOOD TO HAVE']]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53362c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.615016Z",
     "iopub.status.busy": "2024-01-02T12:42:11.614668Z",
     "iopub.status.idle": "2024-01-02T12:42:11.620318Z",
     "shell.execute_reply": "2024-01-02T12:42:11.619473Z"
    },
    "papermill": {
     "duration": 0.022099,
     "end_time": "2024-01-02T12:42:11.622393",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.600294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe530073",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:11.650736Z",
     "iopub.status.busy": "2024-01-02T12:42:11.650390Z",
     "iopub.status.idle": "2024-01-02T12:42:21.668857Z",
     "shell.execute_reply": "2024-01-02T12:42:21.667829Z"
    },
    "papermill": {
     "duration": 10.035004,
     "end_time": "2024-01-02T12:42:21.671122",
     "exception": false,
     "start_time": "2024-01-02T12:42:11.636118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\r\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\r\n",
      "/kaggle/working/config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config /kaggle/input/base-config/base_config.cfg /kaggle/working/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3737a392",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:21.700678Z",
     "iopub.status.busy": "2024-01-02T12:42:21.700335Z",
     "iopub.status.idle": "2024-01-02T12:42:21.710115Z",
     "shell.execute_reply": "2024-01-02T12:42:21.709335Z"
    },
    "papermill": {
     "duration": 0.026642,
     "end_time": "2024-01-02T12:42:21.712080",
     "exception": false,
     "start_time": "2024-01-02T12:42:21.685438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_spacy_doc(file,data):\n",
    "  nlp=spacy.blank('en')\n",
    "  db= DocBin()\n",
    "  for text, annot in tqdm(data):\n",
    "    doc=nlp.make_doc(text)\n",
    "    annot=annot['entities']\n",
    "\n",
    "    ents=[]\n",
    "    entity_indices=[]\n",
    "\n",
    "    for start, end, label in annot:\n",
    "      skip_entity= False\n",
    "      for idx in range(start,end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity=True\n",
    "          break\n",
    "      if skip_entity==True:\n",
    "        continue\n",
    "\n",
    "      entity_indices=entity_indices+list(range(start,end))\n",
    "\n",
    "      try:\n",
    "        span=doc.char_span(start,end,label=label,alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        err_data=str([start,end])+ '  ' + str(text)+'/n'\n",
    "        file.write(err_data)\n",
    "\n",
    "      else:\n",
    "        ents.append(span)\n",
    "    try:\n",
    "      doc.ents=ents\n",
    "      db.add(doc)\n",
    "    except:\n",
    "      pass\n",
    "  return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c8c754",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:21.740858Z",
     "iopub.status.busy": "2024-01-02T12:42:21.740043Z",
     "iopub.status.idle": "2024-01-02T12:42:22.917153Z",
     "shell.execute_reply": "2024-01-02T12:42:22.916291Z"
    },
    "papermill": {
     "duration": 1.193894,
     "end_time": "2024-01-02T12:42:22.919534",
     "exception": false,
     "start_time": "2024-01-02T12:42:21.725640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test=train_test_split(annotations,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc5e58f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:22.948266Z",
     "iopub.status.busy": "2024-01-02T12:42:22.947897Z",
     "iopub.status.idle": "2024-01-02T12:42:22.954663Z",
     "shell.execute_reply": "2024-01-02T12:42:22.953700Z"
    },
    "papermill": {
     "duration": 0.023594,
     "end_time": "2024-01-02T12:42:22.956907",
     "exception": false,
     "start_time": "2024-01-02T12:42:22.933313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeeaab70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:22.986247Z",
     "iopub.status.busy": "2024-01-02T12:42:22.985902Z",
     "iopub.status.idle": "2024-01-02T12:42:24.769068Z",
     "shell.execute_reply": "2024-01-02T12:42:24.768082Z"
    },
    "papermill": {
     "duration": 1.800879,
     "end_time": "2024-01-02T12:42:24.771481",
     "exception": false,
     "start_time": "2024-01-02T12:42:22.970602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 214.43it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 196.90it/s]\n"
     ]
    }
   ],
   "source": [
    "file=open('/kaggle/working/error.txt','w')\n",
    "db=get_spacy_doc(file,train)\n",
    "db.to_disk('/kaggle/working/train_data.spacy')\n",
    "\n",
    "db=get_spacy_doc(file,test)\n",
    "db.to_disk('/kaggle/working/test_data.spacy')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c9708f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T12:42:24.803594Z",
     "iopub.status.busy": "2024-01-02T12:42:24.802994Z",
     "iopub.status.idle": "2024-01-02T13:46:46.342952Z",
     "shell.execute_reply": "2024-01-02T13:46:46.341847Z"
    },
    "papermill": {
     "duration": 3861.559242,
     "end_time": "2024-01-02T13:46:46.345626",
     "exception": false,
     "start_time": "2024-01-02T12:42:24.786384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\r\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\r\n",
      "\u001b[38;5;4mℹ Saving to output directory: /kaggle/working\u001b[0m\r\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\r\n",
      "\u001b[1m\r\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\r\n",
      "[2024-01-02 12:42:35,682] [INFO] Set up nlp object from config\r\n",
      "[2024-01-02 12:42:35,723] [INFO] Pipeline: ['transformer', 'ner']\r\n",
      "[2024-01-02 12:42:35,730] [INFO] Created vocabulary\r\n",
      "[2024-01-02 12:42:35,730] [INFO] Finished initializing nlp object\r\n",
      "config.json: 100%|█████████████████████████████| 481/481 [00:00<00:00, 2.28MB/s]\r\n",
      "vocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 12.3MB/s]\r\n",
      "merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 21.5MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 1.36M/1.36M [00:00<00:00, 33.0MB/s]\r\n",
      "model.safetensors: 100%|██████████████████████| 499M/499M [00:02<00:00, 208MB/s]\r\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\r\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\r\n",
      "[2024-01-02 12:43:05,217] [INFO] Initialized pipeline components: ['transformer', 'ner']\r\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\r\n",
      "\u001b[1m\r\n",
      "============================= Training pipeline =============================\u001b[0m\r\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\r\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\r\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \r\n",
      "---  ------  -------------  --------  ------  ------  ------  ------\r\n",
      "  0       0        3534.18    658.72    0.00    0.00    0.00    0.00\r\n",
      " 28     200      495436.45  52348.61   50.00   54.55   46.15    0.50\r\n",
      " 57     400       10909.86   4671.19   49.35   50.00   48.72    0.49\r\n",
      " 85     600        1590.12    360.03   47.37   48.65   46.15    0.47\r\n",
      "114     800          93.56    177.53   47.50   46.34   48.72    0.48\r\n",
      "142    1000          44.70    128.28   44.44   42.86   46.15    0.44\r\n",
      "171    1200          45.41    126.45   52.50   51.22   53.85    0.53\r\n",
      "200    1400          24.21     88.25   45.33   47.22   43.59    0.45\r\n",
      "228    1600          37.95    109.16   52.50   51.22   53.85    0.53\r\n",
      "257    1800          17.78     82.66   48.48   40.00   61.54    0.48\r\n",
      "285    2000          37.09    107.30   47.37   48.65   46.15    0.47\r\n",
      "314    2200          53.26    122.58   54.32   52.38   56.41    0.54\r\n",
      "342    2400          40.76    103.32   58.23   57.50   58.97    0.58\r\n",
      "371    2600          28.15     88.73   53.85   53.85   53.85    0.54\r\n",
      "400    2800          33.61     99.30   46.15   40.38   53.85    0.46\r\n",
      "428    3000          49.97    130.18   42.11   43.24   41.03    0.42\r\n",
      "457    3200          19.95     86.40   40.54   42.86   38.46    0.41\r\n",
      "485    3400          52.91    115.93   31.75   41.67   25.64    0.32\r\n",
      "514    3600          10.60     73.56   42.11   43.24   41.03    0.42\r\n",
      "542    3800           0.00     63.49   39.47   40.54   38.46    0.39\r\n",
      "571    4000           0.00     62.23   42.11   43.24   41.03    0.42\r\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\r\n",
      "/kaggle/working/model-last\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train /kaggle/working/config.cfg --output /kaggle/working/ --paths.train /kaggle/working/train_data.spacy --paths.dev /kaggle/working/test_data.spacy --gpu-id 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1120fdc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:46.387618Z",
     "iopub.status.busy": "2024-01-02T13:46:46.387206Z",
     "iopub.status.idle": "2024-01-02T13:46:50.909009Z",
     "shell.execute_reply": "2024-01-02T13:46:50.908228Z"
    },
    "papermill": {
     "duration": 4.545929,
     "end_time": "2024-01-02T13:46:50.911319",
     "exception": false,
     "start_time": "2024-01-02T13:46:46.365390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp=spacy.load('/kaggle/working/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71d6235c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:50.949346Z",
     "iopub.status.busy": "2024-01-02T13:46:50.948469Z",
     "iopub.status.idle": "2024-01-02T13:46:50.973308Z",
     "shell.execute_reply": "2024-01-02T13:46:50.972634Z"
    },
    "papermill": {
     "duration": 0.045618,
     "end_time": "2024-01-02T13:46:50.975168",
     "exception": false,
     "start_time": "2024-01-02T13:46:50.929550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/test-data/config.txt') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5745fcbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:51.013073Z",
     "iopub.status.busy": "2024-01-02T13:46:51.012523Z",
     "iopub.status.idle": "2024-01-02T13:46:52.609579Z",
     "shell.execute_reply": "2024-01-02T13:46:52.608579Z"
    },
    "papermill": {
     "duration": 1.617936,
     "end_time": "2024-01-02T13:46:52.611715",
     "exception": false,
     "start_time": "2024-01-02T13:46:50.993779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Hardware Architect, Machine Learning, Sunnyvale, CA, USA, 8, TensorFlow, PyTorch, TPU, Machine Learning)\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30551b82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:52.649708Z",
     "iopub.status.busy": "2024-01-02T13:46:52.649109Z",
     "iopub.status.idle": "2024-01-02T13:46:52.654882Z",
     "shell.execute_reply": "2024-01-02T13:46:52.654029Z"
    },
    "papermill": {
     "duration": 0.026962,
     "end_time": "2024-01-02T13:46:52.656950",
     "exception": false,
     "start_time": "2024-01-02T13:46:52.629988",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware Architect JOB TITLE\n",
      "Machine Learning MUST HAVE\n",
      "Sunnyvale LOCATION\n",
      "CA LOCATION\n",
      "USA LOCATION\n",
      "8 EXPERIENCE\n",
      "TensorFlow GOOD TO HAVE\n",
      "PyTorch GOOD TO HAVE\n",
      "TPU GOOD TO HAVE\n",
      "Machine Learning MUST HAVE\n",
      "Hardware Architect, Machine Learning\n",
      "corporate_fare\n",
      "Google\n",
      "place\n",
      "Sunnyvale, CA, USA\n",
      "Minimum qualifications:\n",
      "Bachelor's degree in Computer Science, Electrical Engineering, a related field, or equivalent practical experience.\n",
      "8 years of experience in computer or chip architecture.\n",
      "Experience with semiconductor technologies and trends (e.g., process, memory, interconnect, and packaging).\n",
      "\n",
      "Preferred qualifications:\n",
      "Experience with learning frameworks including TensorFlow and PyTorch.\n",
      "Experience building hardware solutions for Machine Learning.\n",
      "Experience with outreach to Machine Learning researchers and application developers.\n",
      "Knowledge of Machine Learning market, technological and business trends, software ecosystem, and emerging applications.\n",
      "About the job\n",
      "Our computational challenges are so big, complex and unique we can't just purchase off-the-shelf hardware, we've got to make it ourselves. Your team designs and builds the hardware, software and networking technologies that power all of Google's services. As a Hardware Engineer, you design and build the systems that are the heart of the world's largest and most powerful computing infrastructure. You develop from the lowest levels of circuit design to large system design and see those systems all the way through to high volume manufacturing. Your work has the potential to shape the machinery that goes into our cutting-edge data centers affecting millions of Google users.\n",
      "\n",
      "In this role, you will create the custom chips for Google Tensor Processing Unit (TPU). You will work with the Google AI community and with external partners. You will combine the latest innovations in Machine Learning and integrated circuits to create advanced hardware acceleration solutions for Machine Learning training and inference.\n",
      "\n",
      "Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We keep our networks up and running, ensuring our users have the best and fastest experience possible.\n",
      "\n",
      "The US base salary range for this full-time position is $172,000-$263,000 + bonus + equity + benefits. Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training. Your recruiter can share more about the specific salary range for your preferred location during the hiring process.\n",
      "\n",
      "Please note that the compensation details listed in US role postings reflect the base salary only, and do not include bonus, equity, or benefits. Learn more about benefits at Google.\n",
      "\n",
      "Responsibilities\n",
      "Collaborate on architectural innovations for Google’s semiconductor TPU roadmap.\n",
      "Monitor industrial and academic trends in artificial intelligence and determine where they should intersect our roadmaps.\n",
      "Evaluate the power, performance, and cost of prospective architecture and subsystems.\n",
      "Engage with system and application software engineers to ensure optimization of the entire hardware/software stack.\n",
      "Engage with design, verification, and validation engineers to realize the architecture.\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "  print(ent.text, ent.label_)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42b91498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:52.695167Z",
     "iopub.status.busy": "2024-01-02T13:46:52.694435Z",
     "iopub.status.idle": "2024-01-02T13:46:52.698552Z",
     "shell.execute_reply": "2024-01-02T13:46:52.697781Z"
    },
    "papermill": {
     "duration": 0.025408,
     "end_time": "2024-01-02T13:46:52.700441",
     "exception": false,
     "start_time": "2024-01-02T13:46:52.675033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords = []\n",
    "seen_skills = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c95efeb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:52.738412Z",
     "iopub.status.busy": "2024-01-02T13:46:52.738134Z",
     "iopub.status.idle": "2024-01-02T13:46:52.743400Z",
     "shell.execute_reply": "2024-01-02T13:46:52.742569Z"
    },
    "papermill": {
     "duration": 0.026749,
     "end_time": "2024-01-02T13:46:52.745225",
     "exception": false,
     "start_time": "2024-01-02T13:46:52.718476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    if ent.label_ in (\"MUST HAVE\",\"GOOD TO HAVE\", \"LOCATION\", \"EXPERIENCE\"):\n",
    "        if ent.text not in seen_skills:\n",
    "            keywords.append({\"text\": ent.text, \"label\": ent.label_})\n",
    "            seen_skills.add(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "412efc54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:52.782292Z",
     "iopub.status.busy": "2024-01-02T13:46:52.782008Z",
     "iopub.status.idle": "2024-01-02T13:46:52.786645Z",
     "shell.execute_reply": "2024-01-02T13:46:52.785811Z"
    },
    "papermill": {
     "duration": 0.025335,
     "end_time": "2024-01-02T13:46:52.788470",
     "exception": false,
     "start_time": "2024-01-02T13:46:52.763135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"keywords.json\", \"w\") as f:\n",
    "    json.dump(keywords, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09361651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T13:46:52.825138Z",
     "iopub.status.busy": "2024-01-02T13:46:52.824851Z",
     "iopub.status.idle": "2024-01-02T13:46:52.830519Z",
     "shell.execute_reply": "2024-01-02T13:46:52.829408Z"
    },
    "papermill": {
     "duration": 0.026051,
     "end_time": "2024-01-02T13:46:52.832372",
     "exception": false,
     "start_time": "2024-01-02T13:46:52.806321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Machine Learning, Label: MUST HAVE\n",
      "Text: Sunnyvale, Label: LOCATION\n",
      "Text: CA, Label: LOCATION\n",
      "Text: USA, Label: LOCATION\n",
      "Text: 8, Label: EXPERIENCE\n",
      "Text: TensorFlow, Label: GOOD TO HAVE\n",
      "Text: PyTorch, Label: GOOD TO HAVE\n",
      "Text: TPU, Label: GOOD TO HAVE\n"
     ]
    }
   ],
   "source": [
    "with open(\"keywords.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "for keyword in data:\n",
    "    print(f\"Text: {keyword['text']}, Label: {keyword['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48490a59",
   "metadata": {
    "papermill": {
     "duration": 0.017901,
     "end_time": "2024-01-02T13:46:52.868567",
     "exception": false,
     "start_time": "2024-01-02T13:46:52.850666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4249709,
     "sourceId": 7322631,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4249727,
     "sourceId": 7322654,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4250089,
     "sourceId": 7323127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3929.28885,
   "end_time": "2024-01-02T13:46:54.316284",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-02T12:41:25.027434",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
